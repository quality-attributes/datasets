annotation_approver,id,label,meta,meta.number,meta.pretext,meta.title,text,user
,1976,144,,,,,"Magic variable role_names to also list roles belonged to via dependencies?

ISSUE TYPE

Feature Idea

ANSIBLE VERSION
ansible 2.0.2.0 (not that relevant here though)
COMPONENT NAME
magic variables
SUMMARY
The magic variable role_names, containing the list of all role names that the current target of the playbook belongs to, is quite useful in certain scenarios. For example, I use the same playbook to set up development vagrant VMs as well as test and production servers. A few tasks in certain roles are conditional on role membership, e.g. having when: ""'dev' in role_names"".
However, it looks like roles that are included via role dependencies in meta/main.yml files are not included in the role_names list. You could make an argument either way, but it seems to me that they should be there for completeness' sake, since role membership via dependency is still a type of role membership.
Seeing as role_names isn't really documented (""role_names"" site:docs.ansible.com on Google returns nothing, at least), it probably isn't very widely used right now, and changing the behaviour probably won't break much. Is this something you could consider?",3
,1976,148,,,,,"Magic variable role_names to also list roles belonged to via dependencies?

ISSUE TYPE

Feature Idea

ANSIBLE VERSION
ansible 2.0.2.0 (not that relevant here though)
COMPONENT NAME
magic variables
SUMMARY
The magic variable role_names, containing the list of all role names that the current target of the playbook belongs to, is quite useful in certain scenarios. For example, I use the same playbook to set up development vagrant VMs as well as test and production servers. A few tasks in certain roles are conditional on role membership, e.g. having when: ""'dev' in role_names"".
However, it looks like roles that are included via role dependencies in meta/main.yml files are not included in the role_names list. You could make an argument either way, but it seems to me that they should be there for completeness' sake, since role membership via dependency is still a type of role membership.
Seeing as role_names isn't really documented (""role_names"" site:docs.ansible.com on Google returns nothing, at least), it probably isn't very widely used right now, and changing the behaviour probably won't break much. Is this something you could consider?",2
,1977,148,,,,,"SmartOS IP address fact

On SmartOS, in contrast with the other OS's, it appears that the IP address object is an actual list of IPs.  Is that to be expected?
for example, in a smartos, I have to address it like this:
{{ ansible_net1['ipv4'][0]['address'] }}

where on linux
{{ ansible_net1['ipv4']['address'] }}

will suffice.",2
,1977,144,,,,,"SmartOS IP address fact

On SmartOS, in contrast with the other OS's, it appears that the IP address object is an actual list of IPs.  Is that to be expected?
for example, in a smartos, I have to address it like this:
{{ ansible_net1['ipv4'][0]['address'] }}

where on linux
{{ ansible_net1['ipv4']['address'] }}

will suffice.",3
,1978,148,,,,,"maven_artifact module should allow preserving the name of the downloaded artifact

ISSUE TYPE

Feature Idea

COMPONENT NAME
maven_artifact
ANSIBLE VERSION
ansible 2.3.0 (devel 3e4be156d7) last updated 2017/03/06 21:46:14 (GMT +200)

CONFIGURATION
N/A
OS / ENVIRONMENT
N/A
SUMMARY
When downloading an artifact you can ask the module to determine the latest version automatically (it gets extracted from the repo's metadata). The artifact's name in the repository comes with a version string which currently gets overwritten with 'latest' when downloading.
The problem with this approach is that you sometimes need to know what version the downloaded artifact has for further processing, My use case is transferring the version to the RPM that I build from it.
STEPS TO REPRODUCE
The implementation should introduce an additional parameter which causes what I described above. This is only relevant for the constellation version: latest and dest being a directory, because else the filename chosen by dest or the version explicitly selected win.

- hosts: localhost
  gather_facts: no
  tasks:
    - maven_artifact:
        version: latest
        artifact_id: spring-core
        group_id: org.springframework
        dest: /tmp/
        keep_name: yes
EXPECTED RESULTS
The downloaded artifact contains the dynamically determined version string. In this case, the last version found in this list. At the moment of creating this issue this is /tmp/spring-core-4.3.7.RELEASE.jar
ACTUAL RESULTS
The downloaded artifact is renamed to /tmp/spring-core-latest.jar",2
,1978,142,,,,,"maven_artifact module should allow preserving the name of the downloaded artifact

ISSUE TYPE

Feature Idea

COMPONENT NAME
maven_artifact
ANSIBLE VERSION
ansible 2.3.0 (devel 3e4be156d7) last updated 2017/03/06 21:46:14 (GMT +200)

CONFIGURATION
N/A
OS / ENVIRONMENT
N/A
SUMMARY
When downloading an artifact you can ask the module to determine the latest version automatically (it gets extracted from the repo's metadata). The artifact's name in the repository comes with a version string which currently gets overwritten with 'latest' when downloading.
The problem with this approach is that you sometimes need to know what version the downloaded artifact has for further processing, My use case is transferring the version to the RPM that I build from it.
STEPS TO REPRODUCE
The implementation should introduce an additional parameter which causes what I described above. This is only relevant for the constellation version: latest and dest being a directory, because else the filename chosen by dest or the version explicitly selected win.

- hosts: localhost
  gather_facts: no
  tasks:
    - maven_artifact:
        version: latest
        artifact_id: spring-core
        group_id: org.springframework
        dest: /tmp/
        keep_name: yes
EXPECTED RESULTS
The downloaded artifact contains the dynamically determined version string. In this case, the last version found in this list. At the moment of creating this issue this is /tmp/spring-core-4.3.7.RELEASE.jar
ACTUAL RESULTS
The downloaded artifact is renamed to /tmp/spring-core-latest.jar",3
,1979,143,,,,,"SSH connection is established for every command

Here is the log of the run
$ ansible-playbook deploy.yml -i hosts -vvvv -c ssh

PLAY [app] ********************************************************************

GATHERING FACTS ***************************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE setup
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=jzpkjtgucxrncjxcvemfmfxcxsgigepl] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-jzpkjtgucxrncjxcvemfmfxcxsgigepl; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname]

TASK: [install app-pkg] ****************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE apt pkg=app-pkg state=latest
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=yyqqlkoftogxutbcuhdaczmvxpbeurqf] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-yyqqlkoftogxutbcuhdaczmvxpbeurqf; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname] => {""changed"": false}

TASK: [install nginx] *********************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE apt pkg=nginx state=latest
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=hbtqwkkfqtfqkfdfcamgahpoycrmilqw] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-hbtqwkkfqtfqkfdfcamgahpoycrmilqw; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname] => {""changed"": false}

PLAY RECAP ********************************************************************
hostname      : ok=3    changed=0    unreachable=0    failed=0

The deply.yml is as simple as

---
- hosts: app
  vars: ~
  remote_user: xxxxxxxxx
  sudo: yes

followed by 2 tasks to install packages via apt-get.
The OS on both machines is ubuntu trusty, the requiretty does not exist in sudoers (on the target machine) and setting Defaults !requiretty does not change anything.
In the ansible config I have made only these 2 changes:


uncommented
ssh_args = -o ControlMaster=auto -o ControlPersist=60s


enabled
pipelining = True


It's for ansible 1.9.1 installed from the PPA.
What am I doing wrong?",2
,1979,140,,,,,"SSH connection is established for every command

Here is the log of the run
$ ansible-playbook deploy.yml -i hosts -vvvv -c ssh

PLAY [app] ********************************************************************

GATHERING FACTS ***************************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE setup
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=jzpkjtgucxrncjxcvemfmfxcxsgigepl] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-jzpkjtgucxrncjxcvemfmfxcxsgigepl; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname]

TASK: [install app-pkg] ****************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE apt pkg=app-pkg state=latest
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=yyqqlkoftogxutbcuhdaczmvxpbeurqf] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-yyqqlkoftogxutbcuhdaczmvxpbeurqf; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname] => {""changed"": false}

TASK: [install nginx] *********************************************************
<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx
<hostname> REMOTE_MODULE apt pkg=nginx state=latest
<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/me/.ansible/cp/ansible-ssh-%h-%p-%r"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p ""[sudo via ansible, key=hbtqwkkfqtfqkfdfcamgahpoycrmilqw] password: "" -u root /bin/sh -c '""'""'echo BECOME-SUCCESS-hbtqwkkfqtfqkfdfcamgahpoycrmilqw; LANG=C LC_CTYPE=C /usr/bin/python'""'""''
ok: [hostname] => {""changed"": false}

PLAY RECAP ********************************************************************
hostname      : ok=3    changed=0    unreachable=0    failed=0

The deply.yml is as simple as

---
- hosts: app
  vars: ~
  remote_user: xxxxxxxxx
  sudo: yes

followed by 2 tasks to install packages via apt-get.
The OS on both machines is ubuntu trusty, the requiretty does not exist in sudoers (on the target machine) and setting Defaults !requiretty does not change anything.
In the ansible config I have made only these 2 changes:


uncommented
ssh_args = -o ControlMaster=auto -o ControlPersist=60s


enabled
pipelining = True


It's for ansible 1.9.1 installed from the PPA.
What am I doing wrong?",3
,1980,148,,,,,"rax_identity stacktraces with pyrax >= 1.8.0

Starting with pyrax 1.8.0 and greater installed, the rax_identity module now dies with a stacktrace trying to serialize the output:
failed: [localhost] => {""failed"": true, ""parsed"": false}
invalid output was: Traceback (most recent call last):
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1438, in <module>
    main()
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 110, in main
    cloud_identity(module, state, pyrax.identity)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 82, in cloud_identity
    module.exit_json(changed=changed, identity=instance)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1051, in exit_json
    print self.jsonify(kwargs)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1029, in jsonify
    return json.dumps(data)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py"", line 243, in dumps
    return _default_encoder.encode(obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <'load_balancer' Service object at 0x102bc23d0> is not JSON serializable

As a workaround, rolling back to < 1.8.0 fixes the problem.",3
,1980,142,,,,,"rax_identity stacktraces with pyrax >= 1.8.0

Starting with pyrax 1.8.0 and greater installed, the rax_identity module now dies with a stacktrace trying to serialize the output:
failed: [localhost] => {""failed"": true, ""parsed"": false}
invalid output was: Traceback (most recent call last):
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1438, in <module>
    main()
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 110, in main
    cloud_identity(module, state, pyrax.identity)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 82, in cloud_identity
    module.exit_json(changed=changed, identity=instance)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1051, in exit_json
    print self.jsonify(kwargs)
  File ""/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity"", line 1029, in jsonify
    return json.dumps(data)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py"", line 243, in dumps
    return _default_encoder.encode(obj)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <'load_balancer' Service object at 0x102bc23d0> is not JSON serializable

As a workaround, rolling back to < 1.8.0 fixes the problem.",2
,1981,144,,,,,"Force_handlers with_items does not execute handlers

ISSUE TYPE

Bug Report

COMPONENT NAME
Force handlers with_items
ANSIBLE VERSION
ansible 2.3.0.0
config file = /etc/ansible/ansible.cfg
configured module search path = configured module search path = Default w/o overrides
python version = 2.7.10 (default, May  1 2017, 19:24:18) [GCC 4.4.7 20120313 (Red Hat 4.4.7-16)]
CONFIGURATION
OS / ENVIRONMENT
SUMMARY
We expect handlers to run when at least one item is changed when using with_items and force_handlers.
STEPS TO REPRODUCE
---
- hosts: localhost
  gather_facts: no
  force_handlers: True
  handlers:
    - name: hello_world
      debug:
  vars:
    commands:
      - ls
      - pwd
      - asdf
  tasks:
    - shell: ""{{ item }}""
      with_items: ""{{ commands }}""
      notify: hello_world

EXPECTED RESULTS
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001902"", ""end"": ""2017-05-10 13:59:28.650743"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 13:59:28.648841"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
RUNNING HANDLER [hello_world] *************************************************************************************************************************************************************************************************************************************************
ok: [localhost] => {
""changed"": false,
""msg"": ""Hello world!""
}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=1
ACTUAL RESULTS
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001916"", ""end"": ""2017-05-10 14:00:00.284031"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 14:00:00.282115"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=1",3
,1981,148,,,,,"Force_handlers with_items does not execute handlers

ISSUE TYPE

Bug Report

COMPONENT NAME
Force handlers with_items
ANSIBLE VERSION
ansible 2.3.0.0
config file = /etc/ansible/ansible.cfg
configured module search path = configured module search path = Default w/o overrides
python version = 2.7.10 (default, May  1 2017, 19:24:18) [GCC 4.4.7 20120313 (Red Hat 4.4.7-16)]
CONFIGURATION
OS / ENVIRONMENT
SUMMARY
We expect handlers to run when at least one item is changed when using with_items and force_handlers.
STEPS TO REPRODUCE
---
- hosts: localhost
  gather_facts: no
  force_handlers: True
  handlers:
    - name: hello_world
      debug:
  vars:
    commands:
      - ls
      - pwd
      - asdf
  tasks:
    - shell: ""{{ item }}""
      with_items: ""{{ commands }}""
      notify: hello_world

EXPECTED RESULTS
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001902"", ""end"": ""2017-05-10 13:59:28.650743"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 13:59:28.648841"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
RUNNING HANDLER [hello_world] *************************************************************************************************************************************************************************************************************************************************
ok: [localhost] => {
""changed"": false,
""msg"": ""Hello world!""
}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=1
ACTUAL RESULTS
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001916"", ""end"": ""2017-05-10 14:00:00.284031"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 14:00:00.282115"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=1",2
,1982,144,,,,,"Stack trace in YAML Validation Code

From the ansible-project list, the stack trace is as follows:
Traceback (most recent call last):
  File ""/usr/local/share/python/ansible-playbook"", line 268, in <module>
    sys.exit(main(sys.argv[1:]))
  File ""/usr/local/share/python/ansible-playbook"", line 208, in main
    pb.run()
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/__init__.py"", line 228, in run
    play = Play(self, play_ds, play_basedir)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 80, in __init__
    ds = self._load_roles(self.roles, ds)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 288, in _load_roles
    roles = self._build_role_dependencies(roles, [], self.vars)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 178, in _build_role_dependencies
    defaults_data = utils.parse_yaml_from_file(defaults)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 419, in parse_yaml_from_file
    process_yaml_error(exc, data, path)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 397, in process_yaml_error
    msg = process_common_errors(msg, probline, mark.column)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 358, in process_common_errors
    elif len(probline) and probline[column] == "":"" and probline.find(""="") != -1:
IndexError: string index out of range
The issue ended up being that I missed a ':' between a variable key and value. I assume that this can be reproduced by leaving off a colon anywhere in a variable definition (or maybe even tasks). I specifically created this  it by creating a vars section and putting:
---
variable value
I fixed it by instead doing:
---
variable: value
@mpdehaan mentioned that this would be a pretty quick fix.",3
,1982,148,,,,,"Stack trace in YAML Validation Code

From the ansible-project list, the stack trace is as follows:
Traceback (most recent call last):
  File ""/usr/local/share/python/ansible-playbook"", line 268, in <module>
    sys.exit(main(sys.argv[1:]))
  File ""/usr/local/share/python/ansible-playbook"", line 208, in main
    pb.run()
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/__init__.py"", line 228, in run
    play = Play(self, play_ds, play_basedir)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 80, in __init__
    ds = self._load_roles(self.roles, ds)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 288, in _load_roles
    roles = self._build_role_dependencies(roles, [], self.vars)
  File ""/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py"", line 178, in _build_role_dependencies
    defaults_data = utils.parse_yaml_from_file(defaults)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 419, in parse_yaml_from_file
    process_yaml_error(exc, data, path)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 397, in process_yaml_error
    msg = process_common_errors(msg, probline, mark.column)
  File ""/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py"", line 358, in process_common_errors
    elif len(probline) and probline[column] == "":"" and probline.find(""="") != -1:
IndexError: string index out of range
The issue ended up being that I missed a ':' between a variable key and value. I assume that this can be reproduced by leaving off a colon anywhere in a variable definition (or maybe even tasks). I specifically created this  it by creating a vars section and putting:
---
variable value
I fixed it by instead doing:
---
variable: value
@mpdehaan mentioned that this would be a pretty quick fix.",2
,1983,143,,,,,"password_hash/get_encrypted_password uses passlib default of rounds=656000 which is 131 times glibc default

In the password_hash filter function the underlying passlib call uses the default rounds parameter.
The default for glibc is 5000, the passlib default for sha512 is 656000. This means on a login in a linux account the hash calculation will take significantly longer.
Actually you get basically no rounds parameter when setting it to 5000
$ python -c ""from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5000)""
$6$0zDM3P3/MbIoF0G0$pVmR8hc0ZNYdfGLnFhVKeCGhd32kDXo6ky83JmUWTZCfVQm2IrhnIbrrg5Vyl9XxE5aWzdmuYpvTA6gyOFc5Z.
$ python -c ""from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5001)""
$6$rounds=5001$VFyJ36YLb/RLhx0e$CAFCB/W7ebYEHIFsZtlJSdvuzEtsYOAvtOwCF7ahqpWqLj62TJp4LlzZ/FiW1B2U5kSKh4xibiJgZyd2ceVoI1
Could a simple parameter be added to get_encrypted_password to set the value which has a preferable default what glibc does?",3
,1983,143,,,,,"password_hash/get_encrypted_password uses passlib default of rounds=656000 which is 131 times glibc default

In the password_hash filter function the underlying passlib call uses the default rounds parameter.
The default for glibc is 5000, the passlib default for sha512 is 656000. This means on a login in a linux account the hash calculation will take significantly longer.
Actually you get basically no rounds parameter when setting it to 5000
$ python -c ""from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5000)""
$6$0zDM3P3/MbIoF0G0$pVmR8hc0ZNYdfGLnFhVKeCGhd32kDXo6ky83JmUWTZCfVQm2IrhnIbrrg5Vyl9XxE5aWzdmuYpvTA6gyOFc5Z.
$ python -c ""from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5001)""
$6$rounds=5001$VFyJ36YLb/RLhx0e$CAFCB/W7ebYEHIFsZtlJSdvuzEtsYOAvtOwCF7ahqpWqLj62TJp4LlzZ/FiW1B2U5kSKh4xibiJgZyd2ceVoI1
Could a simple parameter be added to get_encrypted_password to set the value which has a preferable default what glibc does?",2
,1984,148,,,,,"Broken 'Edit on GitHub' link at http://docs.ansible.com/ansible/intro_adhoc.html

ISSUE TYPE

Documentation Report

SUMMARY
'Edit on GitHub' http://joxi.ru/l2Z6VxFwlbVL2J?d=1 link leads to 404 error page http://joxi.ru/eAO14Wfxv1Gdmo?d=1
STEPS TO REPRODUCE
1 Go to http://docs.ansible.com/ansible/intro_adhoc.html
2 Press 'Edit on GitHub' at top right corner.
3 See 404 error page.
EXPECTED RESULTS
Some kind of valid git hub page for editing related page.
ACTUAL RESULTS
404 error page",3
,1984,148,,,,,"Broken 'Edit on GitHub' link at http://docs.ansible.com/ansible/intro_adhoc.html

ISSUE TYPE

Documentation Report

SUMMARY
'Edit on GitHub' http://joxi.ru/l2Z6VxFwlbVL2J?d=1 link leads to 404 error page http://joxi.ru/eAO14Wfxv1Gdmo?d=1
STEPS TO REPRODUCE
1 Go to http://docs.ansible.com/ansible/intro_adhoc.html
2 Press 'Edit on GitHub' at top right corner.
3 See 404 error page.
EXPECTED RESULTS
Some kind of valid git hub page for editing related page.
ACTUAL RESULTS
404 error page",2
,1985,148,,,,,"ansible fails with an exception in python

I sinply run command: ""forever stopall""
but I get this output from stdout when it fails:
fatal: [node1] => Traceback (most recent call last):
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 586, in _executor
exec_rc = self._executor_internal(host, new_stdin)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 789, in _executor_internal
return self._executor_internal_inner(host, self.module_name, self.module_args, inject, port, complex_args=complex_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 1005, in _executor_internal_inner
num_args_post = self._count_module_args(module_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 433, in _count_module_args
vargs = split_args(args)
File ""/Library/Python/2.7/site-packages/ansible/module_utils/splitter.py"", line 73, in split_args
args = args.strip()
AttributeError: 'dict' object has no attribute 'strip'",3
,1985,148,,,,,"ansible fails with an exception in python

I sinply run command: ""forever stopall""
but I get this output from stdout when it fails:
fatal: [node1] => Traceback (most recent call last):
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 586, in _executor
exec_rc = self._executor_internal(host, new_stdin)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 789, in _executor_internal
return self._executor_internal_inner(host, self.module_name, self.module_args, inject, port, complex_args=complex_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 1005, in _executor_internal_inner
num_args_post = self._count_module_args(module_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 433, in _count_module_args
vargs = split_args(args)
File ""/Library/Python/2.7/site-packages/ansible/module_utils/splitter.py"", line 73, in split_args
args = args.strip()
AttributeError: 'dict' object has no attribute 'strip'",2
,1986,144,,,,,"--diff Flag crashes on win_copy

Issue Type:

Bug Report

Ansible Version:
ansible 2.1.0 (devel cd51ba7965) last updated 2016/02/24 21:58:23 (GMT +000)
  lib/ansible/modules/core: (detached HEAD e9454fa44f) last updated 2016/02/24 21:58:47 (GMT +000)
  lib/ansible/modules/extras: (detached HEAD fade5b7936) last updated 2016/02/24 21:58:47 (GMT +000)
  config file = /etc/ansible/ansible.cfg
  configured module search path = /opt/maxpoint/ansible/release/library

Ansible Configuration:
Stock
Environment:
Running from: Ubuntu 14.04
Targeting: Windows 2k12
Summary:
The --diff flag fails when using the win_copy module.
Steps To Reproduce:
Run any playbook that contains the win_copy module while using the --diff flag against a Windows box.
- name: Copy Over Pool Config Scripts
  win_copy:
    src: ""iis_pool_settings.ps1""
    dest: D:\\Temp\\iis_pool_settings.ps1

Expected Results:
I expect to see a diff between the file that was already on disk, and the one I just copied over.
Actual Results:

Here's the entire output
https://gist.github.com/blakfeld/542b191c85f4385e50bf
Here's the relevant output
TASK [company_bidder_deploy : Copy Over company Pool Config Scripts] *********
task path: /opt/company/ansible/release/roles/company_bidder_deploy/tasks/pre_install.yml:29
<computername.companyinteractive.com> ESTABLISH WINRM CONNECTION FOR USER: Administrator on PORT 5986 TO computername.companyinteractive.com
<computername.companyinteractive.com> WINRM CONNECT: transport=ssl endpoint=https://computername.companyinteractive.com:5986/wsman
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414325.16-195672407265922"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM OPEN SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgAiACkALgBGAHUAbABsAE4AYQBtAGUAIAB8ACAAVwByAGkAdABlAC0ASABvAHMAdAAgAC0AUwBlAHAAYQByAGEAdABvAHIAIAAnACcAOwA=']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> PUT ""/tmp/tmpfOwmcO"" TO ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1""
<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgBcAHMAdABhAHQALgBwAHMAMQAiAAoAJABEAGUAYgB1AGcAUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAEMAbwBuAHQAaQBuAHUAZQAiAAoAJABFAHIAcgBvAHIAQQBjAHQAaQBvAG4AUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAFMAdABvAHAAIgAKAFMAZQB0AC0AUwB0AHIAaQBjAHQATQBvAGQAZQAgAC0AVgBlAHIAcwBpAG8AbgAgADIACgAkAGYAZAAgAD0AIABbAFMAeQBzAHQAZQBtAC4ASQBPAC4ARgBpAGwAZQBdADoAOgBDAHIAZQBhAHQAZQAoACQAcABhAHQAaAApAAoAJABzAGgAYQAxACAAPQAgAFsAUwB5AHMAdABlAG0ALgBTAGUAYwB1AHIAaQB0AHkALgBDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAuAFMASABBADEAQwByAHkAcAB0AG8AUwBlAHIAdgBpAGMAZQBQAHIAbwB2AGkAZABlAHIAXQA6ADoAQwByAGUAYQB0AGUAKAApAAoAJABiAHkAdABlAHMAIAA9ACAAQAAoACkAIAAjAGkAbgBpAHQAaQBhAGwAaQB6AGUAIABmAG8AcgAgAGUAbQBwAHQAeQAgAGYAaQBsAGUAIABjAGEAcwBlAAoAfQAKAHAAcgBvAGMAZQBzAHMAIAB7AAoAJABiAHkAdABlAHMAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEMAbwBuAHYAZQByAHQAXQA6ADoARgByAG8AbQBCAGEAcwBlADYANABTAHQAcgBpAG4AZwAoACQAaQBuAHAAdQB0ACkACgAkAHMAaABhADEALgBUAHIAYQBuAHMAZgBvAHIAbQBCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAAsACAAJABiAHkAdABlAHMALAAgADAAKQAgAHwAIABPAHUAdAAtAE4AdQBsAGwACgAkAGYAZAAuAFcAcgBpAHQAZQAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAkAGIAeQB0AGUAcwAuAEwAZQBuAGcAdABoACkACgB9AAoAZQBuAGQAIAB7AAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0ARgBpAG4AYQBsAEIAbABvAGMAawAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABoAGEAcwBoACAAPQAgAFsAUwB5AHMAdABlAG0ALgBCAGkAdABDAG8AbgB2AGUAcgB0AGUAcgBdADoAOgBUAG8AUwB0AHIAaQBuAGcAKAAkAHMAaABhADEALgBIAGEAcwBoACkALgBSAGUAcABsAGEAYwBlACgAIgAtACIALAAgACIAIgApAC4AVABvAEwAbwB3AGUAcgBJAG4AdgBhAHIAaQBhAG4AdAAoACkACgAkAGYAZAAuAEMAbABvAHMAZQAoACkACgBXAHIAaQB0AGUALQBPAHUAdABwAHUAdAAgACIAewAiACIAcwBoAGEAMQAiACIAOgAiACIAJABoAGEAcwBoACIAIgB9ACIACgB9AA==']
<computername.companyinteractive.com> WINRM PUT ""/tmp/tmpfOwmcO"" to ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1"" (offset=10177 size=10177)
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""sha1"":""4b5fc9661a0"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""sha1"":""4b5fc9661a08b2372dc5efa608f719480e61502b""}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
Try
{
& ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1""
}
Catch
{
$_obj = @{ failed = $true }
If ($_.Exception.GetType)
{
$_obj.Add('msg', $_.Exception.Message)
}
Else
{
$_obj.Add('msg', $_.ToString())
}
If ($_.InvocationInfo.PositionMessage)
{
$_obj.Add('exception', $_.InvocationInfo.PositionMessage)
}
ElseIf ($_.ScriptStackTrace)
{
$_obj.Add('exception', $_.ScriptStackTrace)
}
Try
{
$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))
}
Catch
{
}
Echo $_obj | ConvertTo-Json -Compress -Depth 99
Exit 1
}
Finally { Remove-Item ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922"" -Force -Recurse -ErrorAction SilentlyContinue }
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'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']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""stat"":{""exists"":fa"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""stat"":{""exists"":false},""changed"":false}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414326.29-74854495923127"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgAyADkALQA3ADQAOAA1ADQANAA5ADUAOQAyADMAMQAyADcAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.29-74854495923127
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414326.6-190477404467970"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> PUT ""/tmp/tmpcEFZin"" TO ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1""
<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAXABmAGkAbABlAC4AcABzADEAIgAKACQARABlAGIAdQBnAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBDAG8AbgB0AGkAbgB1AGUAIgAKACQARQByAHIAbwByAEEAYwB0AGkAbwBuAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBTAHQAbwBwACIACgBTAGUAdAAtAFMAdAByAGkAYwB0AE0AbwBkAGUAIAAtAFYAZQByAHMAaQBvAG4AIAAyAAoAJABmAGQAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEkATwAuAEYAaQBsAGUAXQA6ADoAQwByAGUAYQB0AGUAKAAkAHAAYQB0AGgAKQAKACQAcwBoAGEAMQAgAD0AIABbAFMAeQBzAHQAZQBtAC4AUwBlAGMAdQByAGkAdAB5AC4AQwByAHkAcAB0AG8AZwByAGEAcABoAHkALgBTAEgAQQAxAEMAcgB5AHAAdABvAFMAZQByAHYAaQBjAGUAUAByAG8AdgBpAGQAZQByAF0AOgA6AEMAcgBlAGEAdABlACgAKQAKACQAYgB5AHQAZQBzACAAPQAgAEAAKAApACAAIwBpAG4AaQB0AGkAYQBsAGkAegBlACAAZgBvAHIAIABlAG0AcAB0AHkAIABmAGkAbABlACAAYwBhAHMAZQAKAH0ACgBwAHIAbwBjAGUAcwBzACAAewAKACQAYgB5AHQAZQBzACAAPQAgAFsAUwB5AHMAdABlAG0ALgBDAG8AbgB2AGUAcgB0AF0AOgA6AEYAcgBvAG0AQgBhAHMAZQA2ADQAUwB0AHIAaQBuAGcAKAAkAGkAbgBwAHUAdAApAAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0AQgBsAG8AYwBrACgAJABiAHkAdABlAHMALAAgADAALAAgACQAYgB5AHQAZQBzAC4ATABlAG4AZwB0AGgALAAgACQAYgB5AHQAZQBzACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABmAGQALgBXAHIAaQB0AGUAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAApAAoAfQAKAGUAbgBkACAAewAKACQAcwBoAGEAMQAuAFQAcgBhAG4AcwBmAG8AcgBtAEYAaQBuAGEAbABCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAMAApACAAfAAgAE8AdQB0AC0ATgB1AGwAbAAKACQAaABhAHMAaAAgAD0AIABbAFMAeQBzAHQAZQBtAC4AQgBpAHQAQwBvAG4AdgBlAHIAdABlAHIAXQA6ADoAVABvAFMAdAByAGkAbgBnACgAJABzAGgAYQAxAC4ASABhAHMAaAApAC4AUgBlAHAAbABhAGMAZQAoACIALQAiACwAIAAiACIAKQAuAFQAbwBMAG8AdwBlAHIASQBuAHYAYQByAGkAYQBuAHQAKAApAAoAJABmAGQALgBDAGwAbwBzAGUAKAApAAoAVwByAGkAdABlAC0ATwB1AHQAcAB1AHQAIAAiAHsAIgAiAHMAaABhADEAIgAiADoAIgAiACQAaABhAHMAaAAiACIAfQAiAAoAfQA=']
<computername.companyinteractive.com> WINRM PUT ""/tmp/tmpcEFZin"" to ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1"" (offset=10656 size=10656)
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""sha1"":""7f5d2fe5184"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""sha1"":""7f5d2fe5184e28c170ff1de2a7b611897f896275""}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
Try
{
& ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1""
}
Catch
{
$_obj = @{ failed = $true }
If ($_.Exception.GetType)
{
$_obj.Add('msg', $_.Exception.Message)
}
Else
{
$_obj.Add('msg', $_.ToString())
}
If ($_.InvocationInfo.PositionMessage)
{
$_obj.Add('exception', $_.InvocationInfo.PositionMessage)
}
ElseIf ($_.ScriptStackTrace)
{
$_obj.Add('exception', $_.ScriptStackTrace)
}
Try
{
$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))
}
Catch
{
}
Echo $_obj | ConvertTo-Json -Compress -Depth 99
Exit 1
}
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgBUAHIAeQAKAHsACgAmACAAIgBDADoAXABVAHMAZQByAHMAXABBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAFwAQQBwAHAARABhAHQAYQBcAEwAbwBjAGEAbABcAFQAZQBtAHAAXABhAG4AcwBpAGIAbABlAC0AdABtAHAALQAxADQANQA2ADQAMQA0ADMAMgA2AC4ANgAtADEAOQAwADQANwA3ADQAMAA0ADQANgA3ADkANwAwAFwAZgBpAGwAZQAuAHAAcwAxACIACgB9AAoAQwBhAHQAYwBoAAoAewAKACQAXwBvAGIAagAgAD0AIABAAHsAIABmAGEAaQBsAGUAZAAgAD0AIAAkAHQAcgB1AGUAIAB9AAoASQBmACAAKAAkAF8ALgBFAHgAYwBlAHAAdABpAG8AbgAuAEcAZQB0AFQAeQBwAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4ARQB4AGMAZQBwAHQAaQBvAG4ALgBNAGUAcwBzAGEAZwBlACkACgB9AAoARQBsAHMAZQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4AVABvAFMAdAByAGkAbgBnACgAKQApAAoAfQAKAEkAZgAgACgAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAZQB4AGMAZQBwAHQAaQBvAG4AJwAsACAAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAH0ACgBFAGwAcwBlAEkAZgAgACgAJABfAC4AUwBjAHIAaQBwAHQAUwB0AGEAYwBrAFQAcgBhAGMAZQApAAoAewAKACQAXwBvAGIAagAuAEEAZABkACgAJwBlAHgAYwBlAHAAdABpAG8AbgAnACwAIAAkAF8ALgBTAGMAcgBpAHAAdABTAHQAYQBjAGsAVAByAGEAYwBlACkACgB9AAoAVAByAHkACgB7AAoAJABfAG8AYgBqAC4AQQBkAGQAKAAnAGUAcgByAG8AcgBfAHIAZQBjAG8AcgBkACcALAAgACgAJABfACAAfAAgAEMAbwBuAHYAZQByAHQAVABvAC0ASgBzAG8AbgAgAHwAIABDAG8AbgB2AGUAcgB0AEYAcgBvAG0ALQBKAHMAbwBuACkAKQAKAH0ACgBDAGEAdABjAGgACgB7AAoAfQAKAEUAYwBoAG8AIAAkAF8AbwBiAGoAIAB8ACAAQwBvAG4AdgBlAHIAdABUAG8ALQBKAHMAbwBuACAALQBDAG8AbQBwAHIAZQBzAHMAIAAtAEQAZQBwAHQAaAAgADkAOQAKAEUAeABpAHQAIAAxAAoAfQA=']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 1, out ""{""msg"":""path will no"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""msg"":""path will not be created"",""failed"":true}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> WINRM CLOSE SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F
An exception occurred during task execution. The full traceback is:
Traceback (most recent call last):
  File ""/opt/ansible-src/lib/ansible/executor/task_executor.py"", line 122, in run
    res = self._execute()
  File ""/opt/ansible-src/lib/ansible/executor/task_executor.py"", line 418, in _execute
    result = self._handler.run(task_vars=variables)
  File ""/opt/ansible-src/lib/ansible/plugins/action/copy.py"", line 202, in run
    diffs.append(self._get_diff_data(dest_file, source_full, task_vars))
  File ""/opt/ansible-src/lib/ansible/plugins/action/__init__.py"", line 614, in _get_diff_data
    if peek_result['state'] == 'absent':
KeyError: 'state'

The problem appears to be because the --diff command is relying upon the existence of a 'state' key in the returned JSON. This appears to be a key added in the module_utils.basic Python module, specifically the add_path_info method, which seems to be called by load_common_file_arguments. I was going to just hack this into win_copy, but it seems to me that the most reasonable solution would be to add load_common_file_arguments to powershell_common.ps1. I'm more than happy to start work on that if the community agrees that is the best answer.",3
,1986,148,,,,,"--diff Flag crashes on win_copy

Issue Type:

Bug Report

Ansible Version:
ansible 2.1.0 (devel cd51ba7965) last updated 2016/02/24 21:58:23 (GMT +000)
  lib/ansible/modules/core: (detached HEAD e9454fa44f) last updated 2016/02/24 21:58:47 (GMT +000)
  lib/ansible/modules/extras: (detached HEAD fade5b7936) last updated 2016/02/24 21:58:47 (GMT +000)
  config file = /etc/ansible/ansible.cfg
  configured module search path = /opt/maxpoint/ansible/release/library

Ansible Configuration:
Stock
Environment:
Running from: Ubuntu 14.04
Targeting: Windows 2k12
Summary:
The --diff flag fails when using the win_copy module.
Steps To Reproduce:
Run any playbook that contains the win_copy module while using the --diff flag against a Windows box.
- name: Copy Over Pool Config Scripts
  win_copy:
    src: ""iis_pool_settings.ps1""
    dest: D:\\Temp\\iis_pool_settings.ps1

Expected Results:
I expect to see a diff between the file that was already on disk, and the one I just copied over.
Actual Results:

Here's the entire output
https://gist.github.com/blakfeld/542b191c85f4385e50bf
Here's the relevant output
TASK [company_bidder_deploy : Copy Over company Pool Config Scripts] *********
task path: /opt/company/ansible/release/roles/company_bidder_deploy/tasks/pre_install.yml:29
<computername.companyinteractive.com> ESTABLISH WINRM CONNECTION FOR USER: Administrator on PORT 5986 TO computername.companyinteractive.com
<computername.companyinteractive.com> WINRM CONNECT: transport=ssl endpoint=https://computername.companyinteractive.com:5986/wsman
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414325.16-195672407265922"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM OPEN SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgAiACkALgBGAHUAbABsAE4AYQBtAGUAIAB8ACAAVwByAGkAdABlAC0ASABvAHMAdAAgAC0AUwBlAHAAYQByAGEAdABvAHIAIAAnACcAOwA=']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> PUT ""/tmp/tmpfOwmcO"" TO ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1""
<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgBcAHMAdABhAHQALgBwAHMAMQAiAAoAJABEAGUAYgB1AGcAUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAEMAbwBuAHQAaQBuAHUAZQAiAAoAJABFAHIAcgBvAHIAQQBjAHQAaQBvAG4AUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAFMAdABvAHAAIgAKAFMAZQB0AC0AUwB0AHIAaQBjAHQATQBvAGQAZQAgAC0AVgBlAHIAcwBpAG8AbgAgADIACgAkAGYAZAAgAD0AIABbAFMAeQBzAHQAZQBtAC4ASQBPAC4ARgBpAGwAZQBdADoAOgBDAHIAZQBhAHQAZQAoACQAcABhAHQAaAApAAoAJABzAGgAYQAxACAAPQAgAFsAUwB5AHMAdABlAG0ALgBTAGUAYwB1AHIAaQB0AHkALgBDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAuAFMASABBADEAQwByAHkAcAB0AG8AUwBlAHIAdgBpAGMAZQBQAHIAbwB2AGkAZABlAHIAXQA6ADoAQwByAGUAYQB0AGUAKAApAAoAJABiAHkAdABlAHMAIAA9ACAAQAAoACkAIAAjAGkAbgBpAHQAaQBhAGwAaQB6AGUAIABmAG8AcgAgAGUAbQBwAHQAeQAgAGYAaQBsAGUAIABjAGEAcwBlAAoAfQAKAHAAcgBvAGMAZQBzAHMAIAB7AAoAJABiAHkAdABlAHMAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEMAbwBuAHYAZQByAHQAXQA6ADoARgByAG8AbQBCAGEAcwBlADYANABTAHQAcgBpAG4AZwAoACQAaQBuAHAAdQB0ACkACgAkAHMAaABhADEALgBUAHIAYQBuAHMAZgBvAHIAbQBCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAAsACAAJABiAHkAdABlAHMALAAgADAAKQAgAHwAIABPAHUAdAAtAE4AdQBsAGwACgAkAGYAZAAuAFcAcgBpAHQAZQAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAkAGIAeQB0AGUAcwAuAEwAZQBuAGcAdABoACkACgB9AAoAZQBuAGQAIAB7AAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0ARgBpAG4AYQBsAEIAbABvAGMAawAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABoAGEAcwBoACAAPQAgAFsAUwB5AHMAdABlAG0ALgBCAGkAdABDAG8AbgB2AGUAcgB0AGUAcgBdADoAOgBUAG8AUwB0AHIAaQBuAGcAKAAkAHMAaABhADEALgBIAGEAcwBoACkALgBSAGUAcABsAGEAYwBlACgAIgAtACIALAAgACIAIgApAC4AVABvAEwAbwB3AGUAcgBJAG4AdgBhAHIAaQBhAG4AdAAoACkACgAkAGYAZAAuAEMAbABvAHMAZQAoACkACgBXAHIAaQB0AGUALQBPAHUAdABwAHUAdAAgACIAewAiACIAcwBoAGEAMQAiACIAOgAiACIAJABoAGEAcwBoACIAIgB9ACIACgB9AA==']
<computername.companyinteractive.com> WINRM PUT ""/tmp/tmpfOwmcO"" to ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1"" (offset=10177 size=10177)
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""sha1"":""4b5fc9661a0"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""sha1"":""4b5fc9661a08b2372dc5efa608f719480e61502b""}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
Try
{
& ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922\stat.ps1""
}
Catch
{
$_obj = @{ failed = $true }
If ($_.Exception.GetType)
{
$_obj.Add('msg', $_.Exception.Message)
}
Else
{
$_obj.Add('msg', $_.ToString())
}
If ($_.InvocationInfo.PositionMessage)
{
$_obj.Add('exception', $_.InvocationInfo.PositionMessage)
}
ElseIf ($_.ScriptStackTrace)
{
$_obj.Add('exception', $_.ScriptStackTrace)
}
Try
{
$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))
}
Catch
{
}
Echo $_obj | ConvertTo-Json -Compress -Depth 99
Exit 1
}
Finally { Remove-Item ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414325.16-195672407265922"" -Force -Recurse -ErrorAction SilentlyContinue }
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'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']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""stat"":{""exists"":fa"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""stat"":{""exists"":false},""changed"":false}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414326.29-74854495923127"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgAyADkALQA3ADQAOAA1ADQANAA5ADUAOQAyADMAMQAyADcAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.29-74854495923127
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
(New-Item -Type Directory -Path $env:temp -Name ""ansible-tmp-1456414326.6-190477404467970"").FullName | Write-Host -Separator '';
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""C:\\Users\\Administrat"", err """">'
<computername.companyinteractive.com> WINRM STDOUT C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> PUT ""/tmp/tmpcEFZin"" TO ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1""
<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAXABmAGkAbABlAC4AcABzADEAIgAKACQARABlAGIAdQBnAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBDAG8AbgB0AGkAbgB1AGUAIgAKACQARQByAHIAbwByAEEAYwB0AGkAbwBuAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBTAHQAbwBwACIACgBTAGUAdAAtAFMAdAByAGkAYwB0AE0AbwBkAGUAIAAtAFYAZQByAHMAaQBvAG4AIAAyAAoAJABmAGQAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEkATwAuAEYAaQBsAGUAXQA6ADoAQwByAGUAYQB0AGUAKAAkAHAAYQB0AGgAKQAKACQAcwBoAGEAMQAgAD0AIABbAFMAeQBzAHQAZQBtAC4AUwBlAGMAdQByAGkAdAB5AC4AQwByAHkAcAB0AG8AZwByAGEAcABoAHkALgBTAEgAQQAxAEMAcgB5AHAAdABvAFMAZQByAHYAaQBjAGUAUAByAG8AdgBpAGQAZQByAF0AOgA6AEMAcgBlAGEAdABlACgAKQAKACQAYgB5AHQAZQBzACAAPQAgAEAAKAApACAAIwBpAG4AaQB0AGkAYQBsAGkAegBlACAAZgBvAHIAIABlAG0AcAB0AHkAIABmAGkAbABlACAAYwBhAHMAZQAKAH0ACgBwAHIAbwBjAGUAcwBzACAAewAKACQAYgB5AHQAZQBzACAAPQAgAFsAUwB5AHMAdABlAG0ALgBDAG8AbgB2AGUAcgB0AF0AOgA6AEYAcgBvAG0AQgBhAHMAZQA2ADQAUwB0AHIAaQBuAGcAKAAkAGkAbgBwAHUAdAApAAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0AQgBsAG8AYwBrACgAJABiAHkAdABlAHMALAAgADAALAAgACQAYgB5AHQAZQBzAC4ATABlAG4AZwB0AGgALAAgACQAYgB5AHQAZQBzACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABmAGQALgBXAHIAaQB0AGUAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAApAAoAfQAKAGUAbgBkACAAewAKACQAcwBoAGEAMQAuAFQAcgBhAG4AcwBmAG8AcgBtAEYAaQBuAGEAbABCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAMAApACAAfAAgAE8AdQB0AC0ATgB1AGwAbAAKACQAaABhAHMAaAAgAD0AIABbAFMAeQBzAHQAZQBtAC4AQgBpAHQAQwBvAG4AdgBlAHIAdABlAHIAXQA6ADoAVABvAFMAdAByAGkAbgBnACgAJABzAGgAYQAxAC4ASABhAHMAaAApAC4AUgBlAHAAbABhAGMAZQAoACIALQAiACwAIAAiACIAKQAuAFQAbwBMAG8AdwBlAHIASQBuAHYAYQByAGkAYQBuAHQAKAApAAoAJABmAGQALgBDAGwAbwBzAGUAKAApAAoAVwByAGkAdABlAC0ATwB1AHQAcAB1AHQAIAAiAHsAIgAiAHMAaABhADEAIgAiADoAIgAiACQAaABhAHMAaAAiACIAfQAiAAoAfQA=']
<computername.companyinteractive.com> WINRM PUT ""/tmp/tmpcEFZin"" to ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1"" (offset=10656 size=10656)
<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out ""{""sha1"":""7f5d2fe5184"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""sha1"":""7f5d2fe5184e28c170ff1de2a7b611897f896275""}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest
Try
{
& ""C:\Users\Administrator\AppData\Local\Temp\ansible-tmp-1456414326.6-190477404467970\file.ps1""
}
Catch
{
$_obj = @{ failed = $true }
If ($_.Exception.GetType)
{
$_obj.Add('msg', $_.Exception.Message)
}
Else
{
$_obj.Add('msg', $_.ToString())
}
If ($_.InvocationInfo.PositionMessage)
{
$_obj.Add('exception', $_.InvocationInfo.PositionMessage)
}
ElseIf ($_.ScriptStackTrace)
{
$_obj.Add('exception', $_.ScriptStackTrace)
}
Try
{
$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))
}
Catch
{
}
Echo $_obj | ConvertTo-Json -Compress -Depth 99
Exit 1
}
<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgBUAHIAeQAKAHsACgAmACAAIgBDADoAXABVAHMAZQByAHMAXABBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAFwAQQBwAHAARABhAHQAYQBcAEwAbwBjAGEAbABcAFQAZQBtAHAAXABhAG4AcwBpAGIAbABlAC0AdABtAHAALQAxADQANQA2ADQAMQA0ADMAMgA2AC4ANgAtADEAOQAwADQANwA3ADQAMAA0ADQANgA3ADkANwAwAFwAZgBpAGwAZQAuAHAAcwAxACIACgB9AAoAQwBhAHQAYwBoAAoAewAKACQAXwBvAGIAagAgAD0AIABAAHsAIABmAGEAaQBsAGUAZAAgAD0AIAAkAHQAcgB1AGUAIAB9AAoASQBmACAAKAAkAF8ALgBFAHgAYwBlAHAAdABpAG8AbgAuAEcAZQB0AFQAeQBwAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4ARQB4AGMAZQBwAHQAaQBvAG4ALgBNAGUAcwBzAGEAZwBlACkACgB9AAoARQBsAHMAZQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4AVABvAFMAdAByAGkAbgBnACgAKQApAAoAfQAKAEkAZgAgACgAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAZQB4AGMAZQBwAHQAaQBvAG4AJwAsACAAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAH0ACgBFAGwAcwBlAEkAZgAgACgAJABfAC4AUwBjAHIAaQBwAHQAUwB0AGEAYwBrAFQAcgBhAGMAZQApAAoAewAKACQAXwBvAGIAagAuAEEAZABkACgAJwBlAHgAYwBlAHAAdABpAG8AbgAnACwAIAAkAF8ALgBTAGMAcgBpAHAAdABTAHQAYQBjAGsAVAByAGEAYwBlACkACgB9AAoAVAByAHkACgB7AAoAJABfAG8AYgBqAC4AQQBkAGQAKAAnAGUAcgByAG8AcgBfAHIAZQBjAG8AcgBkACcALAAgACgAJABfACAAfAAgAEMAbwBuAHYAZQByAHQAVABvAC0ASgBzAG8AbgAgAHwAIABDAG8AbgB2AGUAcgB0AEYAcgBvAG0ALQBKAHMAbwBuACkAKQAKAH0ACgBDAGEAdABjAGgACgB7AAoAfQAKAEUAYwBoAG8AIAAkAF8AbwBiAGoAIAB8ACAAQwBvAG4AdgBlAHIAdABUAG8ALQBKAHMAbwBuACAALQBDAG8AbQBwAHIAZQBzAHMAIAAtAEQAZQBwAHQAaAAgADkAOQAKAEUAeABpAHQAIAAxAAoAfQA=']
<computername.companyinteractive.com> WINRM RESULT u'<Response code 1, out ""{""msg"":""path will no"", err """">'
<computername.companyinteractive.com> WINRM STDOUT {""msg"":""path will not be created"",""failed"":true}
<computername.companyinteractive.com> WINRM STDERR 
<computername.companyinteractive.com> WINRM CLOSE SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F
An exception occurred during task execution. The full traceback is:
Traceback (most recent call last):
  File ""/opt/ansible-src/lib/ansible/executor/task_executor.py"", line 122, in run
    res = self._execute()
  File ""/opt/ansible-src/lib/ansible/executor/task_executor.py"", line 418, in _execute
    result = self._handler.run(task_vars=variables)
  File ""/opt/ansible-src/lib/ansible/plugins/action/copy.py"", line 202, in run
    diffs.append(self._get_diff_data(dest_file, source_full, task_vars))
  File ""/opt/ansible-src/lib/ansible/plugins/action/__init__.py"", line 614, in _get_diff_data
    if peek_result['state'] == 'absent':
KeyError: 'state'

The problem appears to be because the --diff command is relying upon the existence of a 'state' key in the returned JSON. This appears to be a key added in the module_utils.basic Python module, specifically the add_path_info method, which seems to be called by load_common_file_arguments. I was going to just hack this into win_copy, but it seems to me that the most reasonable solution would be to add load_common_file_arguments to powershell_common.ps1. I'm more than happy to start work on that if the community agrees that is the best answer.",2
,1987,142,,,,,"Variable overding during nested includes issue

It seems variables are not overridden when using nested includes (this works ok in v1 ) .This might be related to   #11353 . To reproduce the issue you can follow the steps from #11353 ...
In build.yml I have a variable docker_tags what is overriden in  tasks/docker/base_build.yml... instead of printing the overridden value it prints the value from build.yml. See debug statements after running  'ansible-playbook -v build_admin_ui.yml -i inventories/local/hosts --extra-vars ""admin_ui_version=1234""'",3
,1987,148,,,,,"Variable overding during nested includes issue

It seems variables are not overridden when using nested includes (this works ok in v1 ) .This might be related to   #11353 . To reproduce the issue you can follow the steps from #11353 ...
In build.yml I have a variable docker_tags what is overriden in  tasks/docker/base_build.yml... instead of printing the overridden value it prints the value from build.yml. See debug statements after running  'ansible-playbook -v build_admin_ui.yml -i inventories/local/hosts --extra-vars ""admin_ui_version=1234""'",2
,1988,142,,,,,"[v2] ""win_iis_webbinding"" module require additional parameters to run?

I was run
ansible iis01 -m win_iis_webbinding -a ""name=tiger""
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: +     ~~~~~~~~~~~~~~~~~~~
iis01 | FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""Property 'host_header' cannot be found on this object. Make sure that it exists.""
}
I need run this,it's work
ansible iis01 -m win_iis_webbinding -a ""name=tiger host_header=www.tiger.com protocol=http port=8083 ip=127.0.0.1""
iis01 | SUCCESS => {
    ""added"": [], 
    ""changed"": false, 
    ""matched"": [], 
    ""parameters"": {
        ""HostHeader"": ""www.tiger.com"", 
        ""IPAddress"": ""127.0.0.1"", 
        ""Name"": ""tiger"", 
        ""Port"": ""8083"", 
        ""Protocol"": ""http""
    }, 
    ""removed"": []
}",3
,1988,148,,,,,"[v2] ""win_iis_webbinding"" module require additional parameters to run?

I was run
ansible iis01 -m win_iis_webbinding -a ""name=tiger""
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: +     ~~~~~~~~~~~~~~~~~~~
iis01 | FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""Property 'host_header' cannot be found on this object. Make sure that it exists.""
}
I need run this,it's work
ansible iis01 -m win_iis_webbinding -a ""name=tiger host_header=www.tiger.com protocol=http port=8083 ip=127.0.0.1""
iis01 | SUCCESS => {
    ""added"": [], 
    ""changed"": false, 
    ""matched"": [], 
    ""parameters"": {
        ""HostHeader"": ""www.tiger.com"", 
        ""IPAddress"": ""127.0.0.1"", 
        ""Name"": ""tiger"", 
        ""Port"": ""8083"", 
        ""Protocol"": ""http""
    }, 
    ""removed"": []
}",2
,1989,144,,,,,"ec2 dynamic_inventory tag issue

The special characters escape to underscores in ec2.py does not appear to work poperly.
I have the following tag for an ec2 instance:
aws:cloudformation:stack-name => imdev-flask-3-2-298-l1FiTlA
Per the docs, this should translate to an underscore between stack and name: 'ec2_tag_aws_cloudformation_stack_name'
This does not work, and produced the following var undefined error:
One or more undefined variables: 'ec2_tag_aws_cloudformation_stack_name' is undefined
running ec2.py, the actual output is ""ec2_tag_aws_cloudformation_stack-name"": ""imdev-flask-3-2-298-l1FiTlA""
If I try to use in the ec2.py form, ansible will throw an error for the invalid character:
Failed to template msg=""{{ ec2_tag_aws_cloudformation_stack-name == removed_version }}"": Unable to look up a name or access an attribute in template string. Make sure your variable name does not contain invalid characters like '-'.
However, accessing through the hostvars allows reference to the item with the ""-"". Ex:
""{{  hostvars[inventory_hostname]['ec2_tag_aws_cloudformation_stack-name'] }}""",3
,1989,148,,,,,"ec2 dynamic_inventory tag issue

The special characters escape to underscores in ec2.py does not appear to work poperly.
I have the following tag for an ec2 instance:
aws:cloudformation:stack-name => imdev-flask-3-2-298-l1FiTlA
Per the docs, this should translate to an underscore between stack and name: 'ec2_tag_aws_cloudformation_stack_name'
This does not work, and produced the following var undefined error:
One or more undefined variables: 'ec2_tag_aws_cloudformation_stack_name' is undefined
running ec2.py, the actual output is ""ec2_tag_aws_cloudformation_stack-name"": ""imdev-flask-3-2-298-l1FiTlA""
If I try to use in the ec2.py form, ansible will throw an error for the invalid character:
Failed to template msg=""{{ ec2_tag_aws_cloudformation_stack-name == removed_version }}"": Unable to look up a name or access an attribute in template string. Make sure your variable name does not contain invalid characters like '-'.
However, accessing through the hostvars allows reference to the item with the ""-"". Ex:
""{{  hostvars[inventory_hostname]['ec2_tag_aws_cloudformation_stack-name'] }}""",2
,1990,144,,,,,"documented used of 'lookup' now generates variable undefined error 

The following documented use of the lookup plugin

---
- hosts: all
  tasks:
     - debug: msg=""{{ lookup('env','HOME') }} is an environment variable""

now generates the following error:
TASK: [debug msg=""{{lookup('env','HOME')}} is an environment variable""] ******* 
fatal: [localhost] => One or more undefined variables: 'lookup' is undefined

I've traced this back to commit 5031104.",3
,1990,142,,,,,"documented used of 'lookup' now generates variable undefined error 

The following documented use of the lookup plugin

---
- hosts: all
  tasks:
     - debug: msg=""{{ lookup('env','HOME') }} is an environment variable""

now generates the following error:
TASK: [debug msg=""{{lookup('env','HOME')}} is an environment variable""] ******* 
fatal: [localhost] => One or more undefined variables: 'lookup' is undefined

I've traced this back to commit 5031104.",2
,1991,144,,,,,"Wrong diagnostics in ansible-galaxy on certificate issues

Issue Type:

Bug Report

Component Name

ansible-galaxy

Ansible Version:
evg@thinkpad ~ $ansible --version
ansible 2.2.2.0
  config file = /home/evg/.ansible.cfg
  configured module search path = Default w/o overrides

Ansible Configuration:
Clean out-of-box configuration (on SOME OS).
Environment:
N/A. Generic, non-mainstream OS. In my case, ALT Linux Sisypus (unstable branch).
Summary:
On non-mainstream OS with non-standard SSL/TLS CA certificate paths (not in '/etc/ssl/certs', '/etc/pki/ca-trust/extracted/pem', '/etc/pki/tls/certs', '/usr/share/ca-certificates/cacert.org', '/etc/ansible' in my case) diagnostics on any problem with certificate check in ansible-galaxy is just plain wrong (ERROR! Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED).
Steps To Reproduce:
Try to install any galaxy role just after clean ansible install (ansible-galaxy will fail to check correct SSL/TLS certificate of galaxy.ansible.com with wrong error message).
Expected Results:
evg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent 
- downloading role 'zabbix-agent', owned by dj-wasabi
- downloading role from https://github.com/dj-wasabi/ansible-zabbix-agent/archive/0.3.0.tar.gz
- extracting dj-wasabi.zabbix-agent to /home/evg/.ansible/roles/dj-wasabi.zabbix-agent
- dj-wasabi.zabbix-agent was installed successfully

Actual Results:
evg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent   
 [WARNING]: - dj-wasabi.zabbix-agent was NOT installed successfully: Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED

ERROR! - you can use --ignore-errors to skip failed roles and finish processing the list.

Sanity check:
evg@thinkpad ~ $curl https://galaxy.ansible.com/api/
{""available_versions"":{""v1"":""/api/v1/""},""description"":""GALAXY REST API"",""current_version"":""v1""}

Workarounds:
None. Old workaround for ansible-1.9.x NOT working anymore:
root@thinkpad /etc/ansible #ln -s /usr/share/ca-certificates/ca-bundle.crt .",3
,1991,140,,,,,"Wrong diagnostics in ansible-galaxy on certificate issues

Issue Type:

Bug Report

Component Name

ansible-galaxy

Ansible Version:
evg@thinkpad ~ $ansible --version
ansible 2.2.2.0
  config file = /home/evg/.ansible.cfg
  configured module search path = Default w/o overrides

Ansible Configuration:
Clean out-of-box configuration (on SOME OS).
Environment:
N/A. Generic, non-mainstream OS. In my case, ALT Linux Sisypus (unstable branch).
Summary:
On non-mainstream OS with non-standard SSL/TLS CA certificate paths (not in '/etc/ssl/certs', '/etc/pki/ca-trust/extracted/pem', '/etc/pki/tls/certs', '/usr/share/ca-certificates/cacert.org', '/etc/ansible' in my case) diagnostics on any problem with certificate check in ansible-galaxy is just plain wrong (ERROR! Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED).
Steps To Reproduce:
Try to install any galaxy role just after clean ansible install (ansible-galaxy will fail to check correct SSL/TLS certificate of galaxy.ansible.com with wrong error message).
Expected Results:
evg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent 
- downloading role 'zabbix-agent', owned by dj-wasabi
- downloading role from https://github.com/dj-wasabi/ansible-zabbix-agent/archive/0.3.0.tar.gz
- extracting dj-wasabi.zabbix-agent to /home/evg/.ansible/roles/dj-wasabi.zabbix-agent
- dj-wasabi.zabbix-agent was installed successfully

Actual Results:
evg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent   
 [WARNING]: - dj-wasabi.zabbix-agent was NOT installed successfully: Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED

ERROR! - you can use --ignore-errors to skip failed roles and finish processing the list.

Sanity check:
evg@thinkpad ~ $curl https://galaxy.ansible.com/api/
{""available_versions"":{""v1"":""/api/v1/""},""description"":""GALAXY REST API"",""current_version"":""v1""}

Workarounds:
None. Old workaround for ansible-1.9.x NOT working anymore:
root@thinkpad /etc/ansible #ln -s /usr/share/ca-certificates/ca-bundle.crt .",2
,1992,144,,,,,"Trailing new lines aren't kept by default by template module

ISSUE TYPE

Bug Report

COMPONENT NAME
Jinja
ANSIBLE VERSION
ansible 2.2.0.0

CONFIGURATION
[defaults]
ask_pass = False
remote_user = root
inventory = inventories
timeout = 5
roles_path = roles

ansible_managed = This file is managed by DOCK42 Configuration Management, all manual changes will be lost.

[ssh_connection]
control_path = %(directory)s/%%h-%%p-%%r

OS / ENVIRONMENT

SUMMARY
(referring to commit 1998edd)
When trying to wrap a conditional inside a template in an one liner, the line break will be skipped if there's no trailing character (e.g. like a whitespace to workaround this issue)
STEPS TO REPRODUCE
Create a template like this:
# GSSAPI options
GSSAPIAuthentication {% if openssh.gssapi_authentication %}yes{% else %}no{% endif %}
GSSAPICleanupCredentials no
#GSSAPIStrictAcceptorCheck yes

 openssh.gssapi_authentication: no

- name: Configure sshd
  template: 
    src: sshd_config.j2
    dest: /etc/ssh/sshd_config
    backup: yes
    owner: root 
    group: root 
    mode: 0600
    validate: '/usr/sbin/sshd -T -f %s'
  notify:
    - restart sshd
  become: yes
  tags:
    - sshd_config
EXPECTED RESULTS
# GSSAPI options
GSSAPIAuthentication no 
GSSAPICleanupCredentials no

ACTUAL RESULTS
 # GSSAPI options
GSSAPIAuthentication noGSSAPICleanupCredentials no",3
,1992,148,,,,,"Trailing new lines aren't kept by default by template module

ISSUE TYPE

Bug Report

COMPONENT NAME
Jinja
ANSIBLE VERSION
ansible 2.2.0.0

CONFIGURATION
[defaults]
ask_pass = False
remote_user = root
inventory = inventories
timeout = 5
roles_path = roles

ansible_managed = This file is managed by DOCK42 Configuration Management, all manual changes will be lost.

[ssh_connection]
control_path = %(directory)s/%%h-%%p-%%r

OS / ENVIRONMENT

SUMMARY
(referring to commit 1998edd)
When trying to wrap a conditional inside a template in an one liner, the line break will be skipped if there's no trailing character (e.g. like a whitespace to workaround this issue)
STEPS TO REPRODUCE
Create a template like this:
# GSSAPI options
GSSAPIAuthentication {% if openssh.gssapi_authentication %}yes{% else %}no{% endif %}
GSSAPICleanupCredentials no
#GSSAPIStrictAcceptorCheck yes

 openssh.gssapi_authentication: no

- name: Configure sshd
  template: 
    src: sshd_config.j2
    dest: /etc/ssh/sshd_config
    backup: yes
    owner: root 
    group: root 
    mode: 0600
    validate: '/usr/sbin/sshd -T -f %s'
  notify:
    - restart sshd
  become: yes
  tags:
    - sshd_config
EXPECTED RESULTS
# GSSAPI options
GSSAPIAuthentication no 
GSSAPICleanupCredentials no

ACTUAL RESULTS
 # GSSAPI options
GSSAPIAuthentication noGSSAPICleanupCredentials no",2
,1993,144,,,,,"pre_tasks not working while gathering=smart in ansible.cfg

Issue Type:
Bug report
Ansible Version:
ansible 2.1.0 (devel 45e05ec072) last updated 2016/02/01 14:17:33 (GMT +200)
  lib/ansible/modules/core: (detached HEAD 88e0bfd75d) last updated 2015/11/20 12:34:36 (GMT +200)
  lib/ansible/modules/extras: (detached HEAD 7da1f8d4ca) last updated 2015/11/20 12:34:44 (GMT +200)
  config file = ansible.cfg
  configured module search path = Default w/o overrides

Ansible Configuration:
[defaults]
gathering = smart

Environment:
Mac OS X
Summary:
When gathering=smart used in ansible.cfg pre_tasks are not running.
Steps To Reproduce:
ansible.cfg:
[defaults]
gathering = smart

play.yml:

---
- hosts: localhost

  pre_tasks:
    - name: pre_task
      shell: echo 'hello'

  tasks:
    - name: task
      shell: echo 'still busy'

  post_tasks:
    - name: post_task
      shell: echo 'goodbye'

Expected Results:
PLAY ***************************************************************************

TASK [setup] *******************************************************************
ok: [localhost]

TASK [pre_task] ****************************************************************
changed: [localhost]

TASK [task] ********************************************************************
changed: [localhost]

TASK [post_task] ***************************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=4    changed=3    unreachable=0    failed=0

Actual Results:
PLAY ***************************************************************************

TASK [setup] *******************************************************************
ok: [localhost]

TASK [task] ********************************************************************
changed: [localhost]

TASK [post_task] ***************************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=3    changed=2    unreachable=0    failed=0",3
,1993,148,,,,,"pre_tasks not working while gathering=smart in ansible.cfg

Issue Type:
Bug report
Ansible Version:
ansible 2.1.0 (devel 45e05ec072) last updated 2016/02/01 14:17:33 (GMT +200)
  lib/ansible/modules/core: (detached HEAD 88e0bfd75d) last updated 2015/11/20 12:34:36 (GMT +200)
  lib/ansible/modules/extras: (detached HEAD 7da1f8d4ca) last updated 2015/11/20 12:34:44 (GMT +200)
  config file = ansible.cfg
  configured module search path = Default w/o overrides

Ansible Configuration:
[defaults]
gathering = smart

Environment:
Mac OS X
Summary:
When gathering=smart used in ansible.cfg pre_tasks are not running.
Steps To Reproduce:
ansible.cfg:
[defaults]
gathering = smart

play.yml:

---
- hosts: localhost

  pre_tasks:
    - name: pre_task
      shell: echo 'hello'

  tasks:
    - name: task
      shell: echo 'still busy'

  post_tasks:
    - name: post_task
      shell: echo 'goodbye'

Expected Results:
PLAY ***************************************************************************

TASK [setup] *******************************************************************
ok: [localhost]

TASK [pre_task] ****************************************************************
changed: [localhost]

TASK [task] ********************************************************************
changed: [localhost]

TASK [post_task] ***************************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=4    changed=3    unreachable=0    failed=0

Actual Results:
PLAY ***************************************************************************

TASK [setup] *******************************************************************
ok: [localhost]

TASK [task] ********************************************************************
changed: [localhost]

TASK [post_task] ***************************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=3    changed=2    unreachable=0    failed=0",2
,1994,144,,,,,"Issue with 'include' statements and role path

My ansible's 'include' statement was working fine, but recently after including ymls in subfolder, it somehow brokes the role's path.
Here's the tree of my project:
.
 site.yml
 inventory.ini
 roles
     webservers
         files
            crt.crt
         tasks
            main.yml
            httpd.yml
            dev
               httpd.yml
               main.yml
            prod
                httpd.yml
                main.yml
         templates
            httpd_conf.j2
         vars
             main.yml

site.yml:
 - name: Install base software
   sudo: yes
   vars:
      profile:    ""dev""
   roles:
      - webservers

roles/webservers/tasks/main.yml:
 - name: Install httpd
   include: httpd.yml

 - name: Including specific tasks
   include: ""{{ profile }}/main.yml"" 

Prior to this point, it works just fine. But the next step, after including the ""profile""/main.yml, brokes the role's path.
roles/webservers/tasks/dev/main.yml:
 - name:  Create httpd.conf from template
   template: src=httpd_conf.j2 dest=/etc/httpd/conf/httpd.conf
   with_items: webservers_httpd_vhosts

results an error:
TASK: [webservers | Create httpd.conf from template] *********************
fatal: [192.168.0.2] => input file not found at /home/me/git/repo/projects/ans/roles/webservers/tasks/templates/httpd_conf.j2 or /home/me/git/repo/projects/ans/httpd_conf.j2

FATAL: all hosts have already failed -- aborting

So, after this 'include', it somehow thinks that role's root is located in 'webservers/tasks/', instead of 'webservers/'. But in the same time, it sees the variables in 'webservers/vars/' path.
Moreover, it was working fine and recently just broke. I haven't updated ansible, nor edited it's parameters, just updated the playbook.
If I specify the 'src=../../templates/httpd_conf.j2' it works fine, but since this behaviour just appeared like from nowhere, I can't rely on this path.
I've managed to use this workaround: template: src=""{{ role_path }}/templates/httpd_conf.j2"" dest=/etc/httpd/conf/httpd.conf But that's a workaround. The reason of auto-pathing is broke is still undiscovered.
Ansilbe's version: 1.9.0.1",3
,1994,148,,,,,"Issue with 'include' statements and role path

My ansible's 'include' statement was working fine, but recently after including ymls in subfolder, it somehow brokes the role's path.
Here's the tree of my project:
.
 site.yml
 inventory.ini
 roles
     webservers
         files
            crt.crt
         tasks
            main.yml
            httpd.yml
            dev
               httpd.yml
               main.yml
            prod
                httpd.yml
                main.yml
         templates
            httpd_conf.j2
         vars
             main.yml

site.yml:
 - name: Install base software
   sudo: yes
   vars:
      profile:    ""dev""
   roles:
      - webservers

roles/webservers/tasks/main.yml:
 - name: Install httpd
   include: httpd.yml

 - name: Including specific tasks
   include: ""{{ profile }}/main.yml"" 

Prior to this point, it works just fine. But the next step, after including the ""profile""/main.yml, brokes the role's path.
roles/webservers/tasks/dev/main.yml:
 - name:  Create httpd.conf from template
   template: src=httpd_conf.j2 dest=/etc/httpd/conf/httpd.conf
   with_items: webservers_httpd_vhosts

results an error:
TASK: [webservers | Create httpd.conf from template] *********************
fatal: [192.168.0.2] => input file not found at /home/me/git/repo/projects/ans/roles/webservers/tasks/templates/httpd_conf.j2 or /home/me/git/repo/projects/ans/httpd_conf.j2

FATAL: all hosts have already failed -- aborting

So, after this 'include', it somehow thinks that role's root is located in 'webservers/tasks/', instead of 'webservers/'. But in the same time, it sees the variables in 'webservers/vars/' path.
Moreover, it was working fine and recently just broke. I haven't updated ansible, nor edited it's parameters, just updated the playbook.
If I specify the 'src=../../templates/httpd_conf.j2' it works fine, but since this behaviour just appeared like from nowhere, I can't rely on this path.
I've managed to use this workaround: template: src=""{{ role_path }}/templates/httpd_conf.j2"" dest=/etc/httpd/conf/httpd.conf But that's a workaround. The reason of auto-pathing is broke is still undiscovered.
Ansilbe's version: 1.9.0.1",2
,1995,144,,,,,"Ansible 2.0.0.2 : ""ERROR! file or module does not exist"" while running a playbook with script module

After installing ansible 2.0.0.2 from rpm:
[root@ansibletest setup]# ansible-playbook --version
ansible-playbook 2.0.0.2
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

I get the below error on running the playbook which uses the script module:
[root@ansibletest setup]# ansible-playbook test.yml

PLAY ***************************************************************************

TASK [setup] *******************************************************************
Enter passphrase for key '/root/.ssh/id_rsa':
ok: [192.168.2.101]

TASK [Run test.sh script] ****************************************************
fatal: [192.168.2.101]: FAILED! => {""failed"": true, ""msg"": ""ERROR! file or module does not exist: /path/to/script/test.sh""}

PLAY RECAP *********************************************************************
192.168.2.101              : ok=1    changed=0    unreachable=0    failed=1

The Playbook in question: -

---
- hosts: 192.168.2.101
  remote_user: root
  vars:
    var1: data
    param1: 1
  tasks:
    - name: Run test.sh script
      become: yes
      script: /{{ var1 }}/path/to/test.sh {{ param1 }} creates=/{{ var1 }}/{{ param1 }}/executed.txt",3
,1995,142,,,,,"Ansible 2.0.0.2 : ""ERROR! file or module does not exist"" while running a playbook with script module

After installing ansible 2.0.0.2 from rpm:
[root@ansibletest setup]# ansible-playbook --version
ansible-playbook 2.0.0.2
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

I get the below error on running the playbook which uses the script module:
[root@ansibletest setup]# ansible-playbook test.yml

PLAY ***************************************************************************

TASK [setup] *******************************************************************
Enter passphrase for key '/root/.ssh/id_rsa':
ok: [192.168.2.101]

TASK [Run test.sh script] ****************************************************
fatal: [192.168.2.101]: FAILED! => {""failed"": true, ""msg"": ""ERROR! file or module does not exist: /path/to/script/test.sh""}

PLAY RECAP *********************************************************************
192.168.2.101              : ok=1    changed=0    unreachable=0    failed=1

The Playbook in question: -

---
- hosts: 192.168.2.101
  remote_user: root
  vars:
    var1: data
    param1: 1
  tasks:
    - name: Run test.sh script
      become: yes
      script: /{{ var1 }}/path/to/test.sh {{ param1 }} creates=/{{ var1 }}/{{ param1 }}/executed.txt",2
,1996,148,,,,,"Variable interpolation in hostvars

Issue Type: Feature Idea
Ansible Version: ansible 1.6.6
Environment: N/A
Summary:
When variables are accessed through hostvars, jinja2 expressions inside those variables should be interpolated.
There was discussion about this issue on ansible-project mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/ansible-project/pdWFR2Q8U8E/m6sgB1C5K1YJ
And here is a filter that shows simple implementation (but might be pretty slow, and I think it should be in hostvars not a filter): https://groups.google.com/group/ansible-project/attach/16318558e74073ae/hostvars.py?part=0.1&view=1
Steps To Reproduce:
Given vars like:
foo: y
bar: ""x is {{ foo }}""

The values for both {{ bar }} and {{ hostvars[some_host].bar }} should be ""x is y"".
Currently the value of {{ hostvars[some_host].bar }} would be (literally) ""x is {{ foo }}"".",3
,1996,143,,,,,"Variable interpolation in hostvars

Issue Type: Feature Idea
Ansible Version: ansible 1.6.6
Environment: N/A
Summary:
When variables are accessed through hostvars, jinja2 expressions inside those variables should be interpolated.
There was discussion about this issue on ansible-project mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/ansible-project/pdWFR2Q8U8E/m6sgB1C5K1YJ
And here is a filter that shows simple implementation (but might be pretty slow, and I think it should be in hostvars not a filter): https://groups.google.com/group/ansible-project/attach/16318558e74073ae/hostvars.py?part=0.1&view=1
Steps To Reproduce:
Given vars like:
foo: y
bar: ""x is {{ foo }}""

The values for both {{ bar }} and {{ hostvars[some_host].bar }} should be ""x is y"".
Currently the value of {{ hostvars[some_host].bar }} would be (literally) ""x is {{ foo }}"".",2
,1997,142,,,,,"win_chocolatey installing powershell when powershell4 is installed and state: latest

ISSUE TYPE


Bug Report

COMPONENT NAME
win_chocolatey
ANSIBLE VERSION

$ ansible --version
ansible 2.4.0 (devel cc50b803df) last updated 2017/03/22 14:15:18 (GMT -500)
  config file =
  configured module search path = Default w/o overrides
  python version = 2.7.10 (default, Jul 30 2016, 19:40:32) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]

CONFIGURATION
Ansible configuration from git clone
OS / ENVIRONMENT
Control host macOS 10.12.3
Managed host Windows 7
SUMMARY
Attempting to upgrade powershell4 to powershell5 using the win_chocolatey module with state: latest
Might be related to #21873
Related to #22892
STEPS TO REPRODUCE

Have chocolatey installed
Install powershell4


- name: install powershell 
  win_chocolatey:
    name: ""{{ item }}""
    state: latest
  with_items:
    - ""powershell""
  register: check_powershell5
EXPECTED RESULTS
Powershell4 would be upgraded to Powershell5
ACTUAL RESULTS
https://gist.github.com/basictheprogram/74b48320e386d308a69f82f7c90019b5
choco_summary.log
https://gist.github.com/basictheprogram/de146516fe292a43630c10e7df205f38
chocolatey.log
https://gist.github.com/basictheprogram/5cc240c1f89fba6ef7ac769ff5e41a01",3
,1997,148,,,,,"win_chocolatey installing powershell when powershell4 is installed and state: latest

ISSUE TYPE


Bug Report

COMPONENT NAME
win_chocolatey
ANSIBLE VERSION

$ ansible --version
ansible 2.4.0 (devel cc50b803df) last updated 2017/03/22 14:15:18 (GMT -500)
  config file =
  configured module search path = Default w/o overrides
  python version = 2.7.10 (default, Jul 30 2016, 19:40:32) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]

CONFIGURATION
Ansible configuration from git clone
OS / ENVIRONMENT
Control host macOS 10.12.3
Managed host Windows 7
SUMMARY
Attempting to upgrade powershell4 to powershell5 using the win_chocolatey module with state: latest
Might be related to #21873
Related to #22892
STEPS TO REPRODUCE

Have chocolatey installed
Install powershell4


- name: install powershell 
  win_chocolatey:
    name: ""{{ item }}""
    state: latest
  with_items:
    - ""powershell""
  register: check_powershell5
EXPECTED RESULTS
Powershell4 would be upgraded to Powershell5
ACTUAL RESULTS
https://gist.github.com/basictheprogram/74b48320e386d308a69f82f7c90019b5
choco_summary.log
https://gist.github.com/basictheprogram/de146516fe292a43630c10e7df205f38
chocolatey.log
https://gist.github.com/basictheprogram/5cc240c1f89fba6ef7ac769ff5e41a01",2
,1998,144,,,,,"Docker module: argument memory_limit is of type <type 'str'> and we were unable to convert to int"" on  Ansible 2.0.2.0-1.el7

ISSUE TYPE


Bug Report

ANSIBLE VERSION

ansible 2.0.2.0

OS / ENVIRONMENT
Centos 7
SUMMARY
After upgrade to ansible 2.0.2.0 it's not possible to enter memory_limit as human readable string (ie. 265MB) only bytes are accepted.
STEPS TO REPRODUCE
try set memory_limit:  256MB

- name: sphinx container
  docker:
        name: sphinx
        image: michalzubkowicz/docker-sphinxsearch
        state: started
        restart_policy: always
        memory_limit: 256MB


EXPECTED RESULTS

Should accept string as early versions
ACTUAL RESULTS

Is showing error
argument memory_limit is of type <type 'str'> and we were unable to convert to int",3
,1998,146,,,,,"Docker module: argument memory_limit is of type <type 'str'> and we were unable to convert to int"" on  Ansible 2.0.2.0-1.el7

ISSUE TYPE


Bug Report

ANSIBLE VERSION

ansible 2.0.2.0

OS / ENVIRONMENT
Centos 7
SUMMARY
After upgrade to ansible 2.0.2.0 it's not possible to enter memory_limit as human readable string (ie. 265MB) only bytes are accepted.
STEPS TO REPRODUCE
try set memory_limit:  256MB

- name: sphinx container
  docker:
        name: sphinx
        image: michalzubkowicz/docker-sphinxsearch
        state: started
        restart_policy: always
        memory_limit: 256MB


EXPECTED RESULTS

Should accept string as early versions
ACTUAL RESULTS

Is showing error
argument memory_limit is of type <type 'str'> and we were unable to convert to int",2
,1999,148,,,,,"openbsd_pkg: add support for pkgname%branch syntax

ISSUE TYPE


Bug Report

COMPONENT NAME
openbsd_pkg
ANSIBLE VERSION
ansible 2.3.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Jun 17 2017, 15:16:02) [GCC 4.2.1 20070719 ]


CONFIGURATION
OS / ENVIRONMENT
OpenBSD -current or 6.1
SUMMARY
Trying to use the pkgname%branch syntax fails
STEPS TO REPRODUCE


ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'


EXPECTED RESULTS
installing openldap-server 2.44 from the databases/openldap branch. works fine in a shell:
#pkg_add -n openldap-server--%openldap
quirks-2.304 signed on 2017-04-02T15:01:33Z
openldap-server-2.4.44p3:icu4c-58.2p0: ok
openldap-server-2.4.44p3:openldap-client-2.4.44p3: ok
openldap-server-2.4.44p3:db-4.6.21p3v0: ok
openldap-server-2.4.44p3:e2fsprogs-1.42.12p4: ok
openldap-server-2.4.44p3: ok


ACTUAL RESULTS

The openbsd_pkg module errors out.

$ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'

localhost | FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to parse package name at versionless_match: openldap-server--%openldap""
}",3
,1999,148,,,,,"openbsd_pkg: add support for pkgname%branch syntax

ISSUE TYPE


Bug Report

COMPONENT NAME
openbsd_pkg
ANSIBLE VERSION
ansible 2.3.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Jun 17 2017, 15:16:02) [GCC 4.2.1 20070719 ]


CONFIGURATION
OS / ENVIRONMENT
OpenBSD -current or 6.1
SUMMARY
Trying to use the pkgname%branch syntax fails
STEPS TO REPRODUCE


ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'


EXPECTED RESULTS
installing openldap-server 2.44 from the databases/openldap branch. works fine in a shell:
#pkg_add -n openldap-server--%openldap
quirks-2.304 signed on 2017-04-02T15:01:33Z
openldap-server-2.4.44p3:icu4c-58.2p0: ok
openldap-server-2.4.44p3:openldap-client-2.4.44p3: ok
openldap-server-2.4.44p3:db-4.6.21p3v0: ok
openldap-server-2.4.44p3:e2fsprogs-1.42.12p4: ok
openldap-server-2.4.44p3: ok


ACTUAL RESULTS

The openbsd_pkg module errors out.

$ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'

localhost | FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to parse package name at versionless_match: openldap-server--%openldap""
}",2
,2000,148,,,,,"Error with paramiko 2.1.0 <--> 2.2.1: Unicode-objects must be encoded before hashing

ISSUE TYPE

Bug Report

COMPONENT NAME
ios_command
but I suspect all commands are impacted
ANSIBLE VERSION
Both latest stable & unstable versions:
ansible 2.3.1.0 (detached HEAD ecb38fdf73) last updated 2017/06/27 16:57:40 (GMT +200)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']
  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]

ansible 2.4.0 (devel 9f7fcf15be) last updated 2017/06/27 16:41:44 (GMT +200)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']
  ansible python module location = /home/actionmystique/src/Ansible/git-ansible/lib/ansible
  executable location = /home/actionmystique/src/Ansible/git-ansible/bin/ansible
  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]

CONFIGURATION
inventory   = ./hosts
library        = /home/actionmystique/Ansible/git-yang-networkop/ansible-101/library
forks = 1000
gathering = explicit
gather_timeout = 30
roles_path = /home/actionmystique/Ansible/Roles/roles
private_role_vars = yes
hash_behaviour = merge
log_path = /var/log/ansible.log
retry_files_enabled = False
show_custom_stats = True
timeout = 60
pipelining = True
connect_timeout = 60
connect_retries = 30
connect_interval = 1
OS / ENVIRONMENT

host: Ubuntu 17.04 4.10
target:

IOS-XEv 16.4.1


paramiko: 2.2.1

SUMMARY
cf. title, for instance with a show running-config command.
I am able to manually ssh into the remote device.
Is the paramiko version too recent?
STEPS TO REPRODUCE
Structure passed as ""provider"": connections.ssh
connections
...
        ssh:
          transport: cli 
          host: ""{{ ansible_host }}""
          # ansible_port
          port: 22
          # ansible_user
          username: admin
          # ansible_ssh_pass
          password: xxxxxxxxxxx
          authorize: yes
          # enable_secret_password
          auth_pass: xxxxxxxxxxx
          # private_key_file
          ssh_keyfile: ""~/.ssh/id_rsa""
          version: 2
          timeout: 10

Role: ios_pull_config:
- include_vars: ""../defaults/{{ os_family }}/connections.yml""
  when: (connections is undefined)

- name: Fetching config from the remote node
  ios_command:
        provider: ""{{ connections.ssh }}""
        commands:
          - ""show {{ config }}""
  register: configuration

Playbook:
- name: Pulling IOS/IOSv/IOSv-L2/IOS-XE/IOS-XEv (CSR-1000v) startup and running configs
  hosts:
    - iosv
    - iosv_l2
    - ios_xev
  gather_facts: no
  roles:
    - { role: ios_pull_config, config: startup-config, with_date_time: 'no' }
    - { role: ios_pull_config, config: running-config, with_date_time: 'no' }

EXPECTED RESULTS
Startup & running configurations from the target IOSv node.
ACTUAL RESULTS: CLI
...
<172.21.100.111> using connection plugin network_cli
<172.21.100.111> socket_path: 
fatal: [XEv_Spine_11]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell""
}
...

ACTUAL RESULTS: log
...
2017-06-27 16:48:19,004 p=27279 u=root |  task path: /home/actionmystique/Ansible/Roles/roles/ios_pull_config/tasks/main.yml:77
2017-06-27 16:48:21,476 p=27374 u=root |  creating new control socket for host 172.21.100.111:22 as user admin
2017-06-27 16:48:21,476 p=27374 u=root |  control socket path is /root/.ansible/pc/4746f9877e
2017-06-27 16:48:21,477 p=27374 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible
2017-06-27 16:48:21,477 p=27374 u=root |  using connection plugin network_cli
2017-06-27 16:48:21,495 p=27375 u=root |  creating new control socket for host 172.21.100.112:22 as user admin
2017-06-27 16:48:21,496 p=27375 u=root |  control socket path is /root/.ansible/pc/96255242f1
2017-06-27 16:48:21,496 p=27375 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible
2017-06-27 16:48:21,496 p=27375 u=root |  using connection plugin network_cli
2017-06-27 16:48:21,585 paramiko.transport starting thread (client mode): 0x9f70b2d0L
2017-06-27 16:48:21,586 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1
2017-06-27 16:48:21,586 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25
2017-06-27 16:48:21,586 paramiko.transport Connected (version 2.0, client Cisco-1.25)
2017-06-27 16:48:21,587 paramiko.transport starting thread (client mode): 0xb9ea82d0L
2017-06-27 16:48:21,587 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False
2017-06-27 16:48:21,587 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1
2017-06-27 16:48:21,587 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1
2017-06-27 16:48:21,587 paramiko.transport HostKey agreed: ssh-rsa
2017-06-27 16:48:21,587 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25
2017-06-27 16:48:21,587 paramiko.transport Cipher agreed: aes128-ctr
2017-06-27 16:48:21,587 paramiko.transport MAC agreed: hmac-sha1
2017-06-27 16:48:21,587 paramiko.transport Connected (version 2.0, client Cisco-1.25)
2017-06-27 16:48:21,587 paramiko.transport Compression agreed: none
2017-06-27 16:48:21,588 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False
2017-06-27 16:48:21,588 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1
2017-06-27 16:48:21,589 paramiko.transport HostKey agreed: ssh-rsa
2017-06-27 16:48:21,589 paramiko.transport Cipher agreed: aes128-ctr
2017-06-27 16:48:21,589 paramiko.transport MAC agreed: hmac-sha1
2017-06-27 16:48:21,589 paramiko.transport Compression agreed: none
2017-06-27 16:48:21,789 paramiko.transport Got server p (2048 bits)
2017-06-27 16:48:21,790 paramiko.transport Got server p (2048 bits)
2017-06-27 16:48:21,884 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>
2017-06-27 16:48:21,885 paramiko.transport Switch to new keys ...
2017-06-27 16:48:21,890 p=27374 u=root |  connecting to host 172.21.100.111 returned an error
2017-06-27 16:48:21,890 p=27374 u=root |  Unicode-objects must be encoded before hashing
2017-06-27 16:48:21,911 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>
2017-06-27 16:48:21,912 paramiko.transport Switch to new keys ...
2017-06-27 16:48:21,915 p=27375 u=root |  connecting to host 172.21.100.112 returned an error
2017-06-27 16:48:21,915 p=27375 u=root |  Unicode-objects must be encoded before hashing
2017-06-27 16:48:21,986 paramiko.transport EOF in transport thread
2017-06-27 16:48:22,012 paramiko.transport EOF in transport thread
2017-06-27 16:48:31,524 p=27279 u=root |  fatal: [XEv_Spine_11]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell""
}
...

ACTUAL RESULTS: Manual SSH
# ssh admin@172.21.100.111
CC
**************************************************************************
* IOSv is strictly limited to use for evaluation, demonstration and IOS  *
* education. IOSv is provided as-is and is not supported by Cisco's      *
* Technical Advisory Center. Any use or disclosure, in whole or in part, *
* of the IOSv Software or Documentation to any third party for any       *
* purposes is expressly prohibited except as otherwise authorized by     *
* Cisco in writing.                                                      *
**************************************************************************CC
XEv_Spine_11# #sh run
Building configuration...
Current configuration : 12818 bytes
!
! Last configuration change at 14:35:32 UTC Tue Jun 27 2017
!
version 16.4
...

WORKAROUND: older paramiko
The latest paramiko version is 2.2.1.
When downgrading to paramiko 2.0.6, this issue is ... gone.
I recommend to put a more stringent requirement on paramiko in requirements.txt until this issue is solved with more recent paramiko, for instance:
paramiko <= 2.0.6

The issue begins with paramiko 2.1.0+",3
,2000,148,,,,,"Error with paramiko 2.1.0 <--> 2.2.1: Unicode-objects must be encoded before hashing

ISSUE TYPE

Bug Report

COMPONENT NAME
ios_command
but I suspect all commands are impacted
ANSIBLE VERSION
Both latest stable & unstable versions:
ansible 2.3.1.0 (detached HEAD ecb38fdf73) last updated 2017/06/27 16:57:40 (GMT +200)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']
  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]

ansible 2.4.0 (devel 9f7fcf15be) last updated 2017/06/27 16:41:44 (GMT +200)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']
  ansible python module location = /home/actionmystique/src/Ansible/git-ansible/lib/ansible
  executable location = /home/actionmystique/src/Ansible/git-ansible/bin/ansible
  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]

CONFIGURATION
inventory   = ./hosts
library        = /home/actionmystique/Ansible/git-yang-networkop/ansible-101/library
forks = 1000
gathering = explicit
gather_timeout = 30
roles_path = /home/actionmystique/Ansible/Roles/roles
private_role_vars = yes
hash_behaviour = merge
log_path = /var/log/ansible.log
retry_files_enabled = False
show_custom_stats = True
timeout = 60
pipelining = True
connect_timeout = 60
connect_retries = 30
connect_interval = 1
OS / ENVIRONMENT

host: Ubuntu 17.04 4.10
target:

IOS-XEv 16.4.1


paramiko: 2.2.1

SUMMARY
cf. title, for instance with a show running-config command.
I am able to manually ssh into the remote device.
Is the paramiko version too recent?
STEPS TO REPRODUCE
Structure passed as ""provider"": connections.ssh
connections
...
        ssh:
          transport: cli 
          host: ""{{ ansible_host }}""
          # ansible_port
          port: 22
          # ansible_user
          username: admin
          # ansible_ssh_pass
          password: xxxxxxxxxxx
          authorize: yes
          # enable_secret_password
          auth_pass: xxxxxxxxxxx
          # private_key_file
          ssh_keyfile: ""~/.ssh/id_rsa""
          version: 2
          timeout: 10

Role: ios_pull_config:
- include_vars: ""../defaults/{{ os_family }}/connections.yml""
  when: (connections is undefined)

- name: Fetching config from the remote node
  ios_command:
        provider: ""{{ connections.ssh }}""
        commands:
          - ""show {{ config }}""
  register: configuration

Playbook:
- name: Pulling IOS/IOSv/IOSv-L2/IOS-XE/IOS-XEv (CSR-1000v) startup and running configs
  hosts:
    - iosv
    - iosv_l2
    - ios_xev
  gather_facts: no
  roles:
    - { role: ios_pull_config, config: startup-config, with_date_time: 'no' }
    - { role: ios_pull_config, config: running-config, with_date_time: 'no' }

EXPECTED RESULTS
Startup & running configurations from the target IOSv node.
ACTUAL RESULTS: CLI
...
<172.21.100.111> using connection plugin network_cli
<172.21.100.111> socket_path: 
fatal: [XEv_Spine_11]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell""
}
...

ACTUAL RESULTS: log
...
2017-06-27 16:48:19,004 p=27279 u=root |  task path: /home/actionmystique/Ansible/Roles/roles/ios_pull_config/tasks/main.yml:77
2017-06-27 16:48:21,476 p=27374 u=root |  creating new control socket for host 172.21.100.111:22 as user admin
2017-06-27 16:48:21,476 p=27374 u=root |  control socket path is /root/.ansible/pc/4746f9877e
2017-06-27 16:48:21,477 p=27374 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible
2017-06-27 16:48:21,477 p=27374 u=root |  using connection plugin network_cli
2017-06-27 16:48:21,495 p=27375 u=root |  creating new control socket for host 172.21.100.112:22 as user admin
2017-06-27 16:48:21,496 p=27375 u=root |  control socket path is /root/.ansible/pc/96255242f1
2017-06-27 16:48:21,496 p=27375 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible
2017-06-27 16:48:21,496 p=27375 u=root |  using connection plugin network_cli
2017-06-27 16:48:21,585 paramiko.transport starting thread (client mode): 0x9f70b2d0L
2017-06-27 16:48:21,586 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1
2017-06-27 16:48:21,586 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25
2017-06-27 16:48:21,586 paramiko.transport Connected (version 2.0, client Cisco-1.25)
2017-06-27 16:48:21,587 paramiko.transport starting thread (client mode): 0xb9ea82d0L
2017-06-27 16:48:21,587 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False
2017-06-27 16:48:21,587 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1
2017-06-27 16:48:21,587 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1
2017-06-27 16:48:21,587 paramiko.transport HostKey agreed: ssh-rsa
2017-06-27 16:48:21,587 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25
2017-06-27 16:48:21,587 paramiko.transport Cipher agreed: aes128-ctr
2017-06-27 16:48:21,587 paramiko.transport MAC agreed: hmac-sha1
2017-06-27 16:48:21,587 paramiko.transport Connected (version 2.0, client Cisco-1.25)
2017-06-27 16:48:21,587 paramiko.transport Compression agreed: none
2017-06-27 16:48:21,588 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False
2017-06-27 16:48:21,588 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1
2017-06-27 16:48:21,589 paramiko.transport HostKey agreed: ssh-rsa
2017-06-27 16:48:21,589 paramiko.transport Cipher agreed: aes128-ctr
2017-06-27 16:48:21,589 paramiko.transport MAC agreed: hmac-sha1
2017-06-27 16:48:21,589 paramiko.transport Compression agreed: none
2017-06-27 16:48:21,789 paramiko.transport Got server p (2048 bits)
2017-06-27 16:48:21,790 paramiko.transport Got server p (2048 bits)
2017-06-27 16:48:21,884 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>
2017-06-27 16:48:21,885 paramiko.transport Switch to new keys ...
2017-06-27 16:48:21,890 p=27374 u=root |  connecting to host 172.21.100.111 returned an error
2017-06-27 16:48:21,890 p=27374 u=root |  Unicode-objects must be encoded before hashing
2017-06-27 16:48:21,911 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>
2017-06-27 16:48:21,912 paramiko.transport Switch to new keys ...
2017-06-27 16:48:21,915 p=27375 u=root |  connecting to host 172.21.100.112 returned an error
2017-06-27 16:48:21,915 p=27375 u=root |  Unicode-objects must be encoded before hashing
2017-06-27 16:48:21,986 paramiko.transport EOF in transport thread
2017-06-27 16:48:22,012 paramiko.transport EOF in transport thread
2017-06-27 16:48:31,524 p=27279 u=root |  fatal: [XEv_Spine_11]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell""
}
...

ACTUAL RESULTS: Manual SSH
# ssh admin@172.21.100.111
CC
**************************************************************************
* IOSv is strictly limited to use for evaluation, demonstration and IOS  *
* education. IOSv is provided as-is and is not supported by Cisco's      *
* Technical Advisory Center. Any use or disclosure, in whole or in part, *
* of the IOSv Software or Documentation to any third party for any       *
* purposes is expressly prohibited except as otherwise authorized by     *
* Cisco in writing.                                                      *
**************************************************************************CC
XEv_Spine_11# #sh run
Building configuration...
Current configuration : 12818 bytes
!
! Last configuration change at 14:35:32 UTC Tue Jun 27 2017
!
version 16.4
...

WORKAROUND: older paramiko
The latest paramiko version is 2.2.1.
When downgrading to paramiko 2.0.6, this issue is ... gone.
I recommend to put a more stringent requirement on paramiko in requirements.txt until this issue is solved with more recent paramiko, for instance:
paramiko <= 2.0.6

The issue begins with paramiko 2.1.0+",2
,2001,148,,,,,"[modules-core: file] Allow for ""directory"" or ""link"" in place of ""file"" with state 

Feature Idea

ANSIBLE VERSION
ansible 2.0.2.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

SUMMARY
using ""file"" to check for directories (or links) is not very intuitive.
It would be great if
directory: path=x/y/z

could be used as an alias for
file: path=x/y/z state=directory",3
,2001,146,,,,,"[modules-core: file] Allow for ""directory"" or ""link"" in place of ""file"" with state 

Feature Idea

ANSIBLE VERSION
ansible 2.0.2.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

SUMMARY
using ""file"" to check for directories (or links) is not very intuitive.
It would be great if
directory: path=x/y/z

could be used as an alias for
file: path=x/y/z state=directory",2
,2002,144,,,,,"GATHERING FACTS fails

$ ansible --version
ansible 1.9.4

$ uname -a
Linux psi-nb 4.3.3-2-ARCH #1 SMP PREEMPT Wed Dec 23 20:09:18 CET 2015 x86_64 GNU/Linux

Guest is CentOS 7 Vagrant BOX
$ uname -a
Linux jenkins 4.4.0-1.el7.elrepo.x86_64 #1 SMP Sun Jan 10 21:17:16 EST 2016 x86_64 x86_64 x86_64 GNU/Linux

When i run vagrant provision or ansible-playbook ... gathering facts fails MOST time:
PLAY [all] ******************************************************************** 

GATHERING FACTS *************************************************************** 
<127.0.0.1> ESTABLISH CONNECTION FOR USER: vagrant
<127.0.0.1> REMOTE_MODULE setup
<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r"" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=""/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && chmod a+rx $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && echo $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191'                                                                                      
<127.0.0.1> PUT /tmp/tmpOf1gPr TO /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup
<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r"" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=""/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key"" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'LANG=C LC_CTYPE=C /usr/bin/python /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup; rm -rf /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/ >/dev/null 2>&1'
failed: [jenkins] => {""failed"": true, ""parsed"": false}
{""verbose_override"": true, ""changed"": false, ""ansible_facts"": {""ansible_product_serial"": ""NA"", ""ansible_form_factor"": ""Other"", ""ansible_product_version"": ""1.2"", ""ansible_fips"": false, ""ansible_swaptotal_mb"": 1535, ""ansible_user_id"": ""vagrant"", ""module_setup"": true, ""ansible_userspace_bits"": ""64"", ""ansible_architecture"": ""x86_64"", ""ansible_distribution_version"": ""7.1.1503"", ""ansible_domain"": ""localdomain"", ""ansible_date_time"": {""tz"": ""EST"", ""hour"": ""08"", ""time"": ""08:08:47"", ""epoch"": ""1452776927"", ""month"": ""01"", ""tz_offset"": ""-0500"", ""second"": ""47"", ""iso8601_micro"": ""2016-01-14T13:08:47.283819Z"", ""weekday"": ""Thursday"", ""year"": ""2016"", ""date"": ""2016-01-14"", ""iso8601"": ""2016-01-14T13:08:47Z"", ""day"": ""14"", ""minute"": ""08""}, ""ansible_python_version"": ""2.7.5"", ""ansible_processor_cores"": 1, ""ansible_virtualization_role"": ""guest"", ""ansible_env"": {""LANG"": ""C"", ""TERM"": ""xterm-256color"", ""SHELL"": ""/bin/bash"", ""XDG_RUNTIME_DIR"": ""/run/user/1000"", ""SHLVL"": ""2"", ""SSH_TTY"": ""/dev/pts/1"", ""LC_CTYPE"": ""C"", ""LESSOPEN"": ""||/usr/bin/lesspipe.sh %s"", ""PATH"": ""/usr/local/bin:/usr/bin"", ""PWD"": ""/home/vagrant"", ""LOGNAME"": ""vagrant"", ""USER"": ""vagrant"", ""MAIL"": ""/var/mail/vagrant"", ""HOME"": ""/home/vagrant"", ""SSH_CONNECTION"": ""10.0.2.2 41752 10.0.2.15 22"", ""XDG_SESSION_ID"": ""9"", ""SSH_CLIENT"": ""10.0.2.2 41752 22"", ""_"": ""/usr/bin/python""}, ""ansible_processor_vcpus"": 1, ""ansible_docker0"": {""macaddress"": ""02:42:33:3c:c5:e0"", ""interfaces"": [], ""mtu"": 1500, ""active"": false, ""promisc"": false, ""stp"": false, ""ipv4"": {""netmask"": ""255.255.0.0"", ""network"": ""172.17.0.0"", ""address"": ""172.17.0.1""}, ""device"": ""docker0"", ""type"": ""bridge"", ""id"": ""8000.0242333cc5e0""}, ""ansible_bios_version"": ""VirtualBox"", ""ansible_processor"": [""GenuineIntel"", ""Intel(R) Core(TM) i5-5200U CPU @ 2.20GHz""], ""ansible_virtualization_type"": ""virtualbox"", ""ansible_lo"": {""mtu"": 65536, ""active"": true, ""promisc"": false, ""ipv4"": {""netmask"": ""255.0.0.0"", ""network"": ""127.0.0.0"", ""address"": ""127.0.0.1""}, ""ipv6"": [{""scope"": ""host"", ""prefix"": ""128"", ""address"": ""::1""}], ""device"": ""lo"", ""type"": ""loopback""}, ""ansible_memtotal_mb"": 2000, ""ansible_ssh_host_key_ecdsa_public"": ""AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBK5Jp5g/+qSXGzxlC6ZlEQfUpiAuZRcXv1gFPe9h1f9T/yVCBqZs2fM37/6cIljYzSwxiJVSveGiomGLxyba9G0="", ""ansible_default_ipv4"": {""macaddress"": ""52:54:00:d3:eb:a4"", ""network"": ""10.0.2.0"", ""mtu"": 1500, ""alias"": ""eth0"", ""netmask"": ""255.255.255.0"", ""address"": ""10.0.2.15"", ""interface"": ""eth0"", ""type"": ""ether"", ""gateway"": ""10.0.2.2""}, ""ansible_swapfree_mb"": 1535, ""ansible_default_ipv6"": {}, ""ansible_distribution_release"": ""Core"", ""ansible_system_vendor"": ""innotek GmbH"", ""ansible_os_family"": ""RedHat"", ""ansible_cmdline"": {""LANG"": ""en_US.UTF-8"", ""systemd.debug"": true, ""BOOT_IMAGE"": ""/vmlinuz-4.4.0-1.el7.elrepo.x86_64"", ""biosdevname"": ""0"", ""quiet"": true, ""net.ifnames"": ""0"", ""rhgb"": true, ""rd.lvm.lv"": ""VolGroup00/LogVol00"", ""crashkernel"": ""auto"", ""console"": ""ttyS0,115200"", ""ro"": true, ""root"": ""/dev/mapper/VolGroup00-LogVol00""}, ""ansible_user_gid"": 1000, ""ansible_selinux"": {""status"": ""disabled""}, ""ansible_userspace_architecture"": ""x86_64"", ""ansible_product_uuid"": ""NA"", ""ansible_system"": ""Linux"", ""ansible_pkg_mgr"": ""yum"", ""ansible_memfree_mb"": 1333, ""ansible_devices"": {""sda"": {""scheduler_mode"": ""deadline"", ""rotational"": ""1"", ""vendor"": ""ATA"", ""sectors"": ""83886080"", ""host"": """", ""sectorsize"": ""512"", ""removable"": ""0"", ""support_discard"": ""0"", ""model"": ""VBOX HARDDISK"", ""size"": ""40.00 GB"", ""holders"": [], ""partitions"": {""sda2"": {""start"": ""4096"", ""sectorsize"": 512, ""sectors"": ""409600"", ""size"": ""200.00 MB""}, ""sda3"": {""start"": ""413696"", ""sectorsize"": 512, ""sectors"": ""83472384"", ""size"": ""39.80 GB""}, ""sda1"": {""start"": ""2048"", ""sectorsize"": 512, ""sectors"": ""2048"", ""size"": ""1.00 MB""}}}}, ""ansible_user_uid"": 1000, ""ansible_memory_mb"": {""real"": {""total"": 2000, ""free"": 1333, ""used"": 667}, ""swap"": {""cached"": 0, ""total"": 1535, ""used"": 0, ""free"": 1535}, ""nocache"": {""used"": 300, ""free"": 1700}}, ""ansible_distribution"": ""CentOS"", ""ansible_distribution_major_version"": ""7"", ""ansible_user_dir"": ""/home/vagrant"", ""ansible_processor_countOpenSSH_7.1p1, OpenSSL 1.0.2e 3 Dec 2015
debug1: Reading configuration data /home/psi/.ssh/config
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 20: Applying options for *
debug1: auto-mux: Trying existing master
debug1: mux_client_request_session: master session id: 2
Shared connection to 127.0.0.1 closed.

anyone konws why?
For me the output looks like incomplete JSON",3
,2003,140,,,,,"Shell with_items Prompts for ssh key password on second command

I have seen something when running the shell module along with a with_items where it now prompts for a key pass phrase. It runs the first command with no password prompt but then on the second one it prompts for a key password and fails the task:
-name: Shell commands
 shell: ""{{item}}""
 with_items:
  - echo ""test 1""
  - echo ""test 2""

When running the playbook you get prompted for a key passphrase:
Enter passphrase for key '/home/vagrant/.ssh/id_rsa':

This previously worked on the v2.0.0-0.3.beta1 tag but no longer works on later tags and on the dev branch.",3
,2004,140,,,,,"Add a way to make boto not verify SSL certs

Starting a year ago with version 2.6.0, boto began verifying servers' SSL certificates by default.  Since most Eucalyptus and Nova installs have self-signed certs, this breaks Ansible's AWS-related modules when they're pointed at one of those clouds using a relatively current version of boto.
They added a validate_certs=False arg to calls like boto.connect_ec2_endpoint one can use to disable this behavior, but right now Ansible doesn't have a way to trigger that.  Modules like ec2, ec2_elb, and so on would benefit from a parameter that lets one turn cert verification off when they need to talk to services with self-signed certs.
EPEL bug that triggered this report:  https://bugzilla.redhat.com/show_bug.cgi?id=1003105",3
,2005,142,,,,,"ansible-galaxy file parameter full path

Hi,
It would be nice when you setup a galaxy dependencies file like this:
jdauphant.nginx,v1.1.1,nginx

to be able to specify the name of the final module path. The goal is when i launch it like this:
ansible-galaxy install -r galaxy.txt -p roles

I get a folder 'roles/nginx' which is better than having 'roles/jdauphant.nginx'
Thanks",3
,2006,144,,,,,"JSON output gets truncated on spaces

Issue Type:
Bug report
Ansible Version:
 ansible --version
ansible 2.0.0
  configured module search path = ../library

and
 ansible --version
ansible 1.9.0.1
  configured module search path = ../library

Environment:
N/A
Summary:
copy: content=""{{ some_data|to_json }}\n"" dest=/tmp/wtf.yaml results in truncated json.
Steps To Reproduce:
Example playbook:
- hosts: web414
  vars:
    some_data:
      something:
        wow: ""hey ho""
  tasks:
    - name: write some json
      copy: content=""{{ some_data|to_json }}\n"" dest=/tmp/wtf.yaml
Resulting file:
web414 ~ # cat /tmp/wtf.yaml
""{""something"": {""wow"": ""heyweb414 ~ #

Expected Results:
File should not be truncated.
Actual Results:
File gets truncated.",3
,2007,140,,,,,"Misleading error when a file isn't found in a role

Issue Type:
Bug Report
Ansible Version:

ansible 2.0.0.2
config file = /home/user/ansible/ansible.cfg
configured module search path = Default w/o overrides

Ansible Configuration:
Clean.
Environment:
Ubuntu 14.04.
Summary:
When a copy or assemble operation can't access the src file in a role a misleading error suggests to look at the playbooks directory.
Steps To Reproduce:
user@server:~/ansible$ touch roles/test/files/snari
user@server:~/ansible$ cat playbooks/test.yml

---
- hosts: all
  roles:
    - test

user@server:~/ansible$ cat roles/test/tasks/main.yml

---
- copy:
    src: snari
    dest: /tmp/snari

user@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml

PLAY ***************************************************************************

TASK [test : copy] *************************************************************
ok: [server]

PLAY RECAP *********************************************************************
server                     : ok=1    changed=0    unreachable=0    failed=0

user@server:~/ansible$ mv roles/test/files/snari{,_}

Expected Results:
user@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml

PLAY ***************************************************************************

TASK [test : copy] *************************************************************
fatal: [server]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""could not find snari in the search path: /home/user/ansible/roles/test/files, /home/user/ansible/playbooks""}

PLAY RECAP *********************************************************************
server                     : ok=0    changed=0    unreachable=0    failed=1

Actual Results:
user@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml

PLAY ***************************************************************************

TASK [test : copy] *************************************************************
fatal: [server]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""could not find src=/home/user/ansible/playbooks/snari""}

PLAY RECAP *********************************************************************
server                     : ok=0    changed=0    unreachable=0    failed=1",3
,2008,142,,,,,"Copy module shows 'changed' in check mode

ISSUE TYPE

Bug Report

COMPONENT NAME
copy
ANSIBLE VERSION
2.3.0.0

CONFIGURATION
NA
OS / ENVIRONMENT
NA
SUMMARY
When using the copy module to copy a file into a directory on the target system, the check mode yields changed though there is no change on the target system. The problem only occurs if the path
is given without trailing slash.
STEPS TO REPRODUCE
Call in check mode after file exists:
- name: copy file
  copy:
    src=myfile
    dest=/path/to/folder
EXPECTED RESULTS
changed: false
ACTUAL RESULTS
changed: true
This effect does not occur if dest is given with a trailing slash:
- name: copy file
  copy:
    src=myfile
    dest=/path/to/folder/",3
,2009,144,,,,,"Ansible fails to load Yaml file with equal-sign as value

Ansible fails when loading a Yaml file like this:
--- 
  foo: 
    - =
The Yaml file is valid (see http://www.yamllint.com/) but ansible fails with ERROR: Syntax Error while loading YAML script ...
Putting quotes around the equal-sign works but i can't do that in my case as this file will be generated by a Yaml parser. So using a workaround is not an option.",3
,2010,142,,,,,"fedora 25 dnf installed packages not marked as user installed

ISSUE TYPE


Bug Report

COMPONENT NAME

dnf
ANSIBLE VERSION

ansible 2.2.1.0
  config file = /home/edo/Sviluppo/fedora-vmware/ansible.cfg
  configured module search path = Default w/o overrides

CONFIGURATION
ansible.cfg
[defaults]
inventory=inventory

[privilege_escalation]
become_method=sudo
become_user=root

OS / ENVIRONMENT
Fedora 25
SUMMARY

When running the playbook below not every package is marked as user installed, this means for example that running ""dnf autoremove"" afterwards will remove packages installed by the playbook.
STEPS TO REPRODUCE
Run the following playbook, then run ""dnf history userinstalled"":

---
- name: Fedora on VMware post-install
  hosts: localhost
  become: yes
  tasks:
    - name: Update everything
      dnf: name=""*"" state=latest
    - name: Install packages
      dnf: name={{ item }} state=latest
      with_items:
        - vim-enhanced
        - emacs
        - tmux
        - powertop
        - gnome-tweak-tool
        - libselinux-python
        - dconf-editor
        - open-vm-tools-desktop
        - git
...
dnf history userinstalled output:
Packages installed by user
anaconda-25.20.9-1.fc25.x86_64
ansible-2.2.1.0-1.fc25.noarch
docker-2:1.12.6-6.gitae7d637.fc25.x86_64
dracut-live-044-78.fc25.x86_64
gnome-tweak-tool-3.22.0-1.fc25.noarch
google-chrome-stable-56.0.2924.87-1.x86_64
kernel-modules-extra-4.9.12-200.fc25.x86_64
langpacks-en-1.0-8.fc25.noarch
memtest86+-5.01-15.fc25.x86_64
python2-dnf-1.1.10-5.fc25.noarch
syslinux-6.04-0.1.fc25.x86_64
tmux-2.2-3.fc25.x86_64

EXPECTED RESULTS
All the packages in the playbook should be listed as user installed, so that 'dnf autoremove' will not remove them. A possible workaround is running 'dnf mark '.
Plus, if you support also dnf autoremove #20333, then interesting things will happen.",3
,2011,148,,,,,"`defaults/main.yml` overriding variables `include_vars:` in roles

In defaults/main.yml:
---
foo: bar
In vars/Darwin.yml:
---
foo: baz
In tasks/main.yml:
- name: include env specific vars
  include_vars: ""{{ item }}""
  with_first_found:
    - ""{{ ansible_distribution }}.yml""
    - ""{{ ansible_os_family }}.yml""
- debug: var=foo
Assume this is run on a Darwin machine we get
foo: bar
where I would expect to get baz.
The work around is to ditch the defaults folder and instead load it with a defaults file with   with_first_found:
Maybe I am doing something backwards here, but the defaults folder seem to take higher precedence over including variables selectively included in the task.",3
,2012,148,,,,,"ansible os_server module doesnot work with async_status

ISSUE TYPE

Bug Report

COMPONENT NAME

module : os_server
ANSIBLE VERSION

ansible 2.1.0.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides


CONFIGURATION

OS / ENVIRONMENT

N/A
SUMMARY

Ansible is unable to parse the os_server module output using async_status .
STEPS TO REPRODUCE

The following tasks are to be written

name: ""Async:: provision/deprovision os_server resources by looping on count""
os_server:
state: ""{{ instance.4 }}""
auth:
auth_url: ""{{ instance.0 }}""
username: ""{{ instance.1 }}""
password: ""{{ instance.2 }}""
project_name: ""{{ instance.3 }}""
name: ""{{ instance.9 }}{{ instance.10 }}{{ instance.11 }}""
image: ""{{ instance.5 }}""
key_name: ""{{ instance.6  }}""
api_timeout: 99999
flavor: ""{{ instance.7 }}""
network: ""{{ instance.8 }}""
with_nested:

[""{{ endpoint }}""]
[""{{ username }}""]
[""{{ password }}""]
[""{{ project }}""]
[""{{ state }}""]
[""{{ res_def['image'] }}""]
[""{{ res_def['keypair']  }}""]
[""{{ res_def['flavor']  }}""]
[""{{ res_def['networks'][0] }}""]
[""{{ res_grp_name }}""]
[""{{ res_def['res_name'] }}""]
""{{ res_count.stdout }}""
loop_control:
loop_var: instance
async: 1000
poll: 0
register: res_def_output
when: async == true


name: 'check on fire and forget task'
async_status: jid={{ item.ansible_job_id }}
register: job_result
until: job_result.finished
retries: 30
with_items: ""{{ res_def_output['results'] }}""

Gives following error :
EXPECTED RESULTS

expected results are the output of the booted instance
ACTUAL RESULTS

Ansible is unable to parse the output of the job

<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: root
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `"" && echo ansible-tmp-1470849705.2-237917709656041=""` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `"" ) && sleep 0'                                                                                              
<127.0.0.1> PUT /tmp/tmpXkXV6P TO /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status
<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status; rm -rf ""/root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/"" > /dev/null 2>&1 && sleep 0'                                                                         
FAILED - RETRYING: TASK: openstack : check on fire and forget task (29 retries left).Result was: {""ansible_job_id"": ""309653937203.28231"", ""attempts"": 1, ""changed"": false, ""finished"": 0, ""invocation"": {""module_args"": {""jid"": ""309653937203.28231"", ""mode"": ""status""}, ""module_name"": ""async_status""}, ""results_file"": ""/root/.ansible_async/309653937203.28231"", ""retries"": 30, ""started"": 1}
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `"" && echo ansible-tmp-1470849710.29-212087273581974=""` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `"" ) && sleep 0'                                                                                           
<127.0.0.1> PUT /tmp/tmpvCVj1J TO /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status
<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status; rm -rf ""/root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/"" > /dev/null 2>&1 && sleep 0'                                                                       
FAILED - RETRYING: TASK: openstack : check on fire and forget task (28 retries left).Result was: {""ansible_job_id"": ""309653937203.28231"", ""attempts"": 2, ""changed"": false, ""finished"": 0, ""invocation"": {""module_args"": {""jid"": ""309653937203.28231"", ""mode"": ""status""}, ""module_name"": ""async_status""}, ""results_file"": ""/root/.ansible_async/309653937203.28231"", ""retries"": 30, ""started"": 1}
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `"" && echo ansible-tmp-1470849715.39-35966468008926=""` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `"" ) && sleep 0'                                                                                              
<127.0.0.1> PUT /tmp/tmpj1oc9X TO /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status
<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status; rm -rf ""/root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/"" > /dev/null 2>&1 && sleep 0'                                                                         
FAILED - RETRYING: TASK: openstack : check on fire and forget task (27 retries left).Result was: {""ansible_job_id"": ""309653937203.28231"", ""attempts"": 3, ""changed"": false, ""finished"": 0, ""invocation"": {""module_args"": {""jid"": ""309653937203.28231"", ""mode"": ""status""}, ""module_name"": ""async_status""}, ""results_file"": ""/root/.ansible_async/309653937203.28231"", ""retries"": 30, ""started"": 1}
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `"" && echo ansible-tmp-1470849720.47-221980927570550=""` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `"" ) && sleep 0'                                                                                           
<127.0.0.1> PUT /tmp/tmpUBrX3j TO /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status
<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status; rm -rf ""/root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/"" > /dev/null 2>&1 && sleep 0'                                                                       
failed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'309653937203.28231', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 0], u'results_file': u'/root/.ansible_async/309653937203.28231'}) => {""ansible_job_id"": ""309653937203.28231"", ""failed"": true, ""finished"": 1, ""invocation"": {""module_args"": {""jid"": ""309653937203.28231"", ""mode"": ""status""}, ""module_name"": ""async_status""}, ""item"": {""ansible_job_id"": ""309653937203.28231"", ""instance"": [""http://localhost:5000/v2.0"", ""e2e-openstack"", ""rgHdUfMqshXWSBYdlfIp"", ""e2e-openstack"", ""present"", ""rhel-6.5_jeos"", ""ci-factory"", ""m1.small"", ""e2e-openstack"", ""testgroup1"", ""ano_inst"", 0], ""results_file"": ""/root/.ansible_async/309653937203.28231"", ""started"": 1}, ""msg"": ""Could not parse job output: No handlers could be found for logger \""keystoneauth.identity.base\""\n\n{\""invocation\"": {\""module_args\"": {\""auth_type\"": null, \""availability_zone\"": null, \""image\"": \""rhel-6.5_jeos\"", \""image_exclude\"": \""(deprecated)\"", \""flavor_include\"": null, \""meta\"": null, \""flavor\"": \""m1.small\"", \""cloud\"": null, \""scheduler_hints\"": null, \""boot_from_volume\"": false, \""userdata\"": null, \""network\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""nics\"": [], \""floating_ips\"": null, \""flavor_ram\"": null, \""volume_size\"": false, \""state\"": \""present\"", \""auto_ip\"": true, \""security_groups\"": [\""default\""], \""config_drive\"": false, \""volumes\"": [], \""key_name\"": \""ci-factory\"", \""api_timeout\"": 99999, \""auth\"": {\""username\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""project_name\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""password\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""auth_url\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\""}, \""endpoint_type\"": \""public\"", \""boot_volume\"": null, \""key\"": null, \""cacert\"": null, \""wait\"": true, \""name\"": \""testgroup1_ano_inst_0\"", \""region_name\"": null, \""timeout\"": 180, \""cert\"": null, \""terminate_volume\"": false, \""verify\"": true, \""floating_ip_pools\"": null}}, \""openstack\"": {\""OS-EXT-STS:task_state\"": null, \""addresses\"": {\""e2e-openstack\"": [{\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:da:36:f5\"", \""version\"": 4, \""addr\"": \""172.16.100.91\"", \""OS-EXT-IPS:type\"": \""fixed\""}, {\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:da:36:f5\"", \""version\"": 4, \""addr\"": \""10.8.183.233\"", \""OS-EXT-IPS:type\"": \""floating\""}]}, \""image\"": {\""id\"": \""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\"", \""name\"": \""rhel-6.5_jeos\""}, \""OS-EXT-STS:vm_state\"": \""active\"", \""OS-SRV-USG:launched_at\"": \""2016-08-10T17:21:52.000000\"", \""NAME_ATTR\"": \""name\"", \""flavor\"": {\""id\"": \""2\"", \""name\"": \""m1.small\""}, \""az\"": \""nova\"", \""id\"": \""d83a4842-b362-44fe-8f73-dc5f8ee47df5\"", \""cloud\"": \""defaults\"", \""user_id\"": \""9c770dbddda444799e627004fee26e0a\"", \""OS-DCF:diskConfig\"": \""MANUAL\"", \""networks\"": {\""e2e-openstack\"": [\""172.16.100.91\"", \""10.8.183.233\""]}, \""accessIPv4\"": \""10.8.183.233\"", \""accessIPv6\"": \""\"", \""security_groups\"": [{\""id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""name\"": \""default\"", \""security_group_rules\"": [{\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""ade9fcb9-14c1-4975-a04d-6007f80005c1\""}, {\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""d03e4bae-24b6-415a-a30c-ee0d060f566f\""}], \""description\"": \""Default security group\""}], \""key_name\"": \""ci-factory\"", \""progress\"": 0, \""OS-EXT-STS:power_state\"": 1, \""OS-EXT-AZ:availability_zone\"": \""nova\"", \""metadata\"": {}, \""status\"": \""ACTIVE\"", \""updated\"": \""2016-08-10T17:21:52Z\"", \""hostId\"": \""45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\"", \""HUMAN_ID\"": true, \""OS-SRV-USG:terminated_at\"": null, \""public_v4\"": \""10.8.183.233\"", \""public_v6\"": \""\"", \""private_v4\"": \""172.16.100.91\"", \""interface_ip\"": \""10.8.183.233\"", \""name\"": \""testgroup1_ano_inst_0\"", \""created\"": \""2016-08-10T17:21:46Z\"", \""tenant_id\"": \""f1dda47890754241a3e111f9b7394707\"", \""region\"": \""\"", \""adminPass\"": \""B2zNdwbcP8ha\"", \""os-extended-volumes:volumes_attached\"": [], \""volumes\"": [], \""config_drive\"": \""\"", \""human_id\"": \""testgroup1_ano_inst_0\""}, \""changed\"": true, \""id\"": \""d83a4842-b362-44fe-8f73-dc5f8ee47df5\"", \""server\"": {\""OS-EXT-STS:task_state\"": null, \""addresses\"": {\""e2e-openstack\"": [{\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:da:36:f5\"", \""version\"": 4, \""addr\"": \""172.16.100.91\"", \""OS-EXT-IPS:type\"": \""fixed\""}, {\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:da:36:f5\"", \""version\"": 4, \""addr\"": \""10.8.183.233\"", \""OS-EXT-IPS:type\"": \""floating\""}]}, \""image\"": {\""id\"": \""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\"", \""name\"": \""rhel-6.5_jeos\""}, \""OS-EXT-STS:vm_state\"": \""active\"", \""OS-SRV-USG:launched_at\"": \""2016-08-10T17:21:52.000000\"", \""NAME_ATTR\"": \""name\"", \""flavor\"": {\""id\"": \""2\"", \""name\"": \""m1.small\""}, \""az\"": \""nova\"", \""id\"": \""d83a4842-b362-44fe-8f73-dc5f8ee47df5\"", \""cloud\"": \""defaults\"", \""user_id\"": \""9c770dbddda444799e627004fee26e0a\"", \""OS-DCF:diskConfig\"": \""MANUAL\"", \""networks\"": {\""e2e-openstack\"": [\""172.16.100.91\"", \""10.8.183.233\""]}, \""accessIPv4\"": \""10.8.183.233\"", \""accessIPv6\"": \""\"", \""security_groups\"": [{\""id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""name\"": \""default\"", \""security_group_rules\"": [{\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""ade9fcb9-14c1-4975-a04d-6007f80005c1\""}, {\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""d03e4bae-24b6-415a-a30c-ee0d060f566f\""}], \""description\"": \""Default security group\""}], \""key_name\"": \""ci-factory\"", \""progress\"": 0, \""OS-EXT-STS:power_state\"": 1, \""OS-EXT-AZ:availability_zone\"": \""nova\"", \""metadata\"": {}, \""status\"": \""ACTIVE\"", \""updated\"": \""2016-08-10T17:21:52Z\"", \""hostId\"": \""45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\"", \""HUMAN_ID\"": true, \""OS-SRV-USG:terminated_at\"": null, \""public_v4\"": \""10.8.183.233\"", \""public_v6\"": \""\"", \""private_v4\"": \""172.16.100.91\"", \""interface_ip\"": \""10.8.183.233\"", \""name\"": \""testgroup1_ano_inst_0\"", \""created\"": \""2016-08-10T17:21:46Z\"", \""tenant_id\"": \""f1dda47890754241a3e111f9b7394707\"", \""region\"": \""\"", \""adminPass\"": \""B2zNdwbcP8ha\"", \""os-extended-volumes:volumes_attached\"": [], \""volumes\"": [], \""config_drive\"": \""\"", \""human_id\"": \""testgroup1_ano_inst_0\""}}\n{\""msg\"": \""Traceback (most recent call last):\\n  File \\\""/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/async_wrapper\\\"", line 89, in _run_module\\n  File \\\""/usr/lib64/python2.7/json/__init__.py\\\"", line 339, in loads\\n    return _default_decoder.decode(s)\\n  File \\\""/usr/lib64/python2.7/json/decoder.py\\\"", line 364, in decode\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n  File \\\""/usr/lib64/python2.7/json/decoder.py\\\"", line 382, in raw_decode\\n    raise ValueError(\\\""No JSON object could be decoded\\\"")\\nValueError: No JSON object could be decoded\\n\"", \""failed\"": 1, \""cmd\"": \""/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/os_server\"", \""data\"": \""No handlers could be found for logger \\\""keystoneauth.identity.base\\\""\\n\\n{\\\""invocation\\\"": {\\\""module_args\\\"": {\\\""auth_type\\\"": null, \\\""availability_zone\\\"": null, \\\""image\\\"": \\\""rhel-6.5_jeos\\\"", \\\""image_exclude\\\"": \\\""(deprecated)\\\"", \\\""flavor_include\\\"": null, \\\""meta\\\"": null, \\\""flavor\\\"": \\\""m1.small\\\"", \\\""cloud\\\"": null, \\\""scheduler_hints\\\"": null, \\\""boot_from_volume\\\"": false, \\\""userdata\\\"": null, \\\""network\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""nics\\\"": [], \\\""floating_ips\\\"": null, \\\""flavor_ram\\\"": null, \\\""volume_size\\\"": false, \\\""state\\\"": \\\""present\\\"", \\\""auto_ip\\\"": true, \\\""security_groups\\\"": [\\\""default\\\""], \\\""config_drive\\\"": false, \\\""volumes\\\"": [], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""api_timeout\\\"": 99999, \\\""auth\\\"": {\\\""username\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""project_name\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""password\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""auth_url\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\""}, \\\""endpoint_type\\\"": \\\""public\\\"", \\\""boot_volume\\\"": null, \\\""key\\\"": null, \\\""cacert\\\"": null, \\\""wait\\\"": true, \\\""name\\\"": \\\""testgroup1_ano_inst_0\\\"", \\\""region_name\\\"": null, \\\""timeout\\\"": 180, \\\""cert\\\"": null, \\\""terminate_volume\\\"": false, \\\""verify\\\"": true, \\\""floating_ip_pools\\\"": null}}, \\\""openstack\\\"": {\\\""OS-EXT-STS:task_state\\\"": null, \\\""addresses\\\"": {\\\""e2e-openstack\\\"": [{\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:da:36:f5\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""172.16.100.91\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""fixed\\\""}, {\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:da:36:f5\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""10.8.183.233\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""floating\\\""}]}, \\\""image\\\"": {\\\""id\\\"": \\\""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\"", \\\""name\\\"": \\\""rhel-6.5_jeos\\\""}, \\\""OS-EXT-STS:vm_state\\\"": \\\""active\\\"", \\\""OS-SRV-USG:launched_at\\\"": \\\""2016-08-10T17:21:52.000000\\\"", \\\""NAME_ATTR\\\"": \\\""name\\\"", \\\""flavor\\\"": {\\\""id\\\"": \\\""2\\\"", \\\""name\\\"": \\\""m1.small\\\""}, \\\""az\\\"": \\\""nova\\\"", \\\""id\\\"": \\\""d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\"", \\\""cloud\\\"": \\\""defaults\\\"", \\\""user_id\\\"": \\\""9c770dbddda444799e627004fee26e0a\\\"", \\\""OS-DCF:diskConfig\\\"": \\\""MANUAL\\\"", \\\""networks\\\"": {\\\""e2e-openstack\\\"": [\\\""172.16.100.91\\\"", \\\""10.8.183.233\\\""]}, \\\""accessIPv4\\\"": \\\""10.8.183.233\\\"", \\\""accessIPv6\\\"": \\\""\\\"", \\\""security_groups\\\"": [{\\\""id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""name\\\"": \\\""default\\\"", \\\""security_group_rules\\\"": [{\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""ade9fcb9-14c1-4975-a04d-6007f80005c1\\\""}, {\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""d03e4bae-24b6-415a-a30c-ee0d060f566f\\\""}], \\\""description\\\"": \\\""Default security group\\\""}], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""progress\\\"": 0, \\\""OS-EXT-STS:power_state\\\"": 1, \\\""OS-EXT-AZ:availability_zone\\\"": \\\""nova\\\"", \\\""metadata\\\"": {}, \\\""status\\\"": \\\""ACTIVE\\\"", \\\""updated\\\"": \\\""2016-08-10T17:21:52Z\\\"", \\\""hostId\\\"": \\\""45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\"", \\\""HUMAN_ID\\\"": true, \\\""OS-SRV-USG:terminated_at\\\"": null, \\\""public_v4\\\"": \\\""10.8.183.233\\\"", \\\""public_v6\\\"": \\\""\\\"", \\\""private_v4\\\"": \\\""172.16.100.91\\\"", \\\""interface_ip\\\"": \\\""10.8.183.233\\\"", \\\""name\\\"": \\\""testgroup1_ano_inst_0\\\"", \\\""created\\\"": \\\""2016-08-10T17:21:46Z\\\"", \\\""tenant_id\\\"": \\\""f1dda47890754241a3e111f9b7394707\\\"", \\\""region\\\"": \\\""\\\"", \\\""adminPass\\\"": \\\""B2zNdwbcP8ha\\\"", \\\""os-extended-volumes:volumes_attached\\\"": [], \\\""volumes\\\"": [], \\\""config_drive\\\"": \\\""\\\"", \\\""human_id\\\"": \\\""testgroup1_ano_inst_0\\\""}, \\\""changed\\\"": true, \\\""id\\\"": \\\""d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\"", \\\""server\\\"": {\\\""OS-EXT-STS:task_state\\\"": null, \\\""addresses\\\"": {\\\""e2e-openstack\\\"": [{\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:da:36:f5\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""172.16.100.91\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""fixed\\\""}, {\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:da:36:f5\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""10.8.183.233\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""floating\\\""}]}, \\\""image\\\"": {\\\""id\\\"": \\\""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\"", \\\""name\\\"": \\\""rhel-6.5_jeos\\\""}, \\\""OS-EXT-STS:vm_state\\\"": \\\""active\\\"", \\\""OS-SRV-USG:launched_at\\\"": \\\""2016-08-10T17:21:52.000000\\\"", \\\""NAME_ATTR\\\"": \\\""name\\\"", \\\""flavor\\\"": {\\\""id\\\"": \\\""2\\\"", \\\""name\\\"": \\\""m1.small\\\""}, \\\""az\\\"": \\\""nova\\\"", \\\""id\\\"": \\\""d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\"", \\\""cloud\\\"": \\\""defaults\\\"", \\\""user_id\\\"": \\\""9c770dbddda444799e627004fee26e0a\\\"", \\\""OS-DCF:diskConfig\\\"": \\\""MANUAL\\\"", \\\""networks\\\"": {\\\""e2e-openstack\\\"": [\\\""172.16.100.91\\\"", \\\""10.8.183.233\\\""]}, \\\""accessIPv4\\\"": \\\""10.8.183.233\\\"", \\\""accessIPv6\\\"": \\\""\\\"", \\\""security_groups\\\"": [{\\\""id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""name\\\"": \\\""default\\\"", \\\""security_group_rules\\\"": [{\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""ade9fcb9-14c1-4975-a04d-6007f80005c1\\\""}, {\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""d03e4bae-24b6-415a-a30c-ee0d060f566f\\\""}], \\\""description\\\"": \\\""Default security group\\\""}], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""progress\\\"": 0, \\\""OS-EXT-STS:power_state\\\"": 1, \\\""OS-EXT-AZ:availability_zone\\\"": \\\""nova\\\"", \\\""metadata\\\"": {}, \\\""status\\\"": \\\""ACTIVE\\\"", \\\""updated\\\"": \\\""2016-08-10T17:21:52Z\\\"", \\\""hostId\\\"": \\\""45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\"", \\\""HUMAN_ID\\\"": true, \\\""OS-SRV-USG:terminated_at\\\"": null, \\\""public_v4\\\"": \\\""10.8.183.233\\\"", \\\""public_v6\\\"": \\\""\\\"", \\\""private_v4\\\"": \\\""172.16.100.91\\\"", \\\""interface_ip\\\"": \\\""10.8.183.233\\\"", \\\""name\\\"": \\\""testgroup1_ano_inst_0\\\"", \\\""created\\\"": \\\""2016-08-10T17:21:46Z\\\"", \\\""tenant_id\\\"": \\\""f1dda47890754241a3e111f9b7394707\\\"", \\\""region\\\"": \\\""\\\"", \\\""adminPass\\\"": \\\""B2zNdwbcP8ha\\\"", \\\""os-extended-volumes:volumes_attached\\\"": [], \\\""volumes\\\"": [], \\\""config_drive\\\"": \\\""\\\"", \\\""human_id\\\"": \\\""testgroup1_ano_inst_0\\\""}}\\n\"", \""ansible_job_id\"": \""309653937203.28231\""}"", ""results_file"": ""/root/.ansible_async/309653937203.28231"", ""started"": 1}                                                                                                                                               
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `"" && echo ansible-tmp-1470849720.76-738324392915=""` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `"" ) && sleep 0'                                                                                                    
<127.0.0.1> PUT /tmp/tmpvHYKYF TO /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status
<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status; rm -rf ""/root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/"" > /dev/null 2>&1 && sleep 0'                                                                             
failed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'557360551178.28260', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 1], u'results_file': u'/root/.ansible_async/557360551178.28260'}) => {""ansible_job_id"": ""557360551178.28260"", ""failed"": true, ""finished"": 1, ""invocation"": {""module_args"": {""jid"": ""557360551178.28260"", ""mode"": ""status""}, ""module_name"": ""async_status""}, ""item"": {""ansible_job_id"": ""557360551178.28260"", ""instance"": [""http://localhost:5000/v2.0"", ""e2e-openstack"", ""rgHdUfMqshXWSBYdlfIp"", ""e2e-openstack"", ""present"", ""rhel-6.5_jeos"", ""ci-factory"", ""m1.small"", ""e2e-openstack"", ""testgroup1"", ""ano_inst"", 1], ""results_file"": ""/root/.ansible_async/557360551178.28260"", ""started"": 1}, ""msg"": ""Could not parse job output: No handlers could be found for logger \""keystoneauth.identity.base\""\n\n{\""invocation\"": {\""module_args\"": {\""auth_type\"": null, \""availability_zone\"": null, \""image\"": \""rhel-6.5_jeos\"", \""image_exclude\"": \""(deprecated)\"", \""flavor_include\"": null, \""meta\"": null, \""flavor\"": \""m1.small\"", \""cloud\"": null, \""scheduler_hints\"": null, \""boot_from_volume\"": false, \""userdata\"": null, \""network\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""nics\"": [], \""floating_ips\"": null, \""flavor_ram\"": null, \""volume_size\"": false, \""state\"": \""present\"", \""auto_ip\"": true, \""security_groups\"": [\""default\""], \""config_drive\"": false, \""volumes\"": [], \""key_name\"": \""ci-factory\"", \""api_timeout\"": 99999, \""auth\"": {\""username\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""project_name\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""password\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\"", \""auth_url\"": \""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\""}, \""endpoint_type\"": \""public\"", \""boot_volume\"": null, \""key\"": null, \""cacert\"": null, \""wait\"": true, \""name\"": \""testgroup1_ano_inst_1\"", \""region_name\"": null, \""timeout\"": 180, \""cert\"": null, \""terminate_volume\"": false, \""verify\"": true, \""floating_ip_pools\"": null}}, \""openstack\"": {\""OS-EXT-STS:task_state\"": null, \""addresses\"": {\""e2e-openstack\"": [{\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:89:04:7c\"", \""version\"": 4, \""addr\"": \""172.16.100.92\"", \""OS-EXT-IPS:type\"": \""fixed\""}, {\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:89:04:7c\"", \""version\"": 4, \""addr\"": \""10.8.182.45\"", \""OS-EXT-IPS:type\"": \""floating\""}]}, \""image\"": {\""id\"": \""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\"", \""name\"": \""rhel-6.5_jeos\""}, \""OS-EXT-STS:vm_state\"": \""active\"", \""OS-SRV-USG:launched_at\"": \""2016-08-10T17:21:52.000000\"", \""NAME_ATTR\"": \""name\"", \""flavor\"": {\""id\"": \""2\"", \""name\"": \""m1.small\""}, \""az\"": \""nova\"", \""id\"": \""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\"", \""cloud\"": \""defaults\"", \""user_id\"": \""9c770dbddda444799e627004fee26e0a\"", \""OS-DCF:diskConfig\"": \""MANUAL\"", \""networks\"": {\""e2e-openstack\"": [\""172.16.100.92\"", \""10.8.182.45\""]}, \""accessIPv4\"": \""10.8.182.45\"", \""accessIPv6\"": \""\"", \""security_groups\"": [{\""id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""name\"": \""default\"", \""security_group_rules\"": [{\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""ade9fcb9-14c1-4975-a04d-6007f80005c1\""}, {\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""d03e4bae-24b6-415a-a30c-ee0d060f566f\""}], \""description\"": \""Default security group\""}], \""key_name\"": \""ci-factory\"", \""progress\"": 0, \""OS-EXT-STS:power_state\"": 1, \""OS-EXT-AZ:availability_zone\"": \""nova\"", \""metadata\"": {}, \""status\"": \""ACTIVE\"", \""updated\"": \""2016-08-10T17:21:52Z\"", \""hostId\"": \""bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\"", \""HUMAN_ID\"": true, \""OS-SRV-USG:terminated_at\"": null, \""public_v4\"": \""10.8.182.45\"", \""public_v6\"": \""\"", \""private_v4\"": \""172.16.100.92\"", \""interface_ip\"": \""10.8.182.45\"", \""name\"": \""testgroup1_ano_inst_1\"", \""created\"": \""2016-08-10T17:21:48Z\"", \""tenant_id\"": \""f1dda47890754241a3e111f9b7394707\"", \""region\"": \""\"", \""adminPass\"": \""iRvzMzj2Q33e\"", \""os-extended-volumes:volumes_attached\"": [], \""volumes\"": [], \""config_drive\"": \""\"", \""human_id\"": \""testgroup1_ano_inst_1\""}, \""changed\"": true, \""id\"": \""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\"", \""server\"": {\""OS-EXT-STS:task_state\"": null, \""addresses\"": {\""e2e-openstack\"": [{\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:89:04:7c\"", \""version\"": 4, \""addr\"": \""172.16.100.92\"", \""OS-EXT-IPS:type\"": \""fixed\""}, {\""OS-EXT-IPS-MAC:mac_addr\"": \""fa:16:3e:89:04:7c\"", \""version\"": 4, \""addr\"": \""10.8.182.45\"", \""OS-EXT-IPS:type\"": \""floating\""}]}, \""image\"": {\""id\"": \""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\"", \""name\"": \""rhel-6.5_jeos\""}, \""OS-EXT-STS:vm_state\"": \""active\"", \""OS-SRV-USG:launched_at\"": \""2016-08-10T17:21:52.000000\"", \""NAME_ATTR\"": \""name\"", \""flavor\"": {\""id\"": \""2\"", \""name\"": \""m1.small\""}, \""az\"": \""nova\"", \""id\"": \""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\"", \""cloud\"": \""defaults\"", \""user_id\"": \""9c770dbddda444799e627004fee26e0a\"", \""OS-DCF:diskConfig\"": \""MANUAL\"", \""networks\"": {\""e2e-openstack\"": [\""172.16.100.92\"", \""10.8.182.45\""]}, \""accessIPv4\"": \""10.8.182.45\"", \""accessIPv6\"": \""\"", \""security_groups\"": [{\""id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""name\"": \""default\"", \""security_group_rules\"": [{\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""ade9fcb9-14c1-4975-a04d-6007f80005c1\""}, {\""direction\"": \""ingress\"", \""protocol\"": null, \""remote_ip_prefix\"": null, \""port_range_max\"": null, \""security_group_id\"": \""df1a797b-009c-4685-a7c9-43863c36d653\"", \""port_range_min\"": null, \""ethertype\"": \""IPv4\"", \""id\"": \""d03e4bae-24b6-415a-a30c-ee0d060f566f\""}], \""description\"": \""Default security group\""}], \""key_name\"": \""ci-factory\"", \""progress\"": 0, \""OS-EXT-STS:power_state\"": 1, \""OS-EXT-AZ:availability_zone\"": \""nova\"", \""metadata\"": {}, \""status\"": \""ACTIVE\"", \""updated\"": \""2016-08-10T17:21:52Z\"", \""hostId\"": \""bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\"", \""HUMAN_ID\"": true, \""OS-SRV-USG:terminated_at\"": null, \""public_v4\"": \""10.8.182.45\"", \""public_v6\"": \""\"", \""private_v4\"": \""172.16.100.92\"", \""interface_ip\"": \""10.8.182.45\"", \""name\"": \""testgroup1_ano_inst_1\"", \""created\"": \""2016-08-10T17:21:48Z\"", \""tenant_id\"": \""f1dda47890754241a3e111f9b7394707\"", \""region\"": \""\"", \""adminPass\"": \""iRvzMzj2Q33e\"", \""os-extended-volumes:volumes_attached\"": [], \""volumes\"": [], \""config_drive\"": \""\"", \""human_id\"": \""testgroup1_ano_inst_1\""}}\n{\""msg\"": \""Traceback (most recent call last):\\n  File \\\""/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/async_wrapper\\\"", line 89, in _run_module\\n  File \\\""/usr/lib64/python2.7/json/__init__.py\\\"", line 339, in loads\\n    return _default_decoder.decode(s)\\n  File \\\""/usr/lib64/python2.7/json/decoder.py\\\"", line 364, in decode\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n  File \\\""/usr/lib64/python2.7/json/decoder.py\\\"", line 382, in raw_decode\\n    raise ValueError(\\\""No JSON object could be decoded\\\"")\\nValueError: No JSON object could be decoded\\n\"", \""failed\"": 1, \""cmd\"": \""/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/os_server\"", \""data\"": \""No handlers could be found for logger \\\""keystoneauth.identity.base\\\""\\n\\n{\\\""invocation\\\"": {\\\""module_args\\\"": {\\\""auth_type\\\"": null, \\\""availability_zone\\\"": null, \\\""image\\\"": \\\""rhel-6.5_jeos\\\"", \\\""image_exclude\\\"": \\\""(deprecated)\\\"", \\\""flavor_include\\\"": null, \\\""meta\\\"": null, \\\""flavor\\\"": \\\""m1.small\\\"", \\\""cloud\\\"": null, \\\""scheduler_hints\\\"": null, \\\""boot_from_volume\\\"": false, \\\""userdata\\\"": null, \\\""network\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""nics\\\"": [], \\\""floating_ips\\\"": null, \\\""flavor_ram\\\"": null, \\\""volume_size\\\"": false, \\\""state\\\"": \\\""present\\\"", \\\""auto_ip\\\"": true, \\\""security_groups\\\"": [\\\""default\\\""], \\\""config_drive\\\"": false, \\\""volumes\\\"": [], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""api_timeout\\\"": 99999, \\\""auth\\\"": {\\\""username\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""project_name\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""password\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"", \\\""auth_url\\\"": \\\""VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\""}, \\\""endpoint_type\\\"": \\\""public\\\"", \\\""boot_volume\\\"": null, \\\""key\\\"": null, \\\""cacert\\\"": null, \\\""wait\\\"": true, \\\""name\\\"": \\\""testgroup1_ano_inst_1\\\"", \\\""region_name\\\"": null, \\\""timeout\\\"": 180, \\\""cert\\\"": null, \\\""terminate_volume\\\"": false, \\\""verify\\\"": true, \\\""floating_ip_pools\\\"": null}}, \\\""openstack\\\"": {\\\""OS-EXT-STS:task_state\\\"": null, \\\""addresses\\\"": {\\\""e2e-openstack\\\"": [{\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:89:04:7c\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""172.16.100.92\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""fixed\\\""}, {\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:89:04:7c\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""10.8.182.45\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""floating\\\""}]}, \\\""image\\\"": {\\\""id\\\"": \\\""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\"", \\\""name\\\"": \\\""rhel-6.5_jeos\\\""}, \\\""OS-EXT-STS:vm_state\\\"": \\\""active\\\"", \\\""OS-SRV-USG:launched_at\\\"": \\\""2016-08-10T17:21:52.000000\\\"", \\\""NAME_ATTR\\\"": \\\""name\\\"", \\\""flavor\\\"": {\\\""id\\\"": \\\""2\\\"", \\\""name\\\"": \\\""m1.small\\\""}, \\\""az\\\"": \\\""nova\\\"", \\\""id\\\"": \\\""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\"", \\\""cloud\\\"": \\\""defaults\\\"", \\\""user_id\\\"": \\\""9c770dbddda444799e627004fee26e0a\\\"", \\\""OS-DCF:diskConfig\\\"": \\\""MANUAL\\\"", \\\""networks\\\"": {\\\""e2e-openstack\\\"": [\\\""172.16.100.92\\\"", \\\""10.8.182.45\\\""]}, \\\""accessIPv4\\\"": \\\""10.8.182.45\\\"", \\\""accessIPv6\\\"": \\\""\\\"", \\\""security_groups\\\"": [{\\\""id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""name\\\"": \\\""defaul \\\""security_group_rules\\\"": [{\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_gd\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""ade9fcb9-14c1-4975-a04d-6007f80005c1\{\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""d03e4bae-24b6-415a-a30c-ee0d060f566f\\\""}], \\\""description\\\"": \\\""t security group\\\""}], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""progress\\\"": 0, \\\""OS-EXT-STS:power_state\\\"": 1, \\\""OS-EXT-AZ:availability_zone\\\"": \\\""nova\\\"", tadata\\\"": {}, \\\""status\\\"": \\\""ACTIVE\\\"", \\\""updated\\\"": \\\""2016-08-10T17:21:52Z\\\"", \\\""hostId\\\"": \\\""bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67, \\\""HUMAN_ID\\\"": true, \\\""OS-SRV-USG:terminated_at\\\"": null, \\\""public_v4\\\"": \\\""10.8.182.45\\\"", \\\""public_v6\\\"": \\\""\\\"", \\\""private_v4\\\"": \\\""172.16.100"", \\\""interface_ip\\\"": \\\""10.8.182.45\\\"", \\\""name\\\"": \\\""testgroup1_ano_inst_1\\\"", \\\""created\\\"": \\\""2016-08-10T17:21:48Z\\\"", \\\""tenant_id\\\"": \\\""f1dda47841a3e111f9b7394707\\\"", \\\""region\\\"": \\\""\\\"", \\\""adminPass\\\"": \\\""iRvzMzj2Q33e\\\"", \\\""os-extended-volumes:volumes_attached\\\"": [], \\\""volumes\\\"": [], \\\""conive\\\"": \\\""\\\"", \\\""human_id\\\"": \\\""testgroup1_ano_inst_1\\\""}, \\\""changed\\\"": true, \\\""id\\\"": \\\""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\"", \\\""server\\\"": {\\XT-STS:task_state\\\"": null, \\\""addresses\\\"": {\\\""e2e-openstack\\\"": [{\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:89:04:7c\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \.16.100.92\\\"", \\\""OS-EXT-IPS:type\\\"": \\\""fixed\\\""}, {\\\""OS-EXT-IPS-MAC:mac_addr\\\"": \\\""fa:16:3e:89:04:7c\\\"", \\\""version\\\"": 4, \\\""addr\\\"": \\\""10.8.182.45\\\""OS-EXT-IPS:type\\\"": \\\""floating\\\""}]}, \\\""image\\\"": {\\\""id\\\"": \\\""3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\"", \\\""name\\\"": \\\""rhel-6.5_jeos\\\""}, \\\""OS-EXT-STtate\\\"": \\\""active\\\"", \\\""OS-SRV-USG:launched_at\\\"": \\\""2016-08-10T17:21:52.000000\\\"", \\\""NAME_ATTR\\\"": \\\""name\\\"", \\\""flavor\\\"": {\\\""id\\\"": \\\""2\\\"", \\\\\"": \\\""m1.small\\\""}, \\\""az\\\"": \\\""nova\\\"", \\\""id\\\"": \\\""d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\"", \\\""cloud\\\"": \\\""defaults\\\"", \\\""user_id\\\"": \\\""9c770d44799e627004fee26e0a\\\"", \\\""OS-DCF:diskConfig\\\"": \\\""MANUAL\\\"", \\\""networks\\\"": {\\\""e2e-openstack\\\"": [\\\""172.16.100.92\\\"", \\\""10.8.182.45\\\""]}, \\\""accessI"": \\\""10.8.182.45\\\"", \\\""accessIPv6\\\"": \\\""\\\"", \\\""security_groups\\\"": [{\\\""id\\\"": \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""name\\\"": \\\""default\\\"",ecurity_group_rules\\\"": [{\\\""direction\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_i \\\""df1a797b-009c-4685-a7c9-43863c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""ade9fcb9-14c1-4975-a04d-6007f80005c1\\\""}, irection\\\"": \\\""ingress\\\"", \\\""protocol\\\"": null, \\\""remote_ip_prefix\\\"": null, \\\""port_range_max\\\"": null, \\\""security_group_id\\\"": \\\""df1a797b-009c-4685-a763c36d653\\\"", \\\""port_range_min\\\"": null, \\\""ethertype\\\"": \\\""IPv4\\\"", \\\""id\\\"": \\\""d03e4bae-24b6-415a-a30c-ee0d060f566f\\\""}], \\\""description\\\"": \\\""Defaulrity group\\\""}], \\\""key_name\\\"": \\\""ci-factory\\\"", \\\""progress\\\"": 0, \\\""OS-EXT-STS:power_state\\\"": 1, \\\""OS-EXT-AZ:availability_zone\\\"": \\\""nova\\\"", \\\""me\\\"": {}, \\\""status\\\"": \\\""ACTIVE\\\"", \\\""updated\\\"": \\\""2016-08-10T17:21:52Z\\\"", \\\""hostId\\\"": \\\""bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\""HUMAN_ID\\\"": true, \\\""OS-SRV-USG:terminated_at\\\"": null, \\\""public_v4\\\"": \\\""10.8.182.45\\\"", \\\""public_v6\\\"": \\\""\\\"", \\\""private_v4\\\"": \\\""172.16.100.92\\\""interface_ip\\\"": \\\""10.8.182.45\\\"", \\\""name\\\"": \\\""testgroup1_ano_inst_1\\\"", \\\""created\\\"": \\\""2016-08-10T17:21:48Z\\\"", \\\""tenant_id\\\"": \\\""f1dda47890754211f9b7394707\\\"", \\\""region\\\"": \\\""\\\"", \\\""adminPass\\\"": \\\""iRvzMzj2Q33e\\\"", \\\""os-extended-volumes:volumes_attached\\\"": [], \\\""volumes\\\"": [], \\\""config_dr"": \\\""\\\"", \\\""human_id\\\"": \\\""testgroup1_ano_inst_1\\\""}}\\n\"", \""ansible_job_id\"": \""557360551178.28260\""}"", ""results_file"": ""/root/.ansible_async/557360551178.282started"": 1}",3
,2013,143,,,,,"Please enhance when: to accept a list

The semantics should be an AND of the items in the list
when:

expression-1
expression-2

should be equivalent to:
when: expression-1 and expression-2
Justification:
Long/complex expressions are made eminently more readable by breaking them down into components.
This is quite general, any boolean expression can be transformed into an AND-form with negation: a or b = ^(^a and ^b)",3
,2014,144,,,,,"include fails to expand host variables

The playbook:

---
- hosts: localhost
  vars:
    - othertasks:
       - othertask1
       - othertask2
  tasks:
    - name: debug
      debug: msg=""tasks/$item.yml""
      with_items: $othertasks

    - include: tasks/$item.yml
      with_items: $othertasks

this works as long as othertasks is defined inline.
When othertasks is removed from playbook and defined in the inventory file or in host_vars, debug continues working but include fails.
Error is ERROR: file not found: ./tasks/$othertasks.yml.",3
,2015,148,,,,,"Copying files with mode 400 uploads with a mode of 620

Hi,
I did post this in the Google forum, but my post never seemed to appear so asking here now instead.
This is probably something that isn't recommended but I'm trying to copy some PEM keys to a destination server so that they can be used by that server at a later time.
I need to make sure that they are set to a mode of 400 as well, so using the following code:
- name: add default ssh keys
  become: true
  copy:
    src: ./.ssh/
    dest: ~/.ssh
    mode: 400
    force: true

I would hope and expect to see the following when listing the files on the destination server:
-r--------

But instead I'm seeing the following:
-rw--w----

Any particular reason why the files being uploaded are actually getting the incorrect permissions?
Or is there a different approach to making sure these files have the correct permissions?
Thanks,
Gary",3
,2016,144,,,,,"tags are not inherented by multiple include levels

Issue Type:

Bug Report (or feature request, depends on how you see it)

Ansible Version:
1.9.4
Ansible Configuration:
Default
Environment:
Debian 8.2 (jessie)
Summary:
See the project structure below
Steps To Reproduce:
I have the following project structure:
roles/
  local/   
    tasks/
      main.yml [1]
      media/
        main.yml [2]
        hipchat.yml
        slack.yml
       ...

[1] main.yml has content:
- include: media/main.yml tags=media
[2] main.yml has content:
- include: hipchat.yml tags=hipchat
- include: slack.yml tags=slack
...

Expected Results:
I expect all tasks from media/main.yml will inherit all tags specified in tasks/main.yml. So, if I run ansible-playbook -i hosts site.yml --tags media, it will run all tasks specified in media/main.yml, and $ ansible-playbook --list-tags -i hosts site.yml reports this tag as well
playbook: site.yml

  play #1 (localhost):  TAGS: []
    TASK TAGS: [alternatives, apt, atom, chrome, clojure, docker, dot-files, dropbox, git, gnome, go, hipchat, idea, jdk, maven, packages, permissions, productivity, sdkman, skype, slack, spotify, system, tools, viber, vim]

Actual Results:
It fails with the following error:
ERROR: tag(s) not found in playbook: media.  possible values: alternatives,apt,atom,chrome,clojure,docker,dot-files,dropbox,git,gnome,go,hipchat,idea,jdk,maven,packages,permissions,productivity,sdkman,skype,slack,spotify,system,tools,viber,vim

P.S.1: The whole source code of the project is available here https://github.com/zshamrock/ididitagain
P.S.2: As a workaround (or the right way to do?), move specific subdirectories, like media, dev, dot-files, etc, in its own roles, and assign tags with role instead, as mentioned here http://docs.ansible.com/ansible/playbooks_tags.html in my site.yml?
roles:
  - { role: webserver, port: 5000, tags: [ 'web', 'foo' ] }",3
,2017,143,,,,,"Seems like default() filter does not work with ansible 2.0.0@devel

Issue Type: Bug Report
Ansible Version: 2.0.0 devel@8d16638
Ansible Configuration:
[defaults]
host_key_checking = False
forks=20

Environment: Mac OS X El Capitan 10.11.1
Summary
I have a task like this:
    - name: Spinning up EC2 instances
      ec2:
        key_name: ""{{ instances_keypair }}""
        instance_type: ""{{ item.0.instance_type }}""
        image: ""{{ item.0.image }}""
        instance_tags: ""{{ item.0.instance_tags }}""
        exact_count: ""{{ item.1.exact_count }}""
        count_tag: ""{{ item.0.count_tag }}""
        vpc_subnet_id: ""{{ item.1.vpc_subnet_id }}""
        group: ""{{ item.0.group }}""
        volumes: ""{{ item.0.volumes }}""
        zone: ""{{ ec2_region + item.1.az }}""
        region: ""{{ ec2_region }}""
        wait: ""{{ item.0.wait|default(yes) }}""
        wait_timeout: ""{{ item.0.wait_timeout|default(300) }}""
      register: ec2
      with_subelements:
        - ec2_instances
        - azs

...where ec2_instances is:
ec2_instances:
  # Memcache Servers
  - type: memcache
    instance_type: m3.medium
    image: ""{{ image_id }}""
    group: ['private']
    instance_tags:
      Name: ""memcache-{{ env }}""
      role: memcache
      environment: ""{{ env }}""
      memcache_environment: ""{{ env }}""
      deployment: ansible
    volumes:
      - device_name: /dev/sdp
        # For any volume, a volume size less than 1 will be interpreted as a request not to create the volume.
        volume_size: 0
        delete_on_termination: true
    count_tag:
      role: memcache
      environment: ""{{ env }}""
      memcache_environment: ""{{ env }}""
      deployment: ansible
    azs:
      - az: a
        # Private Subnet ID in ""a""
        vpc_subnet_id: ""{{ subnets['a']['private'] }}""
        exact_count: 0
      - az: b
        # Private Subnet ID in ""b""
        vpc_subnet_id: ""{{ subnets['b']['private'] }}""
        exact_count: 1
      - az: d
        # Private Subnet ID in ""d""
        vpc_subnet_id: ""{{ subnets['d']['private'] }}""
        exact_count: 0
      - az: e
        # Private Subnet ID in ""e""
        vpc_subnet_id: ""{{ subnets['e']['private'] }}""
        exact_count: 0
    wait: yes
    wait_timeout: 360

  #1 redis cluster (2 nodes)
  - type: redis
    instance_type: m3.medium
    image: ""{{ image_id }}""
    group: ['private']
    instance_tags:
      Name: ""redis-{{ env }}""
      role: redis
      environment: ""{{ env }}""
      redis_environment: ""{{ env }}""
      deployment: ansible
    count_tag:
      role: redis
      environment: ""{{ env }}""
      redis_environment: ""{{ env }}""
      deployment: ansible
    volumes:
      - device_name: /dev/sdp
        volume_size: 10
        delete_on_termination: true
    azs:
      - az: a
        # Private Subnet ID in ""a""
        vpc_subnet_id: ""{{ subnets['a']['private'] }}""
        exact_count: 1
      - az: b
        # Private Subnet ID in ""b""
        vpc_subnet_id: ""{{ subnets['b']['private'] }}""
        exact_count: 1
      - az: d
        # Private Subnet ID in ""d""
        vpc_subnet_id: ""{{ subnets['d']['private'] }}""
        exact_count: 0
      - az: e
        # Private Subnet ID in ""e""
        vpc_subnet_id: ""{{ subnets['e']['private'] }}""
        exact_count: 0

When I execute the task - I face an error when we reach the block without defined (that redis cluster in my case):
    wait: yes
    wait_timeout: 360

The error is:
fatal: [localhost]: FAILED! => {""failed"": true, ""msg"": ""ERROR! 'yes' is undefined""}

Expected Results: workeable default() filter",3
,2018,142,,,,,"Playbooks are runned over other group of hosts than listed by list-hosts

Issue Type:
Bug report
Ansible Version:
starting from commit ae9843f
ansible 1.5 is affected
Environment:
at last Debian 6
Summary:
Please summarize your request in this space.  You will earn bonus points for being succinct, but please add enough detail so we can understand the request.
Steps To Reproduce:
run

ansible-playbook -C site.yml -i hosts --list-hosts
ansible-playbook -C site.yml -i hosts

using files
hosts:
[physical]
phost1

[phost1]
vm1.phost1

site.yml:

---
- hosts: physical
  tasks:
    - ping:

Expected Results:
Groups not declared as children of group 'physical' should not be involved.
playbook: site.yml

  play #1 (physical): host count=1
    phost1


PLAY [physical] *************************************************************** 

TASK: [ping ] ***************************************************************** 
ok: [phost1]

PLAY RECAP ******************************************************************** 
phost1                     : ok=1    changed=0    unreachable=0    failed=0   

Actual Results:
Hosts from groups named after hosts included in group 'physical' (not declared as group's children) are involved by play.
playbook: site.yml

  play #1 (physical): host count=1
    phost1


PLAY [physical] *************************************************************** 

TASK: [ping ] ***************************************************************** 
ok: [vm1.phost1]
ok: [phost1]

PLAY RECAP ******************************************************************** 
phost1                     : ok=1    changed=0    unreachable=0    failed=0   
vm1.phost1                 : ok=1    changed=0    unreachable=0    failed=0",3
,2019,148,,,,,"Variables from group_vars/all are not being picked up when using inventory script

In our case we have seen the above, I still need a minimal test-case to demonstrate this.",3
,2020,148,,,,,"Duplicated newline in Jinja2 variable when copy module is used

Issue Type: Bug Report
Ansible Version: ansible 1.7.2
Environment: Arch Linux
Summary: Variable content gets newlines duplicated when pasted into a file via the copy module
Steps To Reproduce:

Add a step copy: content=""{{ ssl_private_key }}"" dest=/etc/ssl/private/foo.pem in a playbook
Write the content of ssl_private_key in an ansible-vault vars file
Run the playbook
Check if /etc/ssl/private/foo.pem will have an extra newline for each line in the ssl_private_key variable

Expected Results: if the variable ssl_private_key is defined like this:
ssl_private_key: |
   -----BEGIN PRIVATE KEY-----
   MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0
   MQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv
   ZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ
   -----END PRIVATE KEY-----

I expect to have this in /etc/ssl/private/foo.pem:
-----BEGIN PRIVATE KEY-----
MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0
MQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv
ZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ
-----END PRIVATE KEY-----

And this is what I actually get with Ansible 1.7.1 and some previous versions.
Actual Results: But instead, with Ansible 1.7.2 the result will have duplicated \n:
-----BEGIN PRIVATE KEY-----

MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0

MQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv

ZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ

-----END PRIVATE KEY-----",3
,2021,142,,,,,"synchronize action should maybe force the connection for hosts named localhost (edited title)

Issue Type:
Bug Report
Ansible Version:
ansible 1.6 (devel 317c2f4) last updated 2014/04/03 12:41:29 (GMT +200)
Environment:
openSUSE 12.2 (x86_64)
Summary:
Using the synchronize action while deploying to a VM listening on localhost confuses ansible as it tries to unnecessarily delegate to localhost, and also forgets to clear the port.
Steps To Reproduce:
I'm running a simple playbook to deploy some software to a VM running on localhost:2222.
Inventory file:
[localvm]
localhost ansible_ssh_user=root ansible_ssh_port=2222

tasks/main.yml:
- name: Install via rsync
  synchronize: src=../pkg dest=/srv/pkg/

This fails because it tries to connect to 127.0.0.1:2222 with my current username. What is happening, apparently, is that it assumes I'm doing a delegate action and tries to connect to localhost with my current username, but retains the faulty port. There is no need to delegate at all.
Expected Results:
I'd assume it would simply run rsync locally, without delegation, to deploy to the VM.
Actual Results:
It fails because it tries to run an ssh connection to <username>@127.0.0.1 port 2222, which does not work because <username> does not exist on the VM, and port 2222 is the VM. It would succeed on port 22, but that's not necessary.
The following change also makes the above work for me, so it is indeed some bogus delegation business. (The second hunk is only necessary because the first change removes the conn.delegate attribute.)
--- a/lib/ansible/runner/action_plugins/synchronize.py
+++ b/lib/ansible/runner/action_plugins/synchronize.py
@@ -81,7 +81,7 @@ class ActionModule(object):
         self.transport_overridden = False

         if inject.get('delegate_to') is None:
-            inject['delegate_to'] = '127.0.0.1'
+            # inject['delegate_to'] = '127.0.0.1'
             # IF original transport is not local, override transport and disable sudo.
             if self.original_transport != 'local':
                 inject['ansible_connection'] = 'local'
@@ -136,11 +136,11 @@ class ActionModule(object):

         # CHECK DELEGATE HOST INFO
         use_delegate = False
-        if conn.delegate != conn.host:
-            if 'hostvars' in inject:
-                if conn.delegate in inject['hostvars'] and self.original_transport != 'local':
-                    # use a delegate host instead of localhost
-                    use_delegate = True
+        # if conn.delegate != conn.host:
+        #     if 'hostvars' in inject:
+        #         if conn.delegate in inject['hostvars'] and self.original_transport != 'local':
+        #             # use a delegate host instead of localhost
+        #             use_delegate = True

         # COMPARE DELEGATE, HOST AND TRANSPORT
         process_args = False",3
,2022,148,,,,,"Docker - dns argument not working

Issue Type:
Bug Report
Ansible Version:
Ansible 1.7 (but had the same issue with 1.6.1 and 1.6)
Environment:
Running ansible from Mac OS
Managing Debian
Summary:
With the docker module, using the dns argument does not add the dns to the container. (nothing's added to the container's /etc/resolv.conf)
Steps To Reproduce:
    - name: Create docker proxy
      docker: image=ubuntu name=test hostname=test count=1 privileged=yes dns=127.0.0.1 command=/bin/bash


Expected Results:
ssh containers_ip
cat /etc/resolv.conf

nameserver 127.0.0.1 should appear at the top of the file
Actual Results:
nameserver 127.0.0.1 does not appear",3
,2023,148,,,,,"--limit does not work with intersecting hostgroups

Issue Type:
Bug Report, Feature Idea
Ansible Version:
ansible 1.5.5
Environment:
N/A
Summary:
I'll be nice, if the --limit switch, will limit task execution to the given hostgroup, also when both hostgroups consists of the same hosts
Steps To Reproduce:
Inventory:
[webservers]
webAA

[webservers-with-cache]
webAA
webBB

Playbook:
---
- hosts: webservers
  remote_user: a
  tasks:
  - shell: id
- hosts: webservers-with-cache
  remote_user: b
  tasks:
  - shell: uname -a
$ ansible-playbook  -v -i ... --limit webservers playbook.yml
Expected Results:
PLAY [webservers] *************************************************************

GATHERING FACTS ***************************************************************
ok: [webAA]

TASK: [shell id] **************************************************************
changed: [webAA] => {""changed"": true, ""cmd"": ""id "", 
""delta"": ""0:00:00.004137"", ""end"": ""2014-05-05 18:31:23.507347"", ""rc"": 0,
""start"": ""2014-05-05 18:31:23.503210"", ""stderr"": """", 
""stdout"": ""uid=1000(a) gid=1000(a) groups=1000(a)""}

PLAY RECAP ********************************************************************
webAA              : ok=2    changed=1    unreachable=0    failed=0

Actual Results:
PLAY [webservers] *************************************************************

GATHERING FACTS ***************************************************************
ok: [webAA]

TASK: [shell id] **************************************************************
changed: [webAA] => {""changed"": true, ""cmd"": ""id "", 
""delta"": ""0:00:00.004137"", ""end"": ""2014-05-05 18:31:23.507347"", ""rc"": 0, 
""start"": ""2014-05-05 18:31:23.503210"", ""stderr"": """", 
""stdout"": ""uid=1000(a) gid=1000(a) groups=1000(a)""}

PLAY [webservers-with-cache] **************************************************

GATHERING FACTS ***************************************************************
ok: [webAA]

TASK: [shell uname -a] ********************************************************
changed: [webAA] => {""changed"": true, ""cmd"": ""uname -a "", 
""delta"": ""0:00:00.002848"", ""end"": ""2014-05-05 18:31:25.982012"", ""rc"": 0, 
""start"": ""2014-05-05 18:31:25.979164"", ""stderr"": """", 
""stdout"": ""Linux ... 2.6.18-371.3.1.el5 #1 SMP Mon Nov 11 03:23:58 ES..""}

PLAY RECAP ********************************************************************
webAA              : ok=4    changed=2    unreachable=0    failed=0",3
,2024,148,,,,,"sudo options in connection.py (CentOS 5.4, -u and -k incompatible?)

sudo: the `-u' and '-k' options may not be used together
I modified below, and fine.
connection.py
        sudocmd = 'sudo -k -p ""%s"" -u %s -- ""$SHELL"" -c %s' % (prompt,

        sudocmd = 'sudo -k;sudo -p ""%s"" -u %s -- ""$SHELL"" -c %s' % (prompt,",3
,2025,142,,,,,"Template variable stuff broken

Yesterday everything was working fine, but after a git pull; make install my run borked big time.
I got:
At revision 233.
Traceback (most recent call last):
  File ""/usr/bin/ansible-playbook"", line 172, in <module>
    sys.exit(main(sys.argv[1:]))
  File ""/usr/bin/ansible-playbook"", line 118, in main
    subset=options.subset,
  File ""/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py"", line 119, in __init__
    (self.playbook, self.play_basedirs) = self._load_playbook_from_file(playbook)
  File ""/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py"", line 169, in     _load_playbook_from_file
    p['vars'].update(incvars)
AttributeError: 'list' object has no attribute 'update'

Investigating this pointed me to my issue playbook, containing
# vim:ff=unix ts=4 sw=4 ai expandtab
# $Id: init.yml 209 2012-11-11 13:26:06Z tonk $

---
- hosts: all
  tasks:
      - name: deploy issue file
        template: src=issue.in dest=/etc/issue owner=root mode=0444

And the template containing
  ------------------------------------------------------------------------------
                               -- W A R N I N G --
                  UNAUTHORIZED ACCESS STRICTLY PROHIBITED!!
  ------------------------------------------------------------------------------
           System Name : {{ ""%-25s""|format(ansible_hostname) }} Location : {{ location }}
           Managed by  : {{ ""%-25s""|format(name)             }} Room     : {{ room }}
  ------------------------------------------------------------------------------
{% if issueremarks is defined %}
{{ issueremarks.center(80) }}
  ------------------------------------------------------------------------------
{% endif %}

When I roll back to the code of yesterday, things work again.
It does look as if an undefined variable is used things break. But that's what the is defined is for, so there seems to be a bug in the template stuff of last night.
Could you look into that, please?",3
,2026,144,,,,,"Hipchat Callback not working

ISSUE TYPE

Bug Report

ANSIBLE VERSION
ansible 2.1.0.0
  config file = /root/ansible/ansible.cfg
  configured module search path = ['modules']

CONFIGURATION
[defaults]
library = modules
log_path = /tmp/ansible.log
roles_path = roles
callback_plugins = callbacks/
deprecation_warnings=False
callback_whitelist = hipchat

OS / ENVIRONMENT
CentOS7
SUMMARY
Hipchat Callback: https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/callback/hipchat.py
is not working.
Vars can not be set.
STEPS TO REPRODUCE
Enable hipchat callback via ansible.cfg whitelisting.
Configure the required Hipchat ENV-Vars.
Run any playbook, following error occurs:
PLAY [Staging Packages] ********************************************************
 [WARNING]: Failure using method (v2_playbook_on_play_start) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x31c4750>):
'Play' object has no attribute 'playbook'
 [WARNING]: Failure using method (v2_playbook_on_stats) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x2c4c750>):
'CallbackModule' object has no attribute 'display'

EXPECTED RESULTS
Message send to hipchat room.
ACTUAL RESULTS
Hipchat message not working
MISC
The display error can be solved by changing the callback from:
self.display.warning('
to
self._display.warning('",3
,2027,142,,,,,"Add documentation for privilege_escalation/become to ansible.cfg

By looking at the example file here it looks as though the new group would look similar to this:
[privilege_escalation]
become=True
become_method='sudo'
become_user='root'
become_ask_pass=False

I can create a pull request to add that section to the docs, just wanted to confirm that will be the correct setup moving forward.
Docs:

Become docs
Ansible configuration file docs",3
,2028,143,,,,,"Delegated Hosts Don't Use Inventory Details or Confirm Existence In Inventory

I wasn't sure if this was a bug or intended behavior. When I think about the issue, it seems more intuitive that you'd use the same name that you would for a host in your inventory and expect Ansible to use the same connection details when running the task.
Issue Type: Bug Report
Ansible Version: 1.7
Environment: Arch to drive, managing Ubuntu 12.04
Summary:
When running a task with delegate_to it doesn't look the provided host up in the inventory. You can pass it anything and it will try to SSH to it. Since that's the case it also doesn't properly look up the ansible_ssh_port for the delegated host.
Steps To Reproduce:
Hosts File:
(Using 127.0.0.1 as an example, but assume that's a WAN IP with a bunch of NAT one-to-many forwards.)
 host1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22
 host2 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22222

Playbook:
- name: A Most Noble Reproduction
  hosts: host1
  remote_user: root

  tasks:
    - name: Do The Delegationating
      shell: hostname
      delegate_to: host2

    - name: Do The Delegationating, Part Deux
      shell: echo 'If you see this in your syslog maybe someone should disable passwordless root logins at the googles.'
      delegate_to: google.com

Expected Results:
changed: [host1 -> host2] => {""changed"": true, ""cmd"": ""hostname"", ""stderr"": """", ""stdout"": ""host2""}
fatal: [host1 -> google.com] => Host google.com ain't all up in your inventory.

Actual Results:
changed: [host1 -> host2] => {""changed"": true, ""cmd"": ""hostname"", ""stderr"": """", ""stdout"": ""host1""}
fatal: [host1 -> google.com] => SSH encountered an unknown error during the connection. We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help diagnose the issue

Edit: Seems likely related to Issue 8224.
Edit2: Made the title better. Seemed ambiguous/not as explanatory as it could be.",3
,2029,140,,,,,"SSH connection got stuck when IP and host have different keys in known_hosts

In following situatation, I'd like to have some kind of error report, but now the connection just got stuck.
I reinstalled one machine, and tried to use same IP and host after installation. The initial ansible connection reported nicely

fatal: [node2-jenkins-slave.dev.sysart.fi] => Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this.  Please add this host's fingerprint to your known_hosts file to manage this host.

After adding doing this and trying to connect, the connection got stuck

ansible -i hosts node2-jenkins-slave.dev.sysart.fi -m ping -u root -k -vvvv
SSH password:
<node2-jenkins-slave.dev.sysart.fi> ESTABLISH CONNECTION FOR USER: root
<node2-jenkins-slave.dev.sysart.fi> REMOTE_MODULE ping
<node2-jenkins-slave.dev.sysart.fi> EXEC sshpass -d6 ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/jyrki/.ansible/cp/ansible-ssh-%h-%p-%r"" -o GSSAPIAuthentication=no -o PubkeyAuthentication=no -o User=root -o ConnectTimeout=10 node2-jenkins-slave.dev.sysart.fi /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678 && echo $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678'

I think that the reason for this had something to do with the warning I got with ssh

ssh node2-jenkins-slave.dev.sysart.fi
Warning: the ECDSA host key for 'node2-jenkins-slave.dev.sysart.fi' differs from the key for the IP >address '192.168.179.43'
Offending key for IP in /home/jyrki/.ssh/known_hosts:107
Matching host key in /home/jyrki/.ssh/known_hosts:133

After removing both keys and adding the host to known_hosts, everything worked.",3
,2030,148,,,,,"Single * pattern not working

Issue Type:
Bug Report
Ansible Version:
$ ansible --version
ansible 1.9.1
configured module search path = None
Ansible Configuration:
I think I changed nothing to the base configuration.
Environment:
Centos 7
Summary:
In the docs it says it is possible to use a single * pattern to target every host, but it doesn't seem to work for me.
Steps To Reproduce:
$ ansible * -m ping
Expected Results:
Same result as $ ansible all -m ping, which is working fine for me.
Actual Results:
Usage: ansible <host-pattern> [options]",3
,2031,148,,,,,"ansible 1.7 passing an argument with newlines to the shell module eats newlines (edited title)

I'm  using quite a lot of multiline strings, and according to http://stackoverflow.com/questions/3790454/in-yaml-how-do-i-break-a-string-over-multiple-lines the way this works in YAML is to use | isntead of > to preserve newlines.
This worked in ansible 1.6.x without problems.
Now, newlines are stripped away..
Example:

---
- hosts: all
  remote_user: root
  tasks:
  - name: bla
    args:
      executable: /bin/bash
    shell: |
      echo hi
      echo hi2

This should print hi and hi2 in two lines but it doesn't anymore.
It breaks a lot of ansible code for me which, unfortunately, relies on shell scripts.",3
,2032,148,,,,,"C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.

ISSUE TYPE


Bug Report

COMPONENT NAME

C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
ANSIBLE VERSION



CONFIGURATION

OS / ENVIRONMENT

SUMMARY

STEPS TO REPRODUCE




EXPECTED RESULTS

ACTUAL RESULTS",3
,2033,148,,,,,"Docs ansible-1.1: ""register"" example broken?

[root@util01 ansible]# ansible-playbook --version
ansible-playbook 1.1
[root@util01 ansible]# rpm -q ansible
ansible-1.1-1.el6.noarch
[root@util01 ansible]# rpm -q --qf ""%{NAME} - %{VENDOR}\n"" ansible
ansible - Fedora Project
[root@util01 ansible]# which ansible-playbook
alias ansible-playbook='ansible-playbook -v -i /root/ansible/hosts'
        /usr/bin/ansible-playbook

Description:
The example [1] for register appears to be broken.
Steps to reproduce:

Copy example from [1] into a yaml file
Run the file with ansible-playbook

[root@util01 tmp]# grep ""hi"" /etc/motd
This motd has the word 'hi' in it
[root@util01 tmp]# cat test_when.yaml
- name: test play
  hosts: util

  tasks:

      - action: shell cat /etc/motd
        register: motd_contents

      - action: shell echo ""motd contains the word hi""
        when: motd_contents.stdout.find('hi') != -1

Expected Result:
Ansible prints the message ""motd contains the word hi""
Observed result:
[root@util01 tmp]# ansible-playbook test_when.yaml
ERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1

This is failing when it gets to the final 'else' clause in compile_when_to_only_if. I added some debugging statements:
    else:
        print ""Tokens: "" + str(tokens)
        print map(lambda t: type(t), tokens)
        raise errors.AnsibleError(""invalid usage of when_ operator: %s"" % expression)

which outputs:
[root@util01 tmp]# ansible-playbook test_when.yaml
Tokens: [""motd_contents.stdout.find('hi')"", '!=', '-1']
[<type 'str'>, <type 'str'>, <type 'str'>]
ERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1

[1] http://ansible.cc/docs/playbooks2.html#register-variables",3
,2034,144,,,,,"New keyword for 'with_sequence' in order to skip task if count < 0 or start > end

Issue Type: Feature request
Ansible Version: 1.8.4
Environment: N/A
Summary:
When using with_sequence ""programmatically"" (i.e., in combination with variables), it would be useful to allow a negative value for the count argument, or a start value that is larger than end. Currently this results in an error (""can't count backwards""), but it could be useful to simply skip the task if the range is empty. This could be achieved by adding an extra keyword, e.g. skip_with_emtpy_range (although it should probably be less verbose), which would be False by default for backwards compatibility.
Example:
---
- hosts: all
  gather_facts: False
  vars:
    - M: 2
    - N: 1
  tasks:
    - name: This taks should be skipped because 'skip_with_empty_range' is True.
      command: echo ""Hello world""
      with_sequence: start={{ M }} end={{ N }} skip_with_empty_range=True
      #with_sequence: count={{ M - N }} skip_with_empty_range=True
Expected Results:
The task should be skipped because skip_with_empty_range is True:
TASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** 
skipping: [localhost]

Actual Results:
Currently the task fails with the error message ""can't count backwards"" (of course, skip_with_emtpy_range needs to be omitted when running the example above because it is not supported yet).
TASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** 
fatal: [localhost] => can't count backwards

FATAL: all hosts have already failed -- aborting",3
,2035,142,,,,,"Could not create retry file '*.retry'. [Errno 2] No such file or directory: ''

ISSUE TYPE

Bug Report

ANSIBLE VERSION
ansible 2.1.1.0 (stable-2.1 4d4bbcbb33) last updated 2016/06/20 08:55:21 (GMT +200)

CONFIGURATION
N/A
OS / ENVIRONMENT
Ubuntu
SUMMARY
If retry_files_save_path isn't set in ansible.cfg, when a playbook fails, a retry files tries to be created in an empty directory.
STEPS TO REPRODUCE
Make any playbook fail.
EXPECTED RESULTS
A try file should be created in the user home directory.
ACTUAL RESULTS
A warning message is printed and a retry file is not created:
 [WARNING]: Could not create retry file 'test_fail.retry'.         [Errno 2] No such file or directory: ''

It looks like the default value for the retry files isn't set to be ~/ like it's mentioned in the documentation.",3
,2036,144,,,,,"Unable to use some environment variables

ISSUE TYPE: Bug report
COMPONENT NAME: ansible_env
ANSIBLE VERSION:
ansible 2.2.1.0
config file = /etc/ansible/ansible.cfg
configured module search path = ['/usr/share/my_modules/', '/etc/ansible/roles/glassfish/library']
CONFIGURATION:
defined values under [default]
OS: Debian Jessie
SUMMARY: Ansible is unable to interpolate an environment variable when it is called from a hosts file or a group_vars/all file.
STEPS TO REPRODUCE:
In /etc/profile.d/required_env_vars.sh --
DEPLOY_DIR=/myhome/deployments
In testPlaybook.yml
deploy_dir = ""{{ ansible_env.DEPLOY_DIR }}/RT{{rt_number}}""
where rt_number is defined via vars_prompt
This fails because DEPLOY_DIR is not found in the output of the setup command for the host (looked in the ansible_env), but HOME is. So it looks like Ansible is unable to use environment variables defined in a profile.d script.
As a work-around, I added an [all:vars] section the inventories/poc/hosts file --
[all:vars]
deploy_dir = $HOME/deployments --> did not work
deploy_dir = /myhome/deployments --> worked
deploy_dir = $DEPLOY_DIR --> did not work
deploy_dir = ""{{ ansible_env.DEPLOY_DIR }}"" --> did not work
Since the second one worked (hardcoded value), I removed the [all:vars] section and used an inventories/poc/group_vars/all file --
deploy_dir = /myhome/deployments
This resulted in the error below --
ERROR! Unexpected Exception: dictionary update sequence element #0 has length 1; 2 is required
the full traceback was:
Traceback (most recent call last):
File ""/usr/bin/ansible-playbook"", line 103, in 
exit_code = cli.run()
File ""/usr/lib/python2.7/dist-packages/ansible/cli/playbook.py"", line 132, in run
inventory = Inventory(loader=loader, variable_manager=variable_manager, host_list=self.options.inventory)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 98, in init
self.parse_inventory(host_list)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 165, in parse_inventory
group.vars = combine_vars(group.vars, self.get_group_variables(group.name))
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 555, in get_group_variables
self._vars_per_group[groupname] = self._get_group_variables(groupname, vault_password=vault_password)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 573, in _get_group_variables
vars = combine_vars(vars, self.get_group_vars(group))
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 775, in get_group_vars
return self._get_hostgroup_vars(host=None, group=group, new_pb_basedir=new_pb_basedir, return_results=return_results)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 839, in _get_hostgroup_vars
host_results = self._variable_manager.add_group_vars_file(base_path, self._loader)
File ""/usr/lib/python2.7/dist-packages/ansible/vars/init.py"", line 619, in add_group_vars_file
data = self._load_inventory_file(path, loader)
File ""/usr/lib/python2.7/dist-packages/ansible/vars/init.py"", line 577, in _load_inventory_file
rval.update(data)
ValueError: dictionary update sequence element #0 has length 1; 2 is required
On my target server, I also tried to set up DEPLOY_DIR as an environment variable in various places --

/etc/environment -- which is not preferred because it can't do variable interpolation (only key=val pairs, no variables on the right hand side). The output of the env command showed the variables I defined as expected.
/etc/profile -- The output of the env command showed the variables defined also.
/etc/profile.d/required_env_vars.sh -- this is preferred because I'd like to keep /etc/profile the same across all servers, and just add custom environment variables via profile.d script. The output of the env command DID NOT show the variables defined, but doing ""echo $variable"" showed the expected value

None of these affected the content of ansible_env (I thought ansible_env would get the variables if the were returned by the ""env"" command?).
So it looks like Ansible does not include all the environment variables defined in its ""environment"" (ansible_env).
UPDATE 1
Okay. My bad. I changed the content of group_vars/all to use a colon instead of an equal and it worked.
deploy_dir : /home/b013000915/deployments
Got confused because in some cases (e.g. facts.d content, some modules), I had to use an equal sign.
But the other issue (not getting some of the environment variables defined) still persists.
UPDATE 2
I also tried using lookup to get the value of the following variables on the target node but it did not work for custom variables (defined via /etc/profile)
In /etc/profile
DEPLOY_DIR=/myhome/deployments
In testPlaybook.yml

name: check using lookup env
debug:
msg: ""deploy_dir is {{ lookup('env', 'DEPLOY_DIR') }}, home is {{ lookup('env', 'HOME') }}""

Output was
ok: [IP address] => {
""msg"": ""deploy_dir is , home is /myhome""
}",3
,2037,143,,,,,"--limit isn't honored at all

Issue Type:
Bug Report
Ansible Version:
$ ansible --version
ansible 2.0.0 (devel c30e464388) last updated 2015/09/22 16:21:32 (GMT +200)
  lib/ansible/modules/core: (detached HEAD 59afecace4) last updated 2015/09/22 16:21:46 (GMT +200)
  lib/ansible/modules/extras: (detached HEAD dee690d7f4) last updated 2015/09/22 16:21:59 (GMT +200)
  config file = 
  configured module search path = None

Ansible Configuration:
n/a
Environment:
n/a
Summary:
I have a simple inventory with four hosts (master1, master2, slave1, slave2) in two different groups (app-server, app-slave). When I use a host-pattern to select one group (e.g. app-server) and specify a host with --limit, ansible just selects all hosts. It does not matter if the host is contained in the selected group.
Steps To Reproduce:
hosts:
[app-server]
master1
master2

[app-slave]
slave1
slave2

Run some module with this inventory, use a host-pattern and limit it with a host. E.g:
ansible -i hosts -m debug -a ""msg=foo"" app-slave -l master1
ansible -i hosts -m debug -a ""msg=foo"" app-slave -l slave1
Expected Results:
No hosts matched

Actual Results:
$ ansible -i hosts -m debug -a ""msg=foo"" app-slave -l master1
slave1 | SUCCESS => {
    ""changed"": false, 
    ""msg"": ""foo""
}
slave2 | SUCCESS => {
    ""changed"": false, 
    ""msg"": ""foo""
}

$ ansible -i hosts -m debug -a ""msg=foo"" app-slave -l slave1
slave1 | SUCCESS => {
    ""changed"": false, 
    ""msg"": ""foo""
}
slave2 | SUCCESS => {
    ""changed"": false, 
    ""msg"": ""foo""
}",3
,2038,142,,,,,"Update, remove or migrate CODING_GUIDELINES.md

ISSUE TYPE

Documentation Report

COMPONENT NAME
CODING_GUIDELINES.md
ANSIBLE VERSION
N/A

CONFIGURATION
N/A
OS / ENVIRONMENT
N/A
SUMMARY
The CODING_GUIDELINES.md file is extremely out of date.  It contains information about coding style, tests, etc.
I bean looking into this document, but found the amount of information needing updating a bit daunting for me to undertake at the moment.
This file should either be updated, removed and potentially migrated into the docsite.
STEPS TO REPRODUCE
N/A
EXPECTED RESULTS
N/A
ACTUAL RESULTS
N/A",3
,2039,148,,,,,"search filter on an undefined variable returns a non-descriptive error

ISSUE TYPE
Bug Report
COMPONENT NAME
core
ANSIBLE VERSION
2.1
CONFIGURATION
OS / ENVIRONMENT
SUMMARY
In templates
Trying to use this template fails with a non-descriptive error when STRINGG is undefined:
{% if STRINGG | search('abc') %}
works!
{% endif %}
The error is:
TypeError: expected string or buffer

The error should be:
AnsibleUndefinedVariable: One or more undefined variables: 'STRINGG' is undefined

In plays
when: ""undefined | search('abc')""
The error is:
Failed to template {% if undefined | search('abc') %} True {% else %} False {% endif %}: an unexpected type error occurred. Error was expected string or buffer

STEPS TO REPRODUCE
EXPECTED RESULTS
ACTUAL RESULTS",3
,2040,140,,,,,"apt_repository: Failed to validate the SSL certificate for launchpad.net:443

Issue Type:
Bug Report
Ansible Version:
ansible 1.8.2
configured module search path = /usr/share/ansible
Environment:
Running from: Linux Mint 17.1 (based on Ubuntu 14.04)
Managing: Ubuntu 10.04, 12.04
Summary:
Adding a PPA with the apt_repository module fails with certificate validation problems.
Manually running add-apt-repository on the host works.
Steps To Reproduce:
- name: ""PHP: Add PHP 5.4 PPA""
  apt_repository:
    repo: ppa:ondrej/php5-oldstable
Expected Results:
PPA added under /etc/apt/sources.list.d/
Actual Results:
TASK: [php | PHP: Add PHP 5.4 PPA] ******************************************** 
failed: [host.example.com] => {""failed"": true}
msg: Failed to validate the SSL certificate for launchpad.net:443. Use validate_certs=no or make sure your managed systems have a valid CA certificate installed. Paths checked for this platform: /etc/ssl/certs, /etc/pki/ca-trust/extracted/pem, /etc/pki/tls/certs, /usr/share/ca-certificates/cacert.org, /etc/ansible",3
,2041,142,,,,,"scp_if_ssh parameter ignored in ansible 2.0.1.0

Issue Type:

Bug Report

Ansible Version:
ansible 2.0.1.0
config file = /etc/ansible/ansible.cfg
configured module search path = /home/share/library
Ansible Configuration:
$ cat /etc/ansible/ansible.cfg | grep scp
if True, make ansible use scp if the connection type is ssh
scp_if_ssh = True
Environment:
N/A
Summary:
Parameter scp_if_ssh is set to True in ansible.cfg. It fails to connect to host with error message ""unable to open an sftp connection"". It shouldn't be attempting an sftp connection.
Steps To Reproduce:
Run the following on a system that rejects sftp connections:
ansible  -m setup

$ ansible-playbook service-pack-6.2.x.yml 

PLAY ***************************************************************************

TASK [setup] *******************************************************************
fatal: [GoldenBoy]: FAILED! => {""failed"": true, ""msg"": ""failed to open a SFTP connection (Channel closed.)""}

NO MORE HOSTS LEFT *************************************************************
    to retry, use: --limit @service-pack-6.2.x.retry

PLAY RECAP *********************************************************************
GoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1
$ ansible-playbook service-pack-6.2.x.yml 

PLAY ***************************************************************************

TASK [setup] *******************************************************************
fatal: [GoldenBoy]: FAILED! => {""failed"": true, ""msg"": ""failed to open a SFTP connection (Channel closed.)""}

NO MORE HOSTS LEFT *************************************************************
    to retry, use: --limit @service-pack-6.2.x.retry

PLAY RECAP *********************************************************************
GoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1


When I uninstall ansible and reinstall 1.9.4 with no configuration changes setup works.
**$ pip uninstall ansible
<< ommitted >>
$ pip install ansible==1.9.4
<<ommitted>>
$ ansible --version
ansible 1.9.4
  config file = /etc/ansible/ansible.cfg
  configured module search path = /home/share/library
$ ansible tigh -m setup
tigh | success >> {
    ""ansible_facts"": {
        ""ansible_all_ipv4_addresses"": [
...
}, 
    ""changed"": false
}

Playbook works as well

$ ansible-playbook service-pack-6.2.x.yml 

PLAY [GoldenBoy] ************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [GoldenBoy]

TASK: [debug var=inventory_hostname] ****************************************** 
ok: [GoldenBoy] => {
    ""var"": {
        ""inventory_hostname"": ""GoldenBoy""
    }
}

TASK: [debug var=ansible_host] ************************************************ 
ok: [GoldenBoy] => {
    ""var"": {
        ""ansible_host"": ""ansible_host""
    }
}

TASK: [revert to snapshot] **************************************************** 
changed: [GoldenBoy -> 127.0.0.1]
...


Expected Results:
It uses scp to succeed.
Actual Results:
It attempts to connect with SFTP.
$ ansible GoldenBoy -m setup
GoldenBoy | FAILED! => {
    ""failed"": true, 
    ""msg"": ""failed to open a SFTP connection (Channel closed.)""
}",3
,2042,140,,,,,"Allow multiple vault passwords/files

vault password could keep prompting until empty password is supplied, vault file could take a list of files
This allows for having multiple vault files with different keys, good for ops team having access to all vaults but qa or dev having access only to specific ones",3
,2043,148,,,,,"Let the inventory file location be a directory

If the file is a directory, run all items within it, whether script or INI file, and blend the results.
This will allow the inventory directory to be used in conf.d form, or even one group per file.
It will also allow for hybrid EC2/local inventory, etc.",3
,2044,142,,,,,"Apt module does not work well when specifying version along with with_items loop

When trying to install several packages with this task:
- name: Install deb packages
  apt: name={{ item }}=1.0*
       update_cache=yes
       state=present
       force=yes
  with_items:
    - package1
    - package2

Only package2 (last one in the loop) is forced to install with 1.0 version. Package1 is installed with the latest version found on the repository. Running the playbook with ""-vvvv"" I can se how the command is eventually executed:
REMOTE_MODULE apt name=package1,package2=1.0* update_cache=yes state=present force=yes
I got it working by rewriting the task as following:
- name: Install deb packages
  apt: name={{ item }}
       update_cache=yes
       state=present
       force=yes
  with_items:
    - package1=1.0*
    - package2=1.0*",3
,2045,144,,,,,"Debug task is run even though dependent task is skipped

I found a case in ansible 1.9.4 (also present in 1.9.2) where a debug task with when: whatever|success runs even though the task that registers whatever was skipped.
I was able to reproduce with this playbook:
- hosts: all
  user: cloud-user

  tasks:
    - name: command that always fails
      shell: /bin/false
      ignore_errors: yes
      register: false_command

    - name: dummy git task
      git: accept_hostkey=yes
      when: false_command|success
      register: dummy_git

    - name: dummy debug
      debug:
        msg: ""This should never happen""
      when: dummy_git|success

which results in the following output:
$ ansible-playbook -i 10.60.3.31, ansible_problem.yml 

PLAY [all] ******************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [10.60.3.31]

TASK: [command that always fails] ********************************************* 
failed: [10.60.3.31] => {""changed"": true, ""cmd"": ""/bin/false"", ""delta"": ""0:00:00.002888"", ""end"": ""2015-11-13 11:26:25.460150"", ""rc"": 1, ""start"": ""2015-11-13 11:26:25.457262"", ""warnings"": []}
...ignoring

TASK: [dummy git task] ******************************************************** 
skipping: [10.60.3.31]

TASK: [dummy debug] *********************************************************** 
ok: [10.60.3.31] => {
    ""msg"": ""This should never happen""
}

PLAY RECAP ******************************************************************** 
10.60.3.31                 : ok=3    changed=1    unreachable=0    failed=0",3
,2046,142,,,,,"yum: ansible not able to find package

ansible-playbook/yum module isn't able to find a package that I can find manually. This issue 'goes away' if I install the package manually.
[wes@mgmt001 ~]$ ssh root@host.domain yum clean all
Loaded plugins: fastestmirror
Cleaning up Everything
Cleaning up list of fastest mirrors
[wes@mgmt001 ~]$ ssh root@host.domain yum list epel-release
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Available Packages
epel-release.noarch                        5-4                        private-repository
[wes@mgmt001 ~]$ ansible-playbook -i /tmp/all_hosts_badtz -f 30 ~/ansible/playbooks/company/company.yml

TASK: [yum: install epel-release] *********************

failed: [host.domain] => {""changed"": false, ""failed"": true, ""msg"": ""No Package matching 'epel-release' found available, installed or updated""}
From playbook:

name: ""yum: install epel-release""
action: yum pkg=epel-release state=latest",3
,2047,144,,,,,"Wrong temp path OSX

ISSUE TYPE

Bug Report

COMPONENT NAME
core
ANSIBLE VERSION
ansible 2.3.0.0
  config file = /Users/mateus/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Apr  4 2017, 08:47:57) [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)]

CONFIGURATION
ask_become_pass=True
ask_sudo_pass=True
local_tmp = /Users/mateus/.ansible/tmp
OS / ENVIRONMENT
macOS 101.12.5
SUMMARY
For some reason Ansible is trying to create a temp file on the wrong directory, since macOS uses /Users/ instead of /home/.
My $HOME variable is fine (/Users/mateus/)
Already tried setting local_tmp and remote_tmp.
As you can see below, the ControlPath is right, only the mkdir is wrong
ACTUAL RESULTS
<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus
<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=""/Users/mateus/.ssh/id_rsa""' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '""'""'echo ~ && sleep 0'""'""''
<[HIDDEN_IP]> (0, '/home/mateus\n', '')
<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus
<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=""/Users/mateus/.ssh/id_rsa""' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '""'""'( umask 77 && mkdir -p ""` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `"" && echo ansible-tmp-1495220924.36-187159279265629=""` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `"" ) && sleep 0'""'""''
<[HIDDEN_IP]> (1, '', 'mkdir: cannot create directory \xe2\x80\x98/home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629\xe2\x80\x99: No space left on device\n')",3
,2048,148,,,,,"async with command not working

Issue Type:

bug

Ansible Version:
ansible 2.0.0.2

Ansible Configuration:
default
Environment:
OS X Yosemite 10.10.4

Summary:
When doing async reboot using ansible, if I call /sbin/shutdown directly, it works.
However if I ran /bin/sleep first then, nothing happens.
Steps To Reproduce:
- name: Reboots machine to new kernel (async)
  command: /bin/sleep 5 && /sbin/shutdown -r now ""Reboot triggered by Ansible""
  async: 1
  poll:  0
  ignore_errors: true

syslog:
Jan 22 19:21:44 example ansible-async_wrapper: Starting module and watcher
Jan 22 19:21:44 example ansible-async_wrapper: Start watching 1282 (1)
Jan 22 19:21:44 example ansible-async_wrapper: Start module (1282)
Jan 22 19:21:44 example ansible-command: Invoked with warn=True executable=None chdir=None _raw_params=/bin/sleep 5 && /sbin/shutdown -r now ""Reboot triggered by Ansible"" removes=None creates=None _uses_shell=False
Jan 22 19:21:44 example ansible-async_wrapper: Module complete (1282)",3
,2049,148,,,,,"Allow group_vars and host_vars to override role variables

ISSUE TYPE


Feature Idea

COMPONENT NAME

Variable Precedence
ANSIBLE VERSION

ansible 2.1.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

CONFIGURATION

N/A
OS / ENVIRONMENT

N/A
SUMMARY

Change Ansible's variable precedence so that groups_vars and host_vars can override all role variables. Roles should be considered standalone or isolated modules and maintain there own variable precedence. Roles can use conditions to set OS specific variables in their vars folder. All variables defined in a role should be able to be overridden outside of the role by group_vars and host_vars. This would take care of the use case for being able to set OS specific variable defaults in a role and being able to override them.
STEPS TO REPRODUCE





EXPECTED RESULTS

ACTUAL RESULTS",3
,2050,148,,,,,"Different behavior for variables used in an included task in a dependent role since 2.0.2.0

ISSUE TYPE
Bug Report
COMPONENT NAME
roles
ANSIBLE VERSION
ansible 2.2.0 (devel d8a3feb976) last updated 2016/07/19 150214 (GMT -700)
lib/ansible/modules/core (detached HEAD 7de287237f) last updated 2016/07/15 125903 (GMT -700)
lib/ansible/modules/extras (detached HEAD 68ca157f3b) last updated 2016/07/15 125903 (GMT -700)
config file =
configured module search path = Default w/o overrides

CONFIGURATION
OS / ENVIRONMENT
N/A
SUMMARY
This one is really similar to #16729
When a dependency role (configured with allow_duplicates ) uses a variable set by a dependent role inside an included task and multiple role are using the same dependency role, the dependency role always use the variable defined in the last called role
STEPS TO REPRODUCE
Create a playbook
Add two role (role1 and role2) with a dependency on another role configured with allow_duplicates: yes (common-role)
Use a variable ( app_name) in an included task in the common role which is defined in role1/vars/main.yml and role2/vars/main.yml
Display the var value in a debug task inside the common role
Directory structure
 roles
     common-role
        meta
           included.yml
           main.yml
        tasks
            main.yml
     role1
        meta
           main.yml
        vars
              main.yml
     role2
         meta
            main.yml
         vars
             main.yml

playbook.yml
  - hosts: localhost
    roles:
      - { role: role1}
      - { role: role2 }

role/common-role/meta/main.yml
allow_duplicates: yes

roles/common-role/tasks/included.yml
- name: ""debug {{app_name}}""
  debug: msg=""apps_name --- {{app_name}}""

roles/common-role/tasks/main.yml
- include: tasks/included.yml

roles/role1/meta/main.yml
dependencies:
  - { role: common-role }

roles/role1/vars/main.yml
app_name: role1AppName

roles/role2/meta/main.yml
dependencies:
  - { role: common-role }

roles/role2/vars/main.yml
app_name: role2AppName

EXPECTED RESULTS
ansible 2.0.1.0 ( and the previous versions) was producing this :
PLAY ***************************************************************************
TASK [setup] *******************************************************************
ok: [localhost]
TASK [common-role : include] ***************************************************
included: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost
TASK [common-role : debug included role1AppName] *******************************
ok: [localhost] => {
    ""msg"": ""apps_name included --- role1AppName""
}
TASK [common-role : include] ***************************************************
included: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost
TASK [common-role : debug included role2AppName] *******************************
ok: [localhost] => {
    ""msg"": ""apps_name included --- role2AppName""
}
PLAY RECAP *********************************************************************
localhost                  : ok=5    changed=0    unreachable=0    failed=0

ACTUAL RESULTS
ansible 2.0.2+ produces this :
PLAY [localhost] ***************************************************************
TASK [setup] *******************************************************************
ok: [localhost]
TASK [common-role : debug included role2AppName] *******************************
ok: [localhost] => {
    ""msg"": ""apps_name included --- role2AppName""
}
TASK [common-role : debug included role2AppName] *******************************
ok: [localhost] => {
    ""msg"": ""apps_name included --- role2AppName""
}
PLAY RECAP *********************************************************************
localhost                  : ok=3    changed=0    unreachable=0    failed=0

tested with ansible 2.0.2.0 , the stable-2.1 branch and the devel branch, same results.",3
,2051,143,,,,,"Wait_for should return matches to groups in its search_regex

ISSUE TYPE

Feature Idea

COMPONENT NAME
wait_for
ANSIBLE VERSION
2.3

CONFIGURATION
n/a
OS / ENVIRONMENT
n/a
SUMMARY
When using the wait_for module to monitor, say, a file for a string matching a particular search_regex, it would be helpful if the matches to that regex were available in the module's output.
In my current use case, I'm monitoring a log file from a remote process which was started asynchronously and want to display certain pertinent information from it in the Ansible output.
STEPS TO REPRODUCE

 - name: Wait for message in log file
    wait_for:
        path: ""somefile.log'
        search_regex: ""SOMETHING_HAPPENED start_delimiter(.*)end_delimiter""
    register: foo

 - name: show text between delimiters
    debug: msg=""The text between delimiters was {{foo.result.matches[0]}}""
EXPECTED RESULTS
The wait_for task will pause until text matching the search_regex is found in somefile.log (or timeout occurs).  The text between the parens in the regex will be available somewhere in the variable registered in wait_for task.
ACTUAL RESULTS
The matches are not available in the registered variable.",3
,2052,144,,,,,"broken plugins cause UnboundLocalError: local variable 'obj' referenced before assignment

ISSUE TYPE

Bug Report

ANSIBLE VERSION
2.2 devel

CONFIGURATION
N/A
OS / ENVIRONMENT
N/A
SUMMARY
PluginLoader.all() creates a variable named ""obj"" in try/except, raises a warning if failed and then attempts to use the variable later even though it may not exist.
STEPS TO REPRODUCE
Create a bad plugin file in ""filter_plugins"" and run a playbook with a debug: var=
mkdir filter_plugins
touch filter_plugins/foobar.py

EXPECTED RESULTS
No traceback, just a warning.
ACTUAL RESULTS
task path: .../tasks/main.yml:57
[WARNING]: Skipping plugin (....library/<BROKEMODULE>.py) as it seems to be invalid: 'module' object has no attribute 'FilterModule'

An exception occurred during task execution. The full traceback is:
Traceback (most recent call last):
 File ""/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py"", line 124, in run
   res = self._execute()
 File ""/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py"", line 401, in _execute
   self._task.post_validate(templar=templar)
 File ""/usr/local/lib/python2.7/dist-packages/ansible/playbook/task.py"", line 246, in post_validate
   super(Task, self).post_validate(templar)
 File ""/usr/local/lib/python2.7/dist-packages/ansible/playbook/base.py"", line 317, in post_validate
   value = templar.template(getattr(self, name))
 File ""/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py"", line 358, in template
   d[k] = self.template(variable[k], preserve_trailing_newlines=preserve_trailing_newlines, fail_on_undefined=fail_on_undefined, overrides=overrides)
 File ""/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py"", line 330, in template
   result = self._do_template(variable, preserve_trailing_newlines=preserve_trailing_newlines, escape_backslashes=escape_backslashes, fail_on_undefined=fail_on_undefined, overrides=overrides)
 File ""/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py"", line 467, in _do_template
   myenv.filters.update(self._get_filters())
 File ""/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py"", line 186, in _get_filters
   plugins = [x for x in self._filter_loader.all()]
 File ""/usr/local/lib/python2.7/dist-packages/ansible/plugins/__init__.py"", line 388, in all
   obj = obj(*args, **kwargs)
UnboundLocalError: local variable 'obj' referenced before assignment",3
,2053,148,,,,,"vmware_guest doesn't create a properly formatted /etc/resolv.conf file

I'm trying Ansible 2.3 RC2
ISSUE TYPE

Bug Report

COMPONENT NAME
vmware_guest: customization: dns_servers
ANSIBLE VERSION
/usr/lib64/python2.6/site-packages/cryptography/__init__.py:26: DeprecationWarning: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of cryptography will drop support for Python 2.6
  DeprecationWarning
ansible 2.3.0.0
  config file = /opt/ansible/.ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)]

CONFIGURATION
forks=10

OS / ENVIRONMENT
Centos 6.8

SUMMARY
I want to provision a VMWare VM and  populate /etc/resolv.conf, so I've puth this in my playbook:
- vmware_guests:
      customization:
        domain: ""{{ guest_domain }}""
        dns_servers: ""{{ guest_dns_servers }}""
        dns_suffix: ""{{ guest_dns_suffix }}""

And defined this variable in a group_vars file
guest_dns_servers:
- 10.75.228.65
  10.75.228.66

But I get a wrongly formatted /etc/resolv.conf file:
[ansible@ansclient111 etc]$ cat resolv.conf
search  mydomain.com
nameserver      10.75.228.65 10.75.228.66

Instead of getting one nameserver line per server, I get all of them in a single line.
STEPS TO REPRODUCE
Provision the VM using the aforementioned playbook/group_vars file
EXPECTED RESULTS
Get a resolv.conf file like this:
search  mydomain.com
nameserver 10.75.228.65
nameserver 10.75.228.66

ACTUAL RESULTS
Got a resolv.conf file like this:
search  mydomain.com
nameserver      10.75.228.65 10.75.228.66",3
,2054,144,,,,,"Unable to manage windows server after binding an IIS https site with option ""Require Server Name Identification""

ISSUE TYPE


Bug Report

COMPONENT NAME

ANSIBLE VERSION

ansible 2.2.1.0
  config file = /home/rui/test/ansible.cfg
  configured module search path = Default w/o overrides

CONFIGURATION

inventory      = ./hosts
OS / ENVIRONMENT
running Ansible from: Linux ubuntu16_Ansible 4.4.0-59-generic
managing: Windows 2016 with IIS
SUMMARY
Unable to manage windows server after binding an IIS https site with option ""Require Server Name Identification""
STEPS TO REPRODUCE

On the Windows 2016 machine there is a IIS web site with binding for https on port 443 using a certificate.
The ansible commands and playbooks are able to manage this machine.
After I change the binding and select the option ""Require Server Name Identification"" (required to use a single IP address to service multiple sites with certificates using ""host name"") ansible stops communication with server.

I can only restore communication after I remove the binding and restart the windows server.

ansible win2016gui2 -m win_ping -vvvv

EXPECTED RESULTS
win2016gui2 | SUCCESS => {
""changed"": false,
""ping"": ""pong""
}
ACTUAL RESULTS


Using /home/rui/test/ansible.cfg as config file
Loading callback plugin minimal of type stdout, v2.0 from /usr/lib/python2.7/dist-packages/ansible/plugins/callback/__init__.pyc
Using module file /usr/lib/python2.7/dist-packages/ansible/modules/core/windows/win_ping.ps1
<192.168.233.141> ESTABLISH WINRM CONNECTION FOR USER: administrator on PORT 5986 TO 192.168.233.141
win2016gui2 | UNREACHABLE! => {
    ""changed"": false,
    ""msg"": ""ssl: (\""bad handshake: SysCallError(104, 'ECONNRESET')\"",)"",
    ""unreachable"": true
}",3
,2055,148,,,,,"ansible_memfree_mb fact combines disk-cache use of memory

Issue Type: Bug Report
Ansible Version: ansible 1.6.3
Environment: N/A
Summary:
I setup a play to alert whenever memory utilization is dangerously high. I immediately received a handful of alerts and was a bit concerned until I realized it takes into account memory being used by the disk cache and not 'real' memory utilization.
Steps To Reproduce:
To reproduce just compare the ansible_memfree_mb fact, which in my example is
        ""ansible_memfree_mb"": 10550,
        ""ansible_memtotal_mb"": 15042,

with the return value of running
free -m

Expected Results: I expected 'real' memory available
Actual Results:
In my case returns
             total       used       free     shared    buffers     cached
Mem:         15042       4483      10559        139        231       2859
-/+ buffers/cache:       1393      13649
Swap:            0          0          0

As you can see, the ansible_facts are correct but misleading, in this case saying only 10gb are free when in fact I have 13gb that are useable.
Perhaps this could be added as a ansible_nocache_memfree_mb fact?",3
,2056,141,,,,,"Include inventory modules in distribution

Currently inventory modules are only available on github.
It would be cool if they could be bundled in with the release so they can be used without needing to be separately downloaded",3
,2057,148,,,,,"Missing `xsltproc` Debian packaging README

packaging/debian/README.md does not include installation of xsltproc which provides missing local eponym command.
make deb
[...]
a2x -D docs/man/man1/ -d manpage -f manpage docs/man/man1/ansible.1.asciidoc
a2x: WARNING: --destination-dir option is only applicable to HTML based outputs
a2x: ERROR: ""xsltproc""  --stringparam callout.graphics 0 --stringparam navig.graphics 0 --stringparam admon.textlabel 1 --stringparam admon.graphics 0  ""/etc/asciidoc/docbook-xsl/manpage.xsl"" ""<snip>/docs/man/man1/ansible.1.xml"" returned non-zero exit status 127
Makefile:117: recipe for target 'docs/man/man1/ansible.1' failed
make: *** [docs/man/man1/ansible.1] Error 1
rm docs/man/man1/ansible.1.asciidoc",3
,2058,144,,,,,"Ansible inventory allows groups to have same name as hosts

Issue Type
Bug Report
Ansible Version:
ansible 1.6 (devel 9da26da) last updated 2014/03/18 11:15:04 (GMT +1000)
Environment:
N/A
Summary:
If you define a group containing another group without the :children, then the intended group is a host, but will also become a group when defining its contents.
The net result is that the host that should inherit characteristics of several layers of parents will not inherit those characteristics as the definitions will be:
grandparent -> parent(host)
parent(group) -> host
rather than
grandparent -> parent -> host
I believe that it should not be possible to define a group with the same name as a host (or at least a warning suggesting it might not be what you want should happen)
Steps To Reproduce:
See https://gist.github.com/willthames/9614054
Expected Results:
See https://gist.github.com/willthames/9614054
Actual Results:
See https://gist.github.com/willthames/9614054",3
,2059,142,,,,,"Inconsistent expansion of the variables in the lookup 'with_items'

Issue Type:
Bug Report
Ansible Version:

Production:

/usr/lib/python2.6/site-packages/ansible # ansible --version
ansible 1.4.1


Fresh clone (13.02.2014)

 /local/home/mike/dev/ansible-latest/ansible # ansible --version
ansible 1.5 (devel 6f405c8970) last updated 2014/02/13 13:14:25 (GMT +200)

Environment:
# uname -a
Linux xxx.cern.ch 2.6.32-431.3.1.el6.x86_64 #1 SMP Mon Jan 6 11:34:51 CET 2014 x86_64 x86_64 x86_64 GNU/Linux

 # lsb_release -a
LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: ScientificCERNSLC
Description:    Scientific Linux CERN SLC release 6.5 (Carbon)
Release:    6.5
Codename:   Carbon

Summary:
Dear Ansible Dev Team,
While migrating code from old ${...} variable expansion to new Jinja2 style {{...}}, I've noticed some incompatibilities with the previous syntax, while I would expect {{...}} be a full equivalent of ${...}, i.e the following case should work just fine:
- name: Java JDK installation
  action: yum2 name=""{{ item }}"" enablerepo=acc-external-do state=installed
  with_items:
     -   - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next }}


where jdk_next is a list of values:
jdk_pro: 1.7.0_45
jdk_6: 1.6.0_43
jdk_7: 1.7.0_45
jdk_next:
  - jdk1.7.0_45

Unfortunately, it does not work.
Steps To Reproduce:

Create the playbook, which uses roles, for example server.yml:

-
  hosts: server
  gather_facts: yes
  user: root
  vars:
    local_to_opt: yes
  roles:
    - base

And put the mentioned vars in role's ""base"" vars/ and mentioned task in role's ""base"" tasks/, as it should be.

Execute ansible playbook.

 ansible-playbook --connection=local server.yml

Expected Results:
With previous syntax:
- name: Java JDK installation
  action: yum2 name=""{{ item }}"" enablerepo=acc-external-do state=installed
  with_items:
    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}

- the result is just fine:
PLAY [server] ***************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [127.0.0.1]

TASK: [base | Java JDK installation] ****************************************** 
ok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)

PLAY RECAP ******************************************************************** 
127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   

Actual Results:
With the code written in Jinja2 style, I've received with the bad result:
PLAY [server] ***************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [127.0.0.1]

TASK: [base | Java JDK installation] ****************************************** 
failed: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']) => {""failed"": true, ""item"": ""jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']""}
msg: ['jdk1.7.0_45'] packages has been failed for installation;

FATAL: all hosts have already failed -- aborting

PLAY RECAP ******************************************************************** 
           to retry, use: --limit @/root/server.retry

127.0.0.1                  : ok=1    changed=0    unreachable=0    failed=1   

Then, even more interesting: I found, that it's not possible anymore specify variable enclosed in {{..}} as a first member of iteration sequence like this:
- name: Java JDK installation
  action: yum2 name=""{{ item }}"" enablerepo=acc-external-do state=installed
  with_items:
    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }}

The result is a syntax error:
ERROR: Syntax Error while loading YAML script, /var/lib/ansible/roles/base/tasks/main.yml
Note: The error may actually appear before this position: line 596, column 22

  with_items:
    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}

While the old syntax was ok:
PLAY [server] ***************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [127.0.0.1]

TASK: [base | Java JDK installation] ****************************************** 
ok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)

PLAY RECAP ******************************************************************** 
127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   

Quick debug & hacking showed the following things:

if you use original anisble syntax, the file expansion string, which arrives in the variable varname of the template() function of utils/template.py, is [u'jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45'], which is correct
if you use jinja2 syntax, the final expansion string, which arrives in the variable varname of the template() function of utils/template.py is [u""jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']""] which is incorrect.

I do not have comments about SyntaxError I've provided earlier as I have no time to debug it :(
Workaround
This hack is acceptable but still ugly - the idea is to use the join() Jinja2 filter, i.e. the following code:
- name: Java JDK installation
  action: yum2 name=""{{ item }}"" enablerepo=acc-external-do state=installed
  with_items:
    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next | join(' ') }}

works:
PLAY [server] ***************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [127.0.0.1]

TASK: [base | Java JDK installation] ****************************************** 
ok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)

PLAY RECAP ******************************************************************** 
127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0",3
,2060,145,,,,,"Implement RHEL5 python-dmidecode support

For older systems lacking sysfs I plan to implement python-dmidecode support in setup as a backup.",3
,2061,142,,,,,"include_once is skipped when required roles are partially skipped

I use an include_role task in a role, but it is skipped when I use a conditional depency role.
I tried to find more infos in the documentation, but there is nothing I can find to explain it if I do anything wrong...
ISSUE TYPE
Bug Report
COMPONENT NAME
include_role or meta/dependencies
ANSIBLE VERSION
ansible 2.3.0.0
  config file =
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Dec 18 2016, 07:03:39) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]

CONFIGURATION
Default configuration
OS / ENVIRONMENT
Mac OS X / Debian
SUMMARY
I use a include_role task which is skipped because I use conditional required roles.
STEPS TO REPRODUCE
roles/shell/meta/main.yml
# Role dependencies
dependencies:
  - { role: homebrew, when: ansible_os_family == 'Darwin' }
  - { role: apt, when: ansible_distribution == 'Debian' }
roles/shell/tasks/main.yml
---
# Tasks for roles

- name: Ensure Oh My Zsh required packages are installed
  package:
    name: ""{{ item }}""
    state: latest
  become: ""{{ ansible_os_family != 'Darwin' }}""
  with_items:
   - curl
   - git
   - zsh

- name: Ensure Zsh is the default shell
  shell: ""chsh -s /bin/zsh {{ansible_user_id}}""
  become: yes
  register: chsh_result
  changed_when: ""chsh_result.stderr.find('no changes made') == -1""

- name: Ensure Oh My Zsh is installed
  shell: ""sh -c \""$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\""""
  args:
    creates: ""{{ ansible_env.HOME}}/.oh-my-zsh""
    executable: /bin/zsh

- name: Ensure auto-suggestions Zsh plugin is installed
  git:
    repo: 'https://github.com/zsh-users/zsh-autosuggestions.git'
    dest: ""{{ ansible_env.HOME }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions""

- name: Ensure dotfiles are installed
  include_role:
    name: dotfiles
In the dotfiles role, nothing special, no required role, just some unconditional tasks:
roles/dotfiles/tasks/main.yml
- name: Ensure repository is cloned
  git:
    repo: 'https://github.com/maxwo/dotfiles.git'
    dest: ""{{ ansible_env.HOME }}/dotfiles""

- name: Ensure SSH dotfiles symlinks are set with correct permissions
  file:
    src: ""{{ ansible_env.HOME}}/dotfiles/ssh/{{ item }}""
    dest: ""{{ ansible_env.HOME }}/.ssh/{{ item }}""
    mode: 0600
    state: link
    force: yes
  with_items:
    - authorized_keys
    - config

- name: Ensure dotfiles standard symlinks are set
  file:
    src: ""{{ ansible_env.HOME }}/dotfiles/{{ item.src }}""
    dest: ""{{ ansible_env.HOME }}/{{ item.dest }}""
    state: link
    force: yes
  with_items:
    - { src: 'gitconfig', dest: '.gitconfig' }
    - { src: 'zshrc', dest: '.zshrc' }
EXPECTED RESULTS
The last include_role should be played.
ACTUAL RESULTS
The last include_role is skipped, whether it is played on a Debian machine or a Mac OS machine.
When I remove the role dependencies, everything is fine.
TASK [icopp.homebrew : Install Homebrew] *********************************************************************************************************************************
ok: [aki.local]

TASK [homebrew : Ensure Homebrew packages are up to date] ****************************************************************************************************************
changed: [aki.local]

TASK [apt : Ensure APT packages are up to date] **************************************************************************************************************************
skipping: [aki.local]

TASK [shell : Ensure Oh My Zsh required packages are installed] **********************************************************************************************************
ok: [aki.local] => (item=curl)
ok: [aki.local] => (item=git)
ok: [aki.local] => (item=zsh)

TASK [shell : Ensure Zsh is the default shell] ***************************************************************************************************************************
ok: [aki.local]

TASK [shell : Ensure Oh My Zsh is installed] *****************************************************************************************************************************
ok: [aki.local]

TASK [shell : Ensure auto-suggestions Zsh plugin is installed] ***********************************************************************************************************
ok: [aki.local]

TASK [dotfiles : Ensure repository is cloned] ****************************************************************************************************************************
skipping: [aki.local]

TASK [dotfiles : Ensure SSH dotfiles symlinks are set with correct permissions] ******************************************************************************************
skipping: [aki.local] => (item=authorized_keys)
skipping: [aki.local] => (item=config)

TASK [dotfiles : Ensure dotfiles standard symlinks are set] **************************************************************************************************************
skipping: [aki.local] => (item={u'dest': u'.gitconfig', u'src': u'gitconfig'})
skipping: [aki.local] => (item={u'dest': u'.zshrc', u'src': u'zshrc'})",3
,2062,142,,,,,"`postgres_user` module doesn't work with AWS RDS databases

Issue Type:
Bug report
Ansible Version:
ansible 1.7.0
Environment:
OSX Mavericks 10.9.4
Summary:
When running against an AWS RDS Postgresql instance, the postgres_user module can't set up new users.
Steps To Reproduce:
Run this task:
- name: Ensure user has access to the database
  postgresql_user: login_host={{ db_host }}
                   port={{ db_port }}
                   login_user={{ db_admin_user }}
                   login_password={{ db_admin_password }}
                   db={{ db_name }}
                   name={{ db_user }}
                   password={{ db_password }}
                   priv=ALL
                   state=present 

Expected Results:
ok: [...] => {""changed"": true, ""db"": ""database""}

Actual Results:
failed: [...] => {""failed"": true, ""parsed"": false}
invalid output was: SUDO-SUCCESS-qwpjepgvenunnlewzwjddpnrbfjyptxo
Traceback (most recent call last):
  File ""/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user"", line 1869, in <module>
    main()
  File ""/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user"", line 497, in main
    changed = user_alter(cursor, module, user, password, role_attr_flags, encrypted, expires)
  File ""/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user"", line 201, in user_alter
    cursor.execute(select, {""user"": user})
psycopg2.ProgrammingError: permission denied for relation pg_authid

Apparently the pg_authid relation is not available in RDS.
Possible workaround: if access denied for pg_authid, then always set the password.",3
,2063,140,,,,,"letsencrypt module does not create any cert when using DNS-based validation

ISSUE TYPE
Bug Report
COMPONENT NAME

letsencrypt
ANSIBLE VERSION
Ansible version is 2.2.1.0
SUMMARY
It looks like the Let's Encrypt Ansible module doesn't create any cert when running with the DNS-based validation.
STEPS TO REPRODUCE
# vars
---
aws:
   access_key: ""some_access_key""
   secret_key: ""some_secret_key""
letsencrypt_account_email: ""some_email@example.com""
letsencrypt_key_filename: ""some_filename""
letsencrypt_dir: ""some/path""
letsencrypt_domains:
   - ""foo""
   - ""bar""

# tasks
---
- name: Install Route53 dependencies
  pip:
    name: boto

- name: Create folder
  file:
    path: ""{{ letsencrypt_dir }}""
    owner: root
    group: root
    mode: 0755
    state: directory

- name: Create key
  shell: 'ssh-keygen -t rsa -b 2048 -C ""{{ letsencrypt_account_email }}"" -f ~/.ssh/{{ letsencrypt_key_filename }} -q -N """"'
  args:
    creates: ""~/.ssh/{{ letsencrypt_key_filename }}""

- name: Create domain key
  shell: 'ssh-keygen -t rsa -b 2048 -C ""{{ letsencrypt_account_email }}"" -f ~/.ssh/id_rsa.{{ item }} -q -N """"'
  args:
    creates: '~/.ssh/id_rsa.{{ item }}'
  with_items: ""{{ letsencrypt_domains }}""

- name: Create CSR (Certificate Signing Request)
  shell: 'openssl req -new -nodes -key ~/.ssh/{{ letsencrypt_key_filename }} -out {{ letsencrypt_dir }}/{{ item }}.csr -subj ""/CN={{ item }}""'
  args:
    creates: '{{ letsencrypt_dir }}/{{ item }}.csr'
  with_items: ""{{ letsencrypt_domains }}""

- name: Create challenge
  letsencrypt:
    account_key: '~/.ssh/id_rsa.{{ item }}'
    challenge: dns-01
    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'
    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'
    remaining_days: 20
  register: letsencrypt_challenge
  with_items: ""{{ letsencrypt_domains }}""

- name: Create Route53 TXT record for DNS-based certificate validation
  route53:
    command: create
    aws_access_key: ""{{ aws.access_key_id }}""
    aws_secret_key: ""{{ aws.secret_access_key }}""
    zone: 'some_hosted_zone.com'
    record: ""{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com""
    retry_interval: 300
    type: TXT
    ttl: 7200
    value: '""{{ item.1.challenge_data[item.0][""dns-01""][""resource_value""] }}""'
    wait: yes
  when: ""letsencrypt_challenge|changed""
  with_together:
    - ""{{ letsencrypt_domains }}""
    - ""{{ letsencrypt_challenge['results'] }}""
  ignore_errors: yes

- name: Validate challenge
  letsencrypt:
    account_key: '~/.ssh/id_rsa.{{ item }}'
    challenge: dns-01
    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'
    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'
    data: '{{ letsencrypt_challenge }}'
    remaining_days: 20
  when: '{{ letsencrypt_challenge|changed }}'
  with_items: ""{{ letsencrypt_domains }}""

- name: Delete Route53 TXT record for DNS-based certificate validation
  route53:
    command: delete
    aws_access_key: ""{{ aws.access_key_id }}""
    aws_secret_key: ""{{ aws.secret_access_key }}""
    zone: 'some_hosted_zone.com'
    record: ""{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com""
    retry_interval: 300
    type: TXT
    ttl: 7200
    value: '""{{ item.1.challenge_data[item.0][""dns-01""][""resource_value""] }}""'
    wait: yes
  when: ""letsencrypt_challenge|changed""
  with_together:
    - ""{{ letsencrypt_domains }}""
    - ""{{ letsencrypt_challenge['results'] }}""
  ignore_errors: yes


ACTUAL RESULTS
No errors are being thrown or anything, it provisions just fine
EXPECTED RESULTS
There should be a cert generated",3
,2064,143,,,,,"Memory load increased in 2.3.0 compared to 2.2.0.1 (high memory use, high ram use)

ISSUE TYPE

Bug Report

COMPONENT NAME
Ansible
ANSIBLE VERSION
2.3.0

CONFIGURATION
[defaults]
hostfile       = inventory/
library        = /usr/share/ansible
remote_tmp     = $HOME/.ansible/tmp
pattern        = *
forks          = 30
poll_interval  = 15
sudo_user      = root
ask_sudo_pass  = True
transport      = ssh
remote_port    = 22
module_lang    = C
allow_world_readable_tmpfiles = True
gathering = implicit
host_key_checking = False
stdout_callback = actionable_debug
sudo_exe = sudo
sudo_flags = -i
timeout = 15
ansible_managed = This file is managed by Ansible.
action_plugins     = /usr/share/ansible_plugins/action_plugins
callback_plugins   = /usr/share/ansible_plugins/callback_plugins:./callback_plugins
connection_plugins = /usr/share/ansible_plugins/connection_plugins
lookup_plugins     = /usr/share/ansible_plugins/lookup_plugins
vars_plugins       = /usr/share/ansible_plugins/vars_plugins
filter_plugins     = /usr/share/ansible_plugins/filter_plugins
fact_caching = jsonfile
fact_caching_connection = /tmp/$USER
retry_files_enabled = False

[privilege_escalation]

[paramiko_connection]

[ssh_connection]
retries=10
pipelining = False

[accelerate]

[selinux]

[colors]

OS / ENVIRONMENT
Ubuntu 14.04.5 (exclusively) hosts & clients
SUMMARY
Increased memory usage when 2.3.0 is rolled out: http://imgur.com/a/ZgUCe
Recovery takes place when 2.2.0.1 is reverted.
STEPS TO REPRODUCE

Install Ansible 2.3.0
Run regular job schedule (6x a day a full run of ~100 roles, several irregular jobs like backups), started from the Rundeck job scheduler over regular SSH.

EXPECTED RESULTS
Similar memory usage.
ACTUAL RESULTS
http://imgur.com/a/ZgUCe",3
,2065,142,,,,,"Homebrew module fails silently after fresh Homebrew installs

Reproduce (done on a fresh OSX Mavericks install + Command line tools)

Run the homebrew installer script.
ruby -e ""$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)""
Completes. Run brew doctor. Everything looks good.
Note that the installer does not (and should not) create /usr/local/Cellar, this is created when the first brew package is installed. So right now no Cellar exists.

Try running a simple playbook with localhost inventory:

---
- name: My playbook
  user: myusername
  hosts: all
  tasks:
    - name: Install brew packages
      homebrew: name=ack state=present

ansible-playbook myplaybook -i localhost_inventory
PLAY [My playbook] ******************************************************
GATHERING FACTS ***************************************************************
ok: [localhost]
TASK: [Install brew packages] *********************************************
ok: [localhost]
PLAY RECAP ********************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0
You'll get something that completes instantly, seems to indicate ack was installed(ok), but it wasn't. No Cellar exists. ack is seriously not installed.
Now go ahead and make the /usr/local/Cellar directory. Run the config again. Boom, suddenly it actually works. There is a noticeable delay as ack is installed and ack immediately works after the playbook completes.
Try deleting the Cellar completely and you're back to the original broken behavior.
A clean install seems like a pretty simple edge case that should be supported. What's up with this? Lemme know if I'm going crazy.",3
,2066,148,,,,,"ansible 1.5 breaks gather_facts+accelerate

Issue Type:
Bug report
Ansible Version:
ansible 1.5
Environment:
Both RHEL6 and Fedora rawhide.
Summary:
Any plays with both:
gather_facts: true
accelerate: true
Fail with:
PLAY [foo] ********************************************************************
GATHERING FACTS ***************************************************************
fatal: [td] => Incorrect permissions on ACCELERATE_KEYS_FILE (/home/kevin/.fireball.keys/td)
ansible never actually connects to the host that I can tell,
perhaps it's thinking accelerate is already started when it's not?
The directory it refers to doesn't exist (since it never connected to the host)
Steps To Reproduce:
Create a playbook with:
gather_facts: true
accelerate: true
and at least one host. Run it.
Expected Results:
Facts are gathered and rest of playbook runs.
Actual Results:
Ansible-playbook fails at gathering facts and errors out.",3
,2067,148,,,,,"Clearing facts (to refresh them) does not work

ISSUE TYPE


Bug Report

COMPONENT NAME


meta
setup

ANSIBLE VERSION

ansible 2.3.0.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Jan 12 2017, 17:59:37) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]

CONFIGURATION

None.
OS / ENVIRONMENT

 lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	Fedora
Description:	Fedora release 25 (Twenty Five)
Release:	25
Codename:	TwentyFive

SUMMARY

If you need to reload some facts (in my case I need the VPN IP after I configured it), it does not work
STEPS TO REPRODUCE

Option A:

- name: vpn client must be running
  service:
    name: openvpn@example
    enabled: true
    state: started

- name: refresh facts
  setup:
Option B:
- name: vpn client must be running
  service:
    name: openvpn@example
    enabled: true
    state: started

- name: refresh facts
  meta: clear_facts

EXPECTED RESULTS

Facts reloaded, VPN ip data available.
ACTUAL RESULTS


Option A:
TASK [vpn-client : refresh facts] ********************************************************************************************************************
ok: [server.example.com]

... but later another task fails with:
fatal: [server.example.com]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'ansible_tun0' is undefined""}

Option B:
TASK [vpn-client : refresh facts] *********************************************************************************************************
fatal: [server.example.com]: FAILED! => {""changed"": false, ""failed"": true, ""module_stderr"": ""Shared connection to server.example.com closed.\r\n"", ""module_stdout"": """", ""msg"": ""MODULE FAILURE"", ""rc"": 0}

WORKAROUND
Run the playbook again ",3
,2068,142,,,,,"jenkins_plugin - incorrect ""changed"" and silent install failures

ISSUE TYPE

Bug Report

COMPONENT NAME
jenkins_plugin
ANSIBLE VERSION
ansible 2.3.1.0
  config file = /home/ghelling/.ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, May 10 2017, 20:04:28) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]

CONFIGURATION
default configuration
OS / ENVIRONMENT
This seems independent of versions, but I see it when running from Fedora while controlling Fedora, CentOS 7,  and RHEL 7 machines with Jenkins 1.651.3 running on them.
SUMMARY
Some plugins, after being installed, are either not installed to the latest version (despite no value being specified for the version) or are not installed at all, despite the module reporting success. On subsequent runs, these same plugins continue to report a changed/updated edition despite there being no change in the version being installed.
An example of some plugins where this behavior has been noticed:

antisamy-markup-formatter (version 1.1 installed, despite 1.5 being available)
scriptler (reports installed, but the plugin fails to be installed)
dynamic-parameter (same as scriptler)

And many others. However, the behavior does not affect all plugins.
STEPS TO REPRODUCE
On clean CentOS system, run the following playbook: https://gist.github.com/greg-hellings/14f58eb19a4992b910f27e53f63573b4
EXPECTED RESULTS
Plugins are installed to the latest version if version is specified as latest.
Spurious ""changed"" values are not reported from the module when nothing gets updated.
The module errors when a plugin install error occurs.
ACTUAL RESULTS
No version specified results in both plugins reporting back ""changed"" when neither the version updates (antisamy-markup-formatter) or the plugin is not installed at all (scriptler).
With the line version: latest added to that final task in the sample file, the scriptler install fails while the antisamy-markup-formatter still reports ""changed"" without actually updating anything.",3
,2069,140,,,,,"ec2_group: add tags

From @jbrockett on December 4, 2014 15:15
Issue Type:
Feature Idea
Component name:

ec2_group
Ansible Version:
ansible 2.3
Environment:
N/A
Summary:
Please add the ability to create and modify tags associated with the security group.  At least being able to set the Name tag would be helpful.",3
,2070,148,,,,,"service: nginx enabled=yes not enabling service ansible 1.7.2

Issue Type:
Bug Report
Ansible Version:
1.7.2
1.6.6
Environment:
Destination: Ubuntu 12.04.4 LTS
Control hosts: Mac OSX and Ubuntu 12.04.4 LTS
Summary:
When trying to enable nginx, which is a typical init.d startup script. It appears to not have any affect.
Have tried with both ansible versions and with Mac OSX and Ubuntu 12.04 control hosts. Destination host is a Ubuntu 12.04
Steps To Reproduce:
Install nginx from deb and it will create a /etc/init.d/nginx script
In a task write:
  - name: start nginx on boot
    service: name=nginx state=started enabled=yes
    tags: nginx_boot

Run the playbook.
ansible-playbook site.yaml -i inventory/prod -l host02.example.com -t nginx_boot

Expected Results:
Service enabled
Actual Results:
Reports changed every time:
TASK: [nginx | start nginx on boot] *******************************************
changed: [host02.example.com]

chkconfig and rc.d directories unchanged:
root@host02:/etc# chkconfig nginx
nginx  off

root@host02:/etc# ls rc*/*nginx*
rc0.d/K00nginx  rc1.d/K00nginx  rc2.d/K00nginx  rc3.d/K00nginx  rc4.d/K00nginx  rc5.d/K00nginx  rc6.d/K00nginx",3
,2071,148,,,,,"Include all dependency roles in include search path for role

ISSUE TYPE

Feature Idea
 - Bug Report

COMPONENT NAME
include
ANSIBLE VERSION
ansible 2.2.0.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides

CONFIGURATION
roles_path set to both my local directory and left as default.
OS / ENVIRONMENT
Fedora 25
SUMMARY
Files present in a parent role's tasks directory are not found by child tasks' include statements.
STEPS TO REPRODUCE
Unzip ansible-test.zip and execute ansible-playbook test.yml. Note that
- include: roles/parent/tasks/parent-task.yml
works, but
- include: parent-task.yml
fails.
EXPECTED RESULTS
parent-task.yml should be found as if it were part of the child task that inherits from it. This would follow the standard (single) role behaviour where ""Any copy, script, template or include tasks (in the role) can reference files in roles/x/{files,templates,tasks}/ (dir depends on task) without having to path them relatively or absolutely"".
ACTUAL RESULTS
$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv
Using /etc/ansible/ansible.cfg as config file
 [WARNING]: provided hosts list is empty, only localhost is available

statically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml
[DEPRECATION WARNING]: Included file '/home/pjanes/ansible-test/parent-task.yml'
 not found, however since this include is not explicitly marked as 'static: 
yes', we will try and include it dynamically later. In the future, this will be 
an error unless 'static: no' is used on the include task. If you do not want 
missing includes to be considered dynamic, use 'static: yes' on the include or 
set the global ansible.cfg options to make all inclues static for tasks and/or 
handlers.
This feature will be removed in a future release. Deprecation warnings
 can be disabled by setting deprecation_warnings=False in ansible.cfg.
Loading callback plugin default of type stdout, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc
Loading callback plugin timestamp of type ek3, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc

PLAYBOOK: test.yml *************************************************************
1 plays in test.yml

PLAY [localhost] ***************************************************************
Friday 13 January 2017  10:38:41 -0500 (0:00:00.001)       0:00:00.001 ******** 
=============================================================================== 

TASK [child : debug] ***********************************************************
task path: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml:2
Friday 13 January 2017  10:38:41 -0500 (0:00:00.030)       0:00:00.032 ******** 
ok: [localhost] => {
    ""msg"": ""task from parent""
}

TASK [child : include] *********************************************************
task path: /home/pjanes/ansible-test/roles/child/tasks/main.yml:3
Friday 13 January 2017  10:38:41 -0500 (0:00:00.010)       0:00:00.042 ******** 
fatal: [localhost]: FAILED! => {
    ""failed"": true, 
    ""reason"": ""the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable""
}
	to retry, use: --limit @/home/pjanes/.ansible-retry/test.retry

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=1   

Friday 13 January 2017  10:38:41 -0500 (0:00:00.009)       0:00:00.051 ******** 
===============================================================================

Adding static: yes to avoid the warning:
$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv
Using /etc/ansible/ansible.cfg as config file
 [WARNING]: provided hosts list is empty, only localhost is available

statically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml
ERROR! the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable",3
,2072,144,,,,,"Vault bytes <=> text string API

ISSUE TYPE

Bug Report

COMPONENT NAME
lib/ansible/parsing/vault/init.py
unittests for vault
ANSIBLE VERSION

devel

SUMMARY
The new vault API is mixing bytes and text in inappropriate ways.
In working on enabling unittests for Python3, I came across the fact that vault unittests were skipping a lot of tests depending on whether they were running on python3 or python2.  Closer examination revealed that the tests were either giving different values to the API or receiving different values back depending on the version of Python that we are running on.  This is problematic API design that we need to change before release.
Good API should follow one of these rules for input and output:

Take bytes and return bytes

The old vault API attempted to use this strategy for ""internal"" functions.  Many of the functions inside of vault were not called from outside of parsing/vault/init.py.  Since they also dealt with operations involving byte strings (encoding, decoding, hexlifying, writing to disk, etc) it made sense that they dealt solely in bytes.


Take text and return text

It is relatively easy to create this sort of API in python3 as combining text and bytes leads to immediate tracebacks.  In python2, it is easy to mix this up with one of the other strategies below as ascii-only bytes and text strings will combine.  Most of ansible's current API does not follow this strategy because we don't trust that the data coming in is going to be the string type we expect.  As we secure our borders (and with python3 tests throwing errors when these are combined inappropriately) we should be able to move more API to this model.


Take either bytes or text, normalize internally, and return text

This is the strategy that a lot of the old external Vault API was taking.  We didn't trust that the rest of Ansible was properly sending us text strings so as the first step we used to_unicode and to_bytes to make sure that the string we were dealing with was the correct type.


Take either bytes or text, normalize internally, and return bytes

Old internal vault API may have taken this strategy.  For internal API we should know that our inputs are only text or only bytes but we may have been paranoid.  Since the data we passed around was often pure-ascii, this couldn't cause tracebacks.


Take either bytes or text.  If bytes were input then output bytes.  If text was input then output text

I would not recommend this for any of our Vault API.  You'll see this in some Python stdlib API like os.path.abspath() (a good usage of the strategy) or os.listdir() (a bad usage of the strategy).  It is appropriate when the purpose of the function is a straightforward text transformation and there is a need to give callers a version that works with text and a version that works with bytes.  This is not the case for Vault's internal API (where we can adapt the callers to use just one API) and it is not needed for Vault's external API (where we should probably always be returning text).  This strategy can also be used for functions which are operating inside of a larger ""native string"" data model as the code run on python2 should be taking in bytes and outputting bytes while the code on python3 is taking in text and outputting text.  Note, though, that this strategy does not validate the input or output while technically the native string model could validate that only bytes was accepted on python2 and only text was accepted on python3.



Whichever strategy is followed, be sure that the same strategy is being applied for both Python2 and Python3.  Most projects (including Ansible) are working towards a single code base that runs on both python2 and python3.  Writing API that expects different types and returns different types on one or the other hampers this overall design.
Also note, it is tempting to do validation of the strings you receive.  I'd hesitate to do that because all checks have a cost.  Even asserts which are really meant for this purpose invokes the cost because no one really runs ansible in python optimized mode where asserts are stripped out.  My general stance has been if you trust the data coming in, there's no need to validate it.  If you don't trust the data, you should use to_bytes() or to_unicode() to accept either bytes or text and normalize to the type you want to operate on.",3
,2073,148,,,,,"hacking/module_formatter build error in Debian [0.9]

I tried to build ansible 0.9 on Debian Squeeze, but the make script fails.
$ sudo aptitude install python-yaml2 python-jinja2 python-paramiko
$ wget https://github.com/downloads/ansible/ansible/ansible-0.9.tar.gz
$ tar -xvzf ansible-0.9.tar.gz
$ cd ansible-0.9

$ make debian
cat: VERSION: No such file or directory
fatal: Not a git repository (or any of the parent directories): .git
Cleaning up distutils stuff
rm -rf build
rm -rf dist
Cleaning up byte compiled python stuff
find . -type f -regex "".*\.py[co]$"" -delete
Cleaning up editor backup files
find . -type f \( -name ""*~"" -or -name ""#*"" \) -delete
find . -type f \( -name ""*.swp"" \) -delete
Cleaning up manpage stuff
find ./docs/man -type f -name ""*.xml"" -delete
find ./docs/man -type f -name ""*.asciidoc"" -delete
find ./docs/man/man3 -type f -name ""*.3"" -delete
Cleaning up output from test runs
rm -rf test/test_data
Cleaning up RPM building stuff
rm -rf MANIFEST rpm-build
Cleaning up Debian building stuff
rm -rf debian
rm -rf deb-build
rm -rf docs/json
rm -rf docs/js
PYTHONPATH=./lib hacking/module_formatter.py -A  -t man -o docs/man/man3/ --module-dir=library --template-dir=hacking/templates
/bin/sh: hacking/module_formatter.py: not found
make: *** [modulepages] Error 127",3
,2074,148,,,,,"Add support for ""build-dep"" to apt module

As a system administrator I want to install build dependency packages for developer and buildhost machines using the same syntax I install normal packages with.
It would be useful if one could install build-deps via the apt module instead of mixing and matching apt module and commands for ""apt-get build-dep """,3
,2075,144,,,,,"sequence lookup shortcut syntax doesn't work and wrong docs

ISSUE TYPE

Bug Report
Documentation Report

COMPONENT NAME
sequence lookup plugin
ANSIBLE VERSION
ansible 2.2.1.0

2.1, 2.3 and 2.4 have the same bug.
CONFIGURATION
Standard config
OS / ENVIRONMENT
N/A
SUMMARY
with_sequence shortcut syntax [start-]end[/stride][:format] is not honoured.
STEPS TO REPRODUCE
---
- hosts: localhost
  connection: local
  gather_facts: no
  tasks:
    - debug:
        msg: ""{{ item }}""
      with_sequence: '5-6'

EXPECTED RESULTS
Loop over [5,6]:
TASK [debug] *******************************************************************
ok: [localhost] => (item=5) => {
    ""item"": ""5"",
    ""msg"": ""5""
}
ok: [localhost] => (item=6) => {
    ""item"": ""6"",
    ""msg"": ""6""
}

ACTUAL RESULTS
Parameter error:
TASK [debug] *******************************************************************
fatal: [localhost]: FAILED! => {""failed"": true, ""msg"": ""unknown error parsing with_sequence arguments: u'5-6'. Error was: unrecognized arguments to with_sequence: [u'_raw_params']""}

Also there is wrong parameters passing in the docs:
    - user:
        name: ""{{ item }}""
        state: present
        groups: ""evens""
      with_sequence:
        - start: 0
        - end: 32
        - format: testuser%02x

Parameters can be passed to with_sequence only as string, not as list or dict.",3
,2480,145,0,15383,,Feature request: Use placeholders to specify the inputs of TFGAN model.,Feature request: Use placeholders to specify the inputs of TFGAN model.,3
,2481,148,1,22826,"I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :
MFCTestTF1.9.obj : error LNK2001:  ""char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)"" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)
1>D:\ProgramData\VS2017 Project\MFCTestTF1.9\Release\MFCTestTF1.9.exe : fatal error LNK1120: 1 
1>MFCTestTF1.9.vcxproj - 
And I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.","Win10 C++ TF1.9, error LNK2001, build by bazel  !","Win10 C++ TF1.9, error LNK2001, build by bazel  !I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :
MFCTestTF1.9.obj : error LNK2001:  ""char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)"" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)
1>D:\ProgramData\VS2017 Project\MFCTestTF1.9\Release\MFCTestTF1.9.exe : fatal error LNK1120: 1 
1>MFCTestTF1.9.vcxproj - 
And I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.",3
,2482,148,2,21037,"can anyone help me??
Traceback (most recent call last):
File ""/Users/meow/generate_tfrecord.py"", line 99, in 
tf.app.run()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""/Users/meow/generate_tfrecord.py"", line 85, in main
writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py"", line 112, in init
compat.as_bytes(path), compat.as_bytes(compression_type), status)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in exit
c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory,"tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directorycan anyone help me??
Traceback (most recent call last):
File ""/Users/meow/generate_tfrecord.py"", line 99, in 
tf.app.run()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""/Users/meow/generate_tfrecord.py"", line 85, in main
writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py"", line 112, in init
compat.as_bytes(path), compat.as_bytes(compression_type), status)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in exit
c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",3
,2483,148,3,452,"I am trying to build tensorflow from source, and bazel is giving some unrelated error
eddie7@albus:~/lab/tensorflow$ git pull
Already up-to-date.
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
.......
ERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: Loading failed; build aborted.
INFO: Elapsed time: 129.792s


FYI: I have also build bazel from source",bazel build error for PolymerElements,"bazel build error for PolymerElementsI am trying to build tensorflow from source, and bazel is giving some unrelated error
eddie7@albus:~/lab/tensorflow$ git pull
Already up-to-date.
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
.......
ERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.
ERROR: Loading failed; build aborted.
INFO: Elapsed time: 129.792s


FYI: I have also build bazel from source",3
,2484,143,4,19640,"I'm using an AWS g3.8xlarge instance which has 2 GPUs.
TF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.
We are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU

06_09_21",Tensorflow Serving not using multi GPU/CUDA cores ,"Tensorflow Serving not using multi GPU/CUDA cores I'm using an AWS g3.8xlarge instance which has 2 GPUs.
TF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.
We are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU

06_09_21",3
,2485,143,5,14761,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 14.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.3.0
Python version:
2.7
Bazel version (if compiling from source):
0.7.0
GCC/Compiler version (if compiling from source):
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
CUDA/cuDNN version:
cuda8.0/cudnn6.0

I tried to convert squeezenet frozen model to lite format with the following command:
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3""
the output is shown below:
2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)
2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.
2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Then I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3""
output:
2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)
2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Although I installed tensorflow with ""pip install tensorflow-gpu"", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",tensorflow lite: error when convert frozen model to lite format,"tensorflow lite: error when convert frozen model to lite formatSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 14.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.3.0
Python version:
2.7
Bazel version (if compiling from source):
0.7.0
GCC/Compiler version (if compiling from source):
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
CUDA/cuDNN version:
cuda8.0/cudnn6.0

I tried to convert squeezenet frozen model to lite format with the following command:
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3""
the output is shown below:
2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)
2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.
2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Then I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3""
output:
2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)
2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Although I installed tensorflow with ""pip install tensorflow-gpu"", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",3
,2486,148,6,111,"Cuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).
  if [ -e ""$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0"" ]; then
    break
  fi
  echo ""Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found""",configure script hardcodes location of cuda that makes it fail on OSX,"configure script hardcodes location of cuda that makes it fail on OSXCuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).
  if [ -e ""$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0"" ]; then
    break
  fi
  echo ""Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found""",3
,2487,144,7,8199,"What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None although a search for distorted image tensorboard doesn't help much...
Environment info
Operating System: 16.04 LTS
Firefox: 51.0.1 (64-bit)
TF: 1.0 (installed via pip)
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$> sudo ls -l /usr/local/cudnn/*
/usr/local/cudnn/include:
total 100
-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h

/usr/local/cudnn/lib64:
total 150908
lrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4
lrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7
lrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10

If installed from binary pip package, provide:

A link to the pip package you installed:
Standard TF pip url.
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

$> python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0

Steps to reproduce (Firefox only)

On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. 
Click on the expand icon 
Enable log scale of y-axis 
Disable log scale of y-axis (note the bug happens regardless of whether you do this) 
Click on expand icon to shrink the graph.

The graph is now overflowing: 
What other attempted solutions have you tried?
Tried to reproduce in Chromium 55.0.2883.87 but unable to.",Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis ,"Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None although a search for distorted image tensorboard doesn't help much...
Environment info
Operating System: 16.04 LTS
Firefox: 51.0.1 (64-bit)
TF: 1.0 (installed via pip)
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$> sudo ls -l /usr/local/cudnn/*
/usr/local/cudnn/include:
total 100
-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h

/usr/local/cudnn/lib64:
total 150908
lrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4
lrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7
lrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10

If installed from binary pip package, provide:

A link to the pip package you installed:
Standard TF pip url.
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

$> python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0

Steps to reproduce (Firefox only)

On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. 
Click on the expand icon 
Enable log scale of y-axis 
Disable log scale of y-axis (note the bug happens regardless of whether you do this) 
Click on expand icon to shrink the graph.

The graph is now overflowing: 
What other attempted solutions have you tried?
Tried to reproduce in Chromium 55.0.2883.87 but unable to.",3
,2488,148,8,15645,"I am trying to use tensrflow-lite in Android. When I add
compile 'org.tensorflow:tensorflow-lite:0.1.1'
I get:
Error:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.
> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class

I am using multidex and AGP 2.3.3.
When I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",Tensorflow lite 0.1.1 causing Build to fail,"Tensorflow lite 0.1.1 causing Build to failI am trying to use tensrflow-lite in Android. When I add
compile 'org.tensorflow:tensorflow-lite:0.1.1'
I get:
Error:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.
> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class

I am using multidex and AGP 2.3.3.
When I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",3
,2489,148,9,21851,"Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",How to install tensorflow in python3.7?,"How to install tensorflow in python3.7?Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",3
,2490,148,10,10074,"I coded a simple feedforward neural network and it works very well.
I tried to save the computation time, and created:
self.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')
in the fnn class.
and i tried to print the total training time per some training epoch.
I made it to grow with time:
# Check & Print training time
till_now = time.time() - start_time
self.total_time += till_now
print_time(self.total_time.eval())
and the result look something like this :
Epoch :   0 | Evaluation :  115 | Learning Rate : 0.50
Training Loss :         0.040919
Validation Loss :      0.0741969
Validation Accuracy :      97.77%
Total time cost : 0.38 seconds
Epoch :   1 | Evaluation :  116 | Learning Rate : 0.50
Training Loss :        0.0417941
Validation Loss :       0.073841
Validation Accuracy :      97.73%
Total time cost : 0.71 seconds
Epoch :   2 | Evaluation :  117 | Learning Rate : 0.50
Training Loss :        0.0334573
Validation Loss :      0.0745566
Validation Accuracy :      97.75%
Total time cost : 1.01 seconds
However, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.
I also checked tf.global_variables() include self.total_time.
What is wrong?",fatal problem with saving variables,"fatal problem with saving variablesI coded a simple feedforward neural network and it works very well.
I tried to save the computation time, and created:
self.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')
in the fnn class.
and i tried to print the total training time per some training epoch.
I made it to grow with time:
# Check & Print training time
till_now = time.time() - start_time
self.total_time += till_now
print_time(self.total_time.eval())
and the result look something like this :
Epoch :   0 | Evaluation :  115 | Learning Rate : 0.50
Training Loss :         0.040919
Validation Loss :      0.0741969
Validation Accuracy :      97.77%
Total time cost : 0.38 seconds
Epoch :   1 | Evaluation :  116 | Learning Rate : 0.50
Training Loss :        0.0417941
Validation Loss :       0.073841
Validation Accuracy :      97.73%
Total time cost : 0.71 seconds
Epoch :   2 | Evaluation :  117 | Learning Rate : 0.50
Training Loss :        0.0334573
Validation Loss :      0.0745566
Validation Accuracy :      97.75%
Total time cost : 1.01 seconds
However, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.
I also checked tf.global_variables() include self.total_time.
What is wrong?",3
,2491,143,11,4431,"Say I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.
This is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?","Forward mode ad, directional derivatives","Forward mode ad, directional derivativesSay I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.
This is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?",3
,2492,143,12,11937,"I wanna add support for my Tensor Processing Unit chip in TensorFlow.
My TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04
I also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.
I'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU
Anyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated
Thank you",TPU support,"TPU supportI wanna add support for my Tensor Processing Unit chip in TensorFlow.
My TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04
I also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.
I'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU
Anyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated
Thank you",3
,2493,144,13,18108,"Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04.9
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.5.1 after successful building, but i want 1.7
Python version: 3.6
Bazel version (if compiling from source): build label 0.11.1
GCC/Compiler version (if compiling from source):5.4.0 when i type gcc --version, 7.2.0 shown in python terminal
CUDA/cuDNN version:cuda 9, cudnn 7
GPU model and memory: gtx1080 ti 11 gb
Exact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
I've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",building from source with branch r1.7 gives tf1.5.1 after building wheel,"building from source with branch r1.7 gives tf1.5.1 after building wheelPlease go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04.9
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.5.1 after successful building, but i want 1.7
Python version: 3.6
Bazel version (if compiling from source): build label 0.11.1
GCC/Compiler version (if compiling from source):5.4.0 when i type gcc --version, 7.2.0 shown in python terminal
CUDA/cuDNN version:cuda 9, cudnn 7
GPU model and memory: gtx1080 ti 11 gb
Exact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
I've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",3
,2494,144,14,19463,"OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
-TensorFlow installed from (source):
tensorflow v1.6.0-0-gd2e24b6039 1.6.0:
Python 3.5:
Reproduce
git clone tensorflow
python3 tensorflow/examples/speech_commands/train.py
python3 tensorflow/examples/speech_commands/freeze.py 
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 
--output_file=/tmp/my_frozen_graph.pb
Error appears:
/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
Converted 6 variables to const ops.
Traceback (most recent call last):
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 180, in 
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 124, in main
os.path.dirname(FLAGS.output_file),
File ""/usr/lib/python3.5/posixpath.py"", line 148, in dirname
i = p.rfind(sep) + 1
AttributeError: 'NoneType' object has no attribute 'rfind'
Thanks",AttributeError: 'NoneType' object has no attribute 'rfind',"AttributeError: 'NoneType' object has no attribute 'rfind'OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
-TensorFlow installed from (source):
tensorflow v1.6.0-0-gd2e24b6039 1.6.0:
Python 3.5:
Reproduce
git clone tensorflow
python3 tensorflow/examples/speech_commands/train.py
python3 tensorflow/examples/speech_commands/freeze.py 
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 
--output_file=/tmp/my_frozen_graph.pb
Error appears:
/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
Converted 6 variables to const ops.
Traceback (most recent call last):
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 180, in 
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 124, in main
os.path.dirname(FLAGS.output_file),
File ""/usr/lib/python3.5/posixpath.py"", line 148, in dirname
i = p.rfind(sep) + 1
AttributeError: 'NoneType' object has no attribute 'rfind'
Thanks",3
,2495,148,15,459,"When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):

It would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",Single scalar summary point not visible in the plot,"Single scalar summary point not visible in the plotWhen only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):

It would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",3
,2496,148,16,10171,"As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.
The tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",tf.contrib.rnn.decoder does not require explicitly build encoder?,"tf.contrib.rnn.decoder does not require explicitly build encoder?As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.
The tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",3
,2497,148,17,3864,"On running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1
Detailed script and output at the bottom.
Problem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.
Environment info
Operating System: ubuntu 16.04
uname -a
Linux  4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
ls -l $CUDA_HOME/lib64/libcud*
-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a
Environment variables
echo $LD_LIBRARY_PATH
/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
echo $CUDA_HOME
/usr/local/cuda
echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/
Tensorflow version
Compiled from source, r0.10, built into pip package and installed this pip wheel
Configured with cuDNN path and version set to system default

The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
see /// OUTPUT /// at the end

If installed from source, provide

The commit hash (git rev-parse HEAD)
n.a.
version r0.10
The output of bazel version
bazel version
Build label: 0.3.1-2016-08-15 (@936c2c2)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Aug 14 23:07:32 2016 (1471216052)
Build timestamp: 1471216052
Build timestamp as int: 1471216052

Steps to reproduce: run this script
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
load data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
reshape to be [samples][channels][width][height]
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')
normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255
one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
define a simple CNN model
def baseline_model():
##### create model
model = Sequential()
model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
##### Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
return model
build the model
model = baseline_model()
Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)
Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print(""CNN Error: %.2f%%"" % (100-scores[1]*100))
///////////// OUTPUT /////////////////
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.91GiB
Free memory: 148.69MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
E tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted (core dumped)","Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORY","Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORYOn running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1
Detailed script and output at the bottom.
Problem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.
Environment info
Operating System: ubuntu 16.04
uname -a
Linux  4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
ls -l $CUDA_HOME/lib64/libcud*
-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a
Environment variables
echo $LD_LIBRARY_PATH
/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
echo $CUDA_HOME
/usr/local/cuda
echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/
Tensorflow version
Compiled from source, r0.10, built into pip package and installed this pip wheel
Configured with cuDNN path and version set to system default

The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
see /// OUTPUT /// at the end

If installed from source, provide

The commit hash (git rev-parse HEAD)
n.a.
version r0.10
The output of bazel version
bazel version
Build label: 0.3.1-2016-08-15 (@936c2c2)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Aug 14 23:07:32 2016 (1471216052)
Build timestamp: 1471216052
Build timestamp as int: 1471216052

Steps to reproduce: run this script
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
load data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
reshape to be [samples][channels][width][height]
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')
normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255
one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
define a simple CNN model
def baseline_model():
##### create model
model = Sequential()
model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
##### Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
return model
build the model
model = baseline_model()
Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)
Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print(""CNN Error: %.2f%%"" % (100-scores[1]*100))
///////////// OUTPUT /////////////////
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.91GiB
Free memory: 148.69MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
E tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted (core dumped)",3
,2498,148,18,18763,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
Bazel version:
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Tried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu
TensorFlow installed from (source or binary):
Installed utilizing pip
TensorFlow version (use command below):
1.7
Python version:
3.6
Exact command to reproduce:

import tensorflow as tf  
import tensorflow.contrib.eager as tfe  

tfe.enable_eager_execution()

class CustomLayer(tf.keras.Model):
    def __init__(self):
        super(CustomLayer, self).__init__()
        print(""blah"")

class CustomNetwork(tf.keras.Model):
    def __init__(self):
        super(CustomNetwork, self).__init__()
        self.custom_layers = CustomLayer()

    def forward(self, x, y=None):
        x = self.custom_layers(x)

CustomNetwork().forward(tf.convert_to_tensor([1]))

Describe the problem
Trying to utilize multiple classes fails in tensorflow eager mode utilizing ""tf.keras.Model"". If I change ""tf.keras.Model"" to ""tfe.Network"" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:
Source code / logs
blah
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-9afa9b91ddef> in <module>()
----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))

<ipython-input-11-484119102aec> in forward(self, x, y)
      5 
      6     def forward(self, x, y=None):
----> 7         x = self.custom_layers(x)

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    237     """"""
    238     # Actually call the layer (optionally building it).
--> 239     output = super(Layer, self).__call__(inputs, **kwargs)
    240     if context.executing_eagerly():
    241       return output

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)
    635     outputs, _ = self._run_internal_graph(inputs,
    636                                           training=training,
--> 637                                           mask=masks)
    638     return outputs
    639 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    770     # does not return a list the same size as `call`
    771     tensor_map = {}
--> 772     for x, y, mask in zip(self.inputs, inputs, masks):
    773       tensor_map[str(id(x))] = (y, mask)
    774 

TypeError: zip argument #1 must support iteration","Multiple Classes fails in Eager Mode (""tf.keras.Model"")","Multiple Classes fails in Eager Mode (""tf.keras.Model"")System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
Bazel version:
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Tried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu
TensorFlow installed from (source or binary):
Installed utilizing pip
TensorFlow version (use command below):
1.7
Python version:
3.6
Exact command to reproduce:

import tensorflow as tf  
import tensorflow.contrib.eager as tfe  

tfe.enable_eager_execution()

class CustomLayer(tf.keras.Model):
    def __init__(self):
        super(CustomLayer, self).__init__()
        print(""blah"")

class CustomNetwork(tf.keras.Model):
    def __init__(self):
        super(CustomNetwork, self).__init__()
        self.custom_layers = CustomLayer()

    def forward(self, x, y=None):
        x = self.custom_layers(x)

CustomNetwork().forward(tf.convert_to_tensor([1]))

Describe the problem
Trying to utilize multiple classes fails in tensorflow eager mode utilizing ""tf.keras.Model"". If I change ""tf.keras.Model"" to ""tfe.Network"" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:
Source code / logs
blah
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-9afa9b91ddef> in <module>()
----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))

<ipython-input-11-484119102aec> in forward(self, x, y)
      5 
      6     def forward(self, x, y=None):
----> 7         x = self.custom_layers(x)

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    237     """"""
    238     # Actually call the layer (optionally building it).
--> 239     output = super(Layer, self).__call__(inputs, **kwargs)
    240     if context.executing_eagerly():
    241       return output

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)
    635     outputs, _ = self._run_internal_graph(inputs,
    636                                           training=training,
--> 637                                           mask=masks)
    638     return outputs
    639 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    770     # does not return a list the same size as `call`
    771     tensor_map = {}
--> 772     for x, y, mask in zip(self.inputs, inputs, masks):
    773       tensor_map[str(id(x))] = (y, mask)
    774 

TypeError: zip argument #1 must support iteration",3
,2499,148,19,7683,"Hello,
I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:
import org.tensorflow.DataType;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;
Can any one help to find out where some thing is missing.",Package not Reslove.,"Package not Reslove.Hello,
I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:
import org.tensorflow.DataType;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;
Can any one help to find out where some thing is missing.",3
,2500,148,20,2138,"Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4
So when I use a simple Embedding RNN Sequence to Sequence Model like this
# choose RNN/GRU/LSTM cell
        with tf.variable_scope(""train_test"", reuse=True):
            self.cell = rnn_cell.LSTMCell(self.memory_dim)

        # embedding model
        with tf.variable_scope(""train_test""):
            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length)
        with tf.variable_scope(""train_test"", reuse = True):
            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)

The above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,

        # choose RNN/GRU/LSTM cell
        with tf.variable_scope(""train_test"", reuse=True):
            self.cell = rnn_cell.LSTMCell(self.memory_dim)

        with tf.variable_scope(""train_test""):
            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length)
        with tf.variable_scope(""train_test"", reuse = True):
            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)


I get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with
batch_size =1 
see.memory_dim = 1

and still got the same segmentation fault.
I get the same error, and the above configuration can certainly not eat my RAM.
This is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.
THE BUG REPORT
The Debug result
(gdb) run train_script_lstm_attn.py 
Starting program: /lusr/bin/python train_script_lstm_attn.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
[New Thread 0x7fffd7a25700 (LWP 45650)]
[New Thread 0x7fffd7224700 (LWP 45651)]
[New Thread 0x7fffd4a23700 (LWP 45652)]
[New Thread 0x7fffd2222700 (LWP 45653)]
[New Thread 0x7fffcfa21700 (LWP 45654)]
[New Thread 0x7fffcd220700 (LWP 45655)]
[New Thread 0x7fffcaa1f700 (LWP 45656)]
[Thread 0x7fffcaa1f700 (LWP 45656) exited]
[Thread 0x7fffcfa21700 (LWP 45654) exited]
[Thread 0x7fffd7a25700 (LWP 45650) exited]
[Thread 0x7fffd2222700 (LWP 45653) exited]
[Thread 0x7fffd7224700 (LWP 45651) exited]
[Thread 0x7fffcd220700 (LWP 45655) exited]
[Thread 0x7fffd4a23700 (LWP 45652) exited]
[New Thread 0x7fffcaa1f700 (LWP 45661)]
[New Thread 0x7fffcd220700 (LWP 46103)]
[New Thread 0x7fffcfa21700 (LWP 46104)]
[New Thread 0x7fffd2222700 (LWP 46105)]
[New Thread 0x7ffed22bf700 (LWP 46106)]
[New Thread 0x7ffed1abe700 (LWP 46107)]
[New Thread 0x7ffed12bd700 (LWP 46108)]
[New Thread 0x7ffed0abc700 (LWP 46109)]
[New Thread 0x7ffec3fff700 (LWP 46110)]
[New Thread 0x7ffec37fe700 (LWP 46111)]
[New Thread 0x7ffec2ffd700 (LWP 46112)]
[New Thread 0x7ffeb77ff700 (LWP 46114)]
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN Black
major: 3 minor: 5 memoryClockRate (GHz) 0.98
pciBusID 0000:05:00.0
Total memory: 6.00GiB
Free memory: 5.91GiB
[New Thread 0x7ffeb6ffe700 (LWP 46115)]
[New Thread 0x7ffeb67fd700 (LWP 46116)]
[New Thread 0x7ffea2bff700 (LWP 46117)]
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: GeForce GTX TITAN Black
major: 3 minor: 5 memoryClockRate (GHz) 0.98
pciBusID 0000:42:00.0
Total memory: 6.00GiB
Free memory: 5.91GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)
[New Thread 0x7ffea23fe700 (LWP 46118)]
[New Thread 0x7ffea1bfd700 (LWP 46119)]
[New Thread 0x7ffea13fc700 (LWP 46120)]
[New Thread 0x7ffea0bfb700 (LWP 46121)]
[New Thread 0x7ffe8bfff700 (LWP 46122)]
[New Thread 0x7ffe8b7fe700 (LWP 46123)]
[New Thread 0x7ffe8affd700 (LWP 46124)]
[New Thread 0x7ffe8a7fc700 (LWP 46125)]
[New Thread 0x7ffe89ffb700 (LWP 46126)]
[New Thread 0x7ffe897fa700 (LWP 46127)]
[New Thread 0x7ffe88ff9700 (LWP 46128)]
[New Thread 0x7ffe7bfff700 (LWP 46129)]
[New Thread 0x7ffe3d23c700 (LWP 46167)]

Program received signal SIGSEGV, Segmentation fault.
__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143
2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.

The Backtrace is attached below
(gdb) backtrace
#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143
#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---
::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#16 0x00007fffedc0bc11 in TF_Run ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#19 0x00007fffece7a4d7 in _wrap_TF_Run ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020
#21 PyEval_EvalFrameEx (f=f@entry=
    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), 
    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, 
    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, 
    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252
#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526
#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)
    at ../Objects/abstract.c:2529
#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, 
    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333
---Type <return> to continue, or q <return> to quit---
#26 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705
#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, 
    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116
#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041
#30 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, 
    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116
#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041
#34 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---
es': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, 
    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116
#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041
#38 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, 
    closure=0x0) at ../Python/ceval.c:3252
#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, 
    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116
#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041
#42 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---
ype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, 
    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106
#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041
#45 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, 
    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106
#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041
#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), 
    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, 
Python Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: 
    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252
Python Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: 
#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667
#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", 
    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), 
---Type <return> to continue, or q <return> to quit---
    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, 
    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370
#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, 
    filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", start=start@entry=257, 
    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), 
    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)
    at ../Python/pythonrun.c:1356
#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, 
    filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)
    at ../Python/pythonrun.c:948
#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, 
---Type <return> to continue, or q <return> to quit---
    filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", closeit=closeit@entry=1, 
    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752
#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640
#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287
#57 0x0000000000578c4e in _start ()
(gdb)",embedding_attention_seq2seq fails / embedding_rnn_seq2seq works,"embedding_attention_seq2seq fails / embedding_rnn_seq2seq worksEnvironment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4
So when I use a simple Embedding RNN Sequence to Sequence Model like this
# choose RNN/GRU/LSTM cell
        with tf.variable_scope(""train_test"", reuse=True):
            self.cell = rnn_cell.LSTMCell(self.memory_dim)

        # embedding model
        with tf.variable_scope(""train_test""):
            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length)
        with tf.variable_scope(""train_test"", reuse = True):
            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)

The above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,

        # choose RNN/GRU/LSTM cell
        with tf.variable_scope(""train_test"", reuse=True):
            self.cell = rnn_cell.LSTMCell(self.memory_dim)

        with tf.variable_scope(""train_test""):
            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length)
        with tf.variable_scope(""train_test"", reuse = True):
            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\
                            self.enc_inp, self.dec_inp, self.cell, \
                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)


I get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with
batch_size =1 
see.memory_dim = 1

and still got the same segmentation fault.
I get the same error, and the above configuration can certainly not eat my RAM.
This is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.
THE BUG REPORT
The Debug result
(gdb) run train_script_lstm_attn.py 
Starting program: /lusr/bin/python train_script_lstm_attn.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
[New Thread 0x7fffd7a25700 (LWP 45650)]
[New Thread 0x7fffd7224700 (LWP 45651)]
[New Thread 0x7fffd4a23700 (LWP 45652)]
[New Thread 0x7fffd2222700 (LWP 45653)]
[New Thread 0x7fffcfa21700 (LWP 45654)]
[New Thread 0x7fffcd220700 (LWP 45655)]
[New Thread 0x7fffcaa1f700 (LWP 45656)]
[Thread 0x7fffcaa1f700 (LWP 45656) exited]
[Thread 0x7fffcfa21700 (LWP 45654) exited]
[Thread 0x7fffd7a25700 (LWP 45650) exited]
[Thread 0x7fffd2222700 (LWP 45653) exited]
[Thread 0x7fffd7224700 (LWP 45651) exited]
[Thread 0x7fffcd220700 (LWP 45655) exited]
[Thread 0x7fffd4a23700 (LWP 45652) exited]
[New Thread 0x7fffcaa1f700 (LWP 45661)]
[New Thread 0x7fffcd220700 (LWP 46103)]
[New Thread 0x7fffcfa21700 (LWP 46104)]
[New Thread 0x7fffd2222700 (LWP 46105)]
[New Thread 0x7ffed22bf700 (LWP 46106)]
[New Thread 0x7ffed1abe700 (LWP 46107)]
[New Thread 0x7ffed12bd700 (LWP 46108)]
[New Thread 0x7ffed0abc700 (LWP 46109)]
[New Thread 0x7ffec3fff700 (LWP 46110)]
[New Thread 0x7ffec37fe700 (LWP 46111)]
[New Thread 0x7ffec2ffd700 (LWP 46112)]
[New Thread 0x7ffeb77ff700 (LWP 46114)]
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN Black
major: 3 minor: 5 memoryClockRate (GHz) 0.98
pciBusID 0000:05:00.0
Total memory: 6.00GiB
Free memory: 5.91GiB
[New Thread 0x7ffeb6ffe700 (LWP 46115)]
[New Thread 0x7ffeb67fd700 (LWP 46116)]
[New Thread 0x7ffea2bff700 (LWP 46117)]
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: GeForce GTX TITAN Black
major: 3 minor: 5 memoryClockRate (GHz) 0.98
pciBusID 0000:42:00.0
Total memory: 6.00GiB
Free memory: 5.91GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)
[New Thread 0x7ffea23fe700 (LWP 46118)]
[New Thread 0x7ffea1bfd700 (LWP 46119)]
[New Thread 0x7ffea13fc700 (LWP 46120)]
[New Thread 0x7ffea0bfb700 (LWP 46121)]
[New Thread 0x7ffe8bfff700 (LWP 46122)]
[New Thread 0x7ffe8b7fe700 (LWP 46123)]
[New Thread 0x7ffe8affd700 (LWP 46124)]
[New Thread 0x7ffe8a7fc700 (LWP 46125)]
[New Thread 0x7ffe89ffb700 (LWP 46126)]
[New Thread 0x7ffe897fa700 (LWP 46127)]
[New Thread 0x7ffe88ff9700 (LWP 46128)]
[New Thread 0x7ffe7bfff700 (LWP 46129)]
[New Thread 0x7ffe3d23c700 (LWP 46167)]

Program received signal SIGSEGV, Segmentation fault.
__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143
2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.

The Backtrace is attached below
(gdb) backtrace
#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143
#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---
::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#16 0x00007fffedc0bc11 in TF_Run ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#19 0x00007fffece7a4d7 in _wrap_TF_Run ()
   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020
#21 PyEval_EvalFrameEx (f=f@entry=
    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), 
    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, 
    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, 
    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252
#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526
#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)
    at ../Objects/abstract.c:2529
#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, 
    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333
---Type <return> to continue, or q <return> to quit---
#26 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705
#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, 
    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116
#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041
#30 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, 
    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116
#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041
#34 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---
es': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at ../Python/ceval.c:3252
#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, 
    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116
#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041
#38 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, 
    closure=0x0) at ../Python/ceval.c:3252
#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, 
    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116
#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041
#42 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---
ype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, 
    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106
#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041
#45 PyEval_EvalFrameEx (
    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, 
    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106
#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041
#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), 
    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, 
Python Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: 
    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252
Python Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: 
#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667
#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", 
    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), 
---Type <return> to continue, or q <return> to quit---
    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, 
    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370
#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, 
    filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", start=start@entry=257, 
    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), 
    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)
    at ../Python/pythonrun.c:1356
#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, 
    filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)
    at ../Python/pythonrun.c:948
#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, 
---Type <return> to continue, or q <return> to quit---
    filename=filename@entry=0x7fffffffed54 ""train_script_lstm_attn.py"", closeit=closeit@entry=1, 
    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752
#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640
#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287
#57 0x0000000000578c4e in _start ()
(gdb)",3
,2501,148,21,9570,"Please go to Stack Overflow for help and support:
http://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",Tensorflow inconsistence results every run,"Tensorflow inconsistence results every runPlease go to Stack Overflow for help and support:
http://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",3
,2502,148,22,96,"Hello,I got the error when i  execute:""pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl""--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.,"tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.Hello,I got the error when i  execute:""pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl""--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",3
,2503,148,23,11633,"I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?","when connect mnist,download mnist data,show network connection error.","when connect mnist,download mnist data,show network connection error.I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?",3
,2504,148,24,2269,"The tensor shape is not fixed, and can change with different input.","How to resize one tensor to (e.g., 1.5 * its original shape)?","How to resize one tensor to (e.g., 1.5 * its original shape)?The tensor shape is not fixed, and can change with different input.",3
,2505,148,25,11665,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.2.0
Python version: 3.6.1
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: See below.

Describe the problem
Feature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.
Source code / logs
import tensorflow as tf
import numpy as np

# Data that we wish to sample, but not save to disk.
X = np.eye(15, dtype=np.float32)

# Create a graph that samples rows from X randomly.
graph = tf.Graph()
with graph.as_default():
    X_init = tf.placeholder(tf.float32, shape=X.shape)
    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,
    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.
    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])
    queue = tf.RandomShuffleQueue(
        capacity=X.shape[0],
        min_after_dequeue=1,
        dtypes=[tf.float32],
        shapes=[X.shape[1]])
    enqueue_op = queue.enqueue_many([X_var])
    row = queue.dequeue()

# Sample a few rows from X.
with graph.as_default():
    sess_params = {
        'scaffold': tf.train.Scaffold(
            init_feed_dict={X_init: X},
            init_fn=lambda scaffold, sess: sess.run(enqueue_op))
    }
    with tf.train.MonitoredTrainingSession(**sess_params) as sess:
        print(sess.run(row))
        print(sess.run(row))",Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`,"Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.2.0
Python version: 3.6.1
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: See below.

Describe the problem
Feature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.
Source code / logs
import tensorflow as tf
import numpy as np

# Data that we wish to sample, but not save to disk.
X = np.eye(15, dtype=np.float32)

# Create a graph that samples rows from X randomly.
graph = tf.Graph()
with graph.as_default():
    X_init = tf.placeholder(tf.float32, shape=X.shape)
    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,
    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.
    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])
    queue = tf.RandomShuffleQueue(
        capacity=X.shape[0],
        min_after_dequeue=1,
        dtypes=[tf.float32],
        shapes=[X.shape[1]])
    enqueue_op = queue.enqueue_many([X_var])
    row = queue.dequeue()

# Sample a few rows from X.
with graph.as_default():
    sess_params = {
        'scaffold': tf.train.Scaffold(
            init_feed_dict={X_init: X},
            init_fn=lambda scaffold, sess: sess.run(enqueue_op))
    }
    with tf.train.MonitoredTrainingSession(**sess_params) as sess:
        print(sess.run(row))
        print(sess.run(row))",3
,2506,148,26,3185,"I use Jupyter notebook to open the .ipynb files, but it shows a red ""None"" kernel on top right corner and all lines of code cannot run.
Method I use:

Build a new directory and extract .ipynb files from examples/udacity to the directory
In terminal, run jupyter notebook","Udacity Notebook with ""None"" kernel","Udacity Notebook with ""None"" kernelI use Jupyter notebook to open the .ipynb files, but it shows a red ""None"" kernel on top right corner and all lines of code cannot run.
Method I use:

Build a new directory and extract .ipynb files from examples/udacity to the directory
In terminal, run jupyter notebook",3
,2507,144,27,11099,"Illustrating image for Concatenate
suggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",Typo in illustrating figure for XLA/Concatenation operation,"Typo in illustrating figure for XLA/Concatenation operationIllustrating image for Concatenate
suggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",3
,2508,143,28,12764,"these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep
Training data is downloaded to /tmp/tmpFB4dsd
Test data is downloaded to /tmp/tmpomj5Pi
2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpWei9wK
accuracy: 0.850071
accuracy_baseline: 0.763774
auc: 0.894038
auc_precision_recall: 0.743199
average_loss: 0.393638
global_step: 2000
label/mean: 0.236226
loss: 39.3179
prediction/mean: 0.242167
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide
Training data is downloaded to /tmp/tmpFJdWft
Test data is downloaded to /tmp/tmpjB5nm7
2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpuPHsDx
accuracy: 0.835391
accuracy_baseline: 0.763774
auc: 0.882763
auc_precision_recall: 0.694257
average_loss: 0.352975
global_step: 2000
label/mean: 0.236226
loss: 35.2563
prediction/mean: 0.240918
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py
Training data is downloaded to /tmp/tmpDdWc_T
Test data is downloaded to /tmp/tmpFF0PZJ
2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpq2M7SE
accuracy: 0.820834
accuracy_baseline: 0.763774
auc: 0.850518
auc_precision_recall: 0.676198
average_loss: 0.424271
global_step: 2000
label/mean: 0.236226
loss: 42.3776
prediction/mean: 0.256489
I don't know why looks likes this, so someone can help me? thank you.",the side &deep model   is not good compare with deep model and wide model ,"the side &deep model   is not good compare with deep model and wide model these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep
Training data is downloaded to /tmp/tmpFB4dsd
Test data is downloaded to /tmp/tmpomj5Pi
2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpWei9wK
accuracy: 0.850071
accuracy_baseline: 0.763774
auc: 0.894038
auc_precision_recall: 0.743199
average_loss: 0.393638
global_step: 2000
label/mean: 0.236226
loss: 39.3179
prediction/mean: 0.242167
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide
Training data is downloaded to /tmp/tmpFJdWft
Test data is downloaded to /tmp/tmpjB5nm7
2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpuPHsDx
accuracy: 0.835391
accuracy_baseline: 0.763774
auc: 0.882763
auc_precision_recall: 0.694257
average_loss: 0.352975
global_step: 2000
label/mean: 0.236226
loss: 35.2563
prediction/mean: 0.240918
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py
Training data is downloaded to /tmp/tmpDdWc_T
Test data is downloaded to /tmp/tmpFF0PZJ
2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpq2M7SE
accuracy: 0.820834
accuracy_baseline: 0.763774
auc: 0.850518
auc_precision_recall: 0.676198
average_loss: 0.424271
global_step: 2000
label/mean: 0.236226
loss: 42.3776
prediction/mean: 0.256489
I don't know why looks likes this, so someone can help me? thank you.",3
,2509,148,29,17358,"I understand tensorflow distributed training and I implemented my own script.
What I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.
Let's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.
My intuition to achieve this goal is to do the following:
...
elif FLAGS.job_name == ""worker"":

    if FLAGS.task_index <= (len(cluster_dict[""worker""][:-2]) - 1):
         logging.info(""Training worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                train_model = Model(
                    mode=tf.contrib.learn.ModeKeys.TRAIN
                )
               with tf.train.MonitoredTrainingSession(
                    is_chief=(FLAGS.task_index == 0),
                    master=server.target,
                    checkpoint_dir=ckpt_dir,
                    config=config_proto,
                    hooks=hooks
                ) as mon_sess:
                    while not mon_sess.should_stop():
                        res = train_model.train(...)
                        ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-2]) - 1):
         logging.info(""Evaluation worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                eval_model = Model(
                    mode=tf.contrib.learn.ModeKeys.EVAL
                )
                ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-1]) - 1):
        logging.info(""Inference worker started"")
        ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                infer_model = Model(
                    mode=tf.contrib.learn.ModeKeys.INFER
                )
                ...

Now, what about the evaluation and inference sessions?
For training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.
Regarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  eval_model.eval(...) or  infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to ""periodically"" is to sleep the thread.
What do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?
Alberto",Distributed training: Evaluation and inference best practices,"Distributed training: Evaluation and inference best practicesI understand tensorflow distributed training and I implemented my own script.
What I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.
Let's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.
My intuition to achieve this goal is to do the following:
...
elif FLAGS.job_name == ""worker"":

    if FLAGS.task_index <= (len(cluster_dict[""worker""][:-2]) - 1):
         logging.info(""Training worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                train_model = Model(
                    mode=tf.contrib.learn.ModeKeys.TRAIN
                )
               with tf.train.MonitoredTrainingSession(
                    is_chief=(FLAGS.task_index == 0),
                    master=server.target,
                    checkpoint_dir=ckpt_dir,
                    config=config_proto,
                    hooks=hooks
                ) as mon_sess:
                    while not mon_sess.should_stop():
                        res = train_model.train(...)
                        ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-2]) - 1):
         logging.info(""Evaluation worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                eval_model = Model(
                    mode=tf.contrib.learn.ModeKeys.EVAL
                )
                ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-1]) - 1):
        logging.info(""Inference worker started"")
        ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                infer_model = Model(
                    mode=tf.contrib.learn.ModeKeys.INFER
                )
                ...

Now, what about the evaluation and inference sessions?
For training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.
Regarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  eval_model.eval(...) or  infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to ""periodically"" is to sleep the thread.
What do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?
Alberto",3
,2510,148,30,14170,"After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":
After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":

c/c_api_function.cc:  static string Normalize(string name);
Suggestion: string& name;
c/c_api_function.cc:string NodeNameMapping::Normalize(string name) {
Suggestion: string& name



compiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);
Suggestion: const string&
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {
Suggestion: const string&
compiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {
Suggestion: string& name
compiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {
Suggestion: string& function_name
compiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);
Suggestion: string& function_name
compiler/xla/util.cc:string SanitizeFileName(string file_name) {
Suggestion: string& file_name
compiler/xla/util.h:string SanitizeFileName(string file_name);
Suggestion: string& file_name



contrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name



core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {
Suggestion: string& device_name
core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {
Suggestion: string& device_name
core/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {
Suggestion: const string& suffix
core/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);
Suggestion: const string& suffix
core/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {
Suggestion: const string& msg



stream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {
Suggestion: const string& kind
stream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);
Suggestion: const string& platform_string","List of functions could be improved with ""const std::string&"" or ""std::string&"" instead ""std::string""","List of functions could be improved with ""const std::string&"" or ""std::string&"" instead ""std::string""After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":
After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":

c/c_api_function.cc:  static string Normalize(string name);
Suggestion: string& name;
c/c_api_function.cc:string NodeNameMapping::Normalize(string name) {
Suggestion: string& name



compiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);
Suggestion: const string&
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {
Suggestion: const string&
compiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {
Suggestion: string& name
compiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {
Suggestion: string& function_name
compiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);
Suggestion: string& function_name
compiler/xla/util.cc:string SanitizeFileName(string file_name) {
Suggestion: string& file_name
compiler/xla/util.h:string SanitizeFileName(string file_name);
Suggestion: string& file_name



contrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name



core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {
Suggestion: string& device_name
core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {
Suggestion: string& device_name
core/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {
Suggestion: const string& suffix
core/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);
Suggestion: const string& suffix
core/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {
Suggestion: const string& msg



stream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {
Suggestion: const string& kind
stream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);
Suggestion: const string& platform_string",3
,2511,148,31,18952,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.8
Python version:  3.6
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory:NA
Exact command to reproduce:
tf.contrib.data.make_csv_dataset()

Describe the problem
The tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.
For example the following code:
dataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)
dataset = dataset.batch(4)
X_iter = dataset.make_one_shot_iterator()
X_batch = X_iter.get_next()
X_batch

results in following dataset:
{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,
 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,
 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,
 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}

However I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:
df_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',
                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)

I know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset(),"Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset()System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.8
Python version:  3.6
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory:NA
Exact command to reproduce:
tf.contrib.data.make_csv_dataset()

Describe the problem
The tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.
For example the following code:
dataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)
dataset = dataset.batch(4)
X_iter = dataset.make_one_shot_iterator()
X_batch = X_iter.get_next()
X_batch

results in following dataset:
{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,
 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,
 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,
 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}

However I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:
df_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',
                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)

I know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",3
,2512,143,32,4917,"I'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:
import numpy as np
import multiprocessing
import tensorflow as tf

n_cpus = multiprocessing.cpu_count()

sess = tf.Session()
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
mult = tf.mul(a, b)

def python_op(x):
    print ""python_op called with {}"".format(x)
    # In my real function, the np.cos and np.sin calls are replaced by
    # python calculations I can't do in tensorflow
    y = np.cos(x)
    z = sess.run(mult, feed_dict={a: y, b: x})
    print ""intermediate result is {}"".format(z)
    return np.sin(z)

n_inputs = n_cpus
input_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
load_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))

output_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
get_result = output_queue.dequeue_many(n_inputs)

def processing_pipeline():
    input_value = input_queue.dequeue()
    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))

# Here's the problem: If we use all CPUs here, the program will deadlock.
# If we change cpus to (cpus-1), it works as expected.
runner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))

coord = tf.train.Coordinator()
runner.create_threads(sess, coord=coord, start=True)

print ""Loading input""
sess.run(load_input)
sess.run(input_queue.close())

try:
    print ""waiting for result""
    result = sess.run(get_result)
    print ""RESULT: {}"".format(result)
except tf.errors.OutOfRangeError:
    print ""Input exhausted""

coord.request_stop()
coord.join()
print ""Done""
The program above deadlocks waiting for sess.run to complete in python_op:
$ python queuetest.py                                                                                                                                                                                                                                                  
Loading input
python_op called with [ 0.65624136]
 python_op called with [ 0.80651367]
python_op called with [ 0.31998941]
 python_op called with [ 0.726421]
 python_op called with [ 0.33133706]
python_op called with [ 0.4912357]
python_op called with [ 0.27365881]
python_op called with [ 0.32846987]

This is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:
$ python queuetest.py                                                                                                                                                                                                                                                        
Loading input
python_op called with [ 0.40804103]
python_op called with [ 0.0182138]
python_op called with [ 0.17579727]
 python_op called with [ 0.29143187]
python_op called with [ 0.11612369]
 intermediate result is [ 0.37454084]
python_op called with [ 0.679506]
python_op called with [ 0.50754625]
intermediate result is [ 0.01821078]
intermediate result is [ 0.52857631]
 intermediate result is [ 0.27914321]
waiting for result
python_op called with [ 0.68288684]
 intermediate result is [ 0.44356483]
intermediate result is [ 0.11534163]
 intermediate result is [ 0.17308778]
intermediate result is [ 0.52975237]
RESULT: [[ 0.36584523]
 [ 0.01820978]
 [ 0.50430447]
 [ 0.42916203]
 [ 0.11508605]
 [ 0.27553213]
 [ 0.1722248 ]
 [ 0.50531965]]
Done

The program also completes successfully if we pass in fewer examples than CPUs in the input queue.
I realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:
""The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.""
So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?
As a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.
OS: Linux
Tensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",QueueRunner deadlock when using all CPUs,"QueueRunner deadlock when using all CPUsI'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:
import numpy as np
import multiprocessing
import tensorflow as tf

n_cpus = multiprocessing.cpu_count()

sess = tf.Session()
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
mult = tf.mul(a, b)

def python_op(x):
    print ""python_op called with {}"".format(x)
    # In my real function, the np.cos and np.sin calls are replaced by
    # python calculations I can't do in tensorflow
    y = np.cos(x)
    z = sess.run(mult, feed_dict={a: y, b: x})
    print ""intermediate result is {}"".format(z)
    return np.sin(z)

n_inputs = n_cpus
input_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
load_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))

output_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])
get_result = output_queue.dequeue_many(n_inputs)

def processing_pipeline():
    input_value = input_queue.dequeue()
    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))

# Here's the problem: If we use all CPUs here, the program will deadlock.
# If we change cpus to (cpus-1), it works as expected.
runner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))

coord = tf.train.Coordinator()
runner.create_threads(sess, coord=coord, start=True)

print ""Loading input""
sess.run(load_input)
sess.run(input_queue.close())

try:
    print ""waiting for result""
    result = sess.run(get_result)
    print ""RESULT: {}"".format(result)
except tf.errors.OutOfRangeError:
    print ""Input exhausted""

coord.request_stop()
coord.join()
print ""Done""
The program above deadlocks waiting for sess.run to complete in python_op:
$ python queuetest.py                                                                                                                                                                                                                                                  
Loading input
python_op called with [ 0.65624136]
 python_op called with [ 0.80651367]
python_op called with [ 0.31998941]
 python_op called with [ 0.726421]
 python_op called with [ 0.33133706]
python_op called with [ 0.4912357]
python_op called with [ 0.27365881]
python_op called with [ 0.32846987]

This is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:
$ python queuetest.py                                                                                                                                                                                                                                                        
Loading input
python_op called with [ 0.40804103]
python_op called with [ 0.0182138]
python_op called with [ 0.17579727]
 python_op called with [ 0.29143187]
python_op called with [ 0.11612369]
 intermediate result is [ 0.37454084]
python_op called with [ 0.679506]
python_op called with [ 0.50754625]
intermediate result is [ 0.01821078]
intermediate result is [ 0.52857631]
 intermediate result is [ 0.27914321]
waiting for result
python_op called with [ 0.68288684]
 intermediate result is [ 0.44356483]
intermediate result is [ 0.11534163]
 intermediate result is [ 0.17308778]
intermediate result is [ 0.52975237]
RESULT: [[ 0.36584523]
 [ 0.01820978]
 [ 0.50430447]
 [ 0.42916203]
 [ 0.11508605]
 [ 0.27553213]
 [ 0.1722248 ]
 [ 0.50531965]]
Done

The program also completes successfully if we pass in fewer examples than CPUs in the input queue.
I realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:
""The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.""
So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?
As a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.
OS: Linux
Tensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",3
,2513,146,33,8273,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",Error:,"Error:NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",3
,2514,146,34,21142,"Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",Feature Request: Provide tf.pow with supporting  broadcasting?,"Feature Request: Provide tf.pow with supporting  broadcasting?Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",3
,2515,148,35,1279,"After sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:
tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
It seems it doesn't work with Python 3. What should I do?",Arch doesn't support it,"Arch doesn't support itAfter sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:
tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
It seems it doesn't work with Python 3. What should I do?",3
,2516,146,36,17877,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): unknown
TensorFlow version (use command below): 1.6.0
Python version: 3.6.3
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce:

import tensorflow as tf
tf.InteractiveSession()
print(tf.manip.roll(tf.range(5), -1, axis=0).eval())
# [1 2 3 4 0]
print(tf.manip.roll(tf.range(5), -1, axis=-1).eval())
# [0 1 2 3 4]

Describe the problem
axis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",tf.manip.roll silently ignores negative axes,"tf.manip.roll silently ignores negative axesSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): unknown
TensorFlow version (use command below): 1.6.0
Python version: 3.6.3
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce:

import tensorflow as tf
tf.InteractiveSession()
print(tf.manip.roll(tf.range(5), -1, axis=0).eval())
# [1 2 3 4 0]
print(tf.manip.roll(tf.range(5), -1, axis=-1).eval())
# [0 1 2 3 4]

Describe the problem
axis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",3
,2517,148,37,5447,"Hi all,
I am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.
The compilation command is :
EXTRA_BAZEL_ARGS=""--jobs 10"" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package
Compilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.
I have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.
Though i am able to compile cpu-only version of tensorflow successfully.
Please let me know if any information is needed from my side.
Eagerly awaiting your replies.
crosstool_wrapper_driver_is_not_gcc.tpl.txt
CROSSTOOL.tpl.txt
tensorflow_build2.log.txt",invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     ,"invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     Hi all,
I am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.
The compilation command is :
EXTRA_BAZEL_ARGS=""--jobs 10"" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package
Compilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.
I have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.
Though i am able to compile cpu-only version of tensorflow successfully.
Please let me know if any information is needed from my side.
Eagerly awaiting your replies.
crosstool_wrapper_driver_is_not_gcc.tpl.txt
CROSSTOOL.tpl.txt
tensorflow_build2.log.txt",3
,2518,148,38,7908,"I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.
using TF:1.0",Changing computer make the pretrained model fail,"Changing computer make the pretrained model failI switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.
using TF:1.0",3
,2519,144,39,4419,"The following fails (shape and name are arbitrary):
tf.get_variable(name='foo', shape=(42,), dtype=tf.int32)

Exception: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.
In contrast, using tf.float32 works just fine.
The problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658
If an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.
While this can be mitigated by doing something like:
tf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))

it feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).
Tested on the current master.",tf.get_variable without an explicit initializer fails for integer types,"tf.get_variable without an explicit initializer fails for integer typesThe following fails (shape and name are arbitrary):
tf.get_variable(name='foo', shape=(42,), dtype=tf.int32)

Exception: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.
In contrast, using tf.float32 works just fine.
The problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658
If an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.
While this can be mitigated by doing something like:
tf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))

it feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).
Tested on the current master.",3
,2520,148,40,806,,google-tensorflow,google-tensorflow,3
,2521,146,41,4021,"Whenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.
Currently the way I use is  to run a search with ctrl + f to find specific documentation related to what I search for.
Numpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.
It would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",Improving Google Indexing for the Documentation,"Improving Google Indexing for the DocumentationWhenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.
Currently the way I use is  to run a search with ctrl + f to find specific documentation related to what I search for.
Numpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.
It would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",3
,2522,148,42,23208,Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?,TfLiteCameraDemo.apk with NNAPI,TfLiteCameraDemo.apk with NNAPICan someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?,3
,2523,141,43,7638,"Links to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.

-->",Links to Android nightly builds on README.md are broken,"Links to Android nightly builds on README.md are brokenLinks to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.

-->",3
,2524,146,44,7558,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.,"tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",3
,2525,148,45,4397,"Summary: Why is it looking for the header file under the lib directory?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

#3989 (and references within)

Environment info
Operating System: Ubuntu 16.04. Docker 1.12.1.
If possible, provide a minimal reproducible example

Modify devel-gpu to read ""FROM nvidia/cuda:8.0-cudnn5-devel"".
Run docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)

What other attempted solutions have you tried?
None yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.
Logs or other output that would be helpful
$ docker build -f Dockerfile.devel-gpu -t tf .

[snip]

Step 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl
 ---> Running in 1f99527c7748
No Google Cloud Platform support will be enabled for TensorFlow
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Extracting Bazel installation...
____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
INFO: Reading 'startup' options from /root/.bazelrc: --batch
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.
Configuration finished
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu.
____Elapsed time: 0.391s
The command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1",Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib),"Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)Summary: Why is it looking for the header file under the lib directory?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

#3989 (and references within)

Environment info
Operating System: Ubuntu 16.04. Docker 1.12.1.
If possible, provide a minimal reproducible example

Modify devel-gpu to read ""FROM nvidia/cuda:8.0-cudnn5-devel"".
Run docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)

What other attempted solutions have you tried?
None yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.
Logs or other output that would be helpful
$ docker build -f Dockerfile.devel-gpu -t tf .

[snip]

Step 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl
 ---> Running in 1f99527c7748
No Google Cloud Platform support will be enabled for TensorFlow
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Extracting Bazel installation...
____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
INFO: Reading 'startup' options from /root/.bazelrc: --batch
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.
Configuration finished
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 412
        _create_cuda_repository(repository_ctx)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 344, in _create_cuda_repository
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 233, in _find_cudnn_header_dir
        fail(""Cannot find cudnn.h under %s"" %...)
Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu.
____Elapsed time: 0.391s
The command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1",3
,2526,148,46,4848,"Environment info
Operating System:
ubuntu 14.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
cuda 8.0.44, cudnn 5.5
If installed from binary pip package, provide:
No
If installed from source, provide

The commit hash (git rev-parse HEAD)
c8d4896
The output of bazel version
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
./configure   # configure with GPU support)
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass
bazel clean
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae
3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com
.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue
$ConfigurationFragmentKey@a93d9174')
at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
David, Damien, looks like bazel clean undid some output of ""./configure"" at head.
Any idea where things went wrong?","""bazel clean"" will undo ""./configure"" when source configured to build for GPU.","""bazel clean"" will undo ""./configure"" when source configured to build for GPU.Environment info
Operating System:
ubuntu 14.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
cuda 8.0.44, cudnn 5.5
If installed from binary pip package, provide:
No
If installed from source, provide

The commit hash (git rev-parse HEAD)
c8d4896
The output of bazel version
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
./configure   # configure with GPU support)
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass
bazel clean
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae
3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com
.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue
$ConfigurationFragmentKey@a93d9174')
at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
David, Damien, looks like bazel clean undid some output of ""./configure"" at head.
Any idea where things went wrong?",3
,2527,148,47,2291,"Environment info
Operating System:
epel-release-6-8.noarch
redhat-release-server-6Server-6.7.0.3.el6.x86_64
Installed version of CUDA and cuDNN:
None
If installed from sources, provide the commit hash:
f8eb1d7
Steps to reproduce

bazel clean
./configure
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

Logs or other output that would be helpful
(If logs are large, please upload as attachment).
ERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'
/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line
/lib64/libm.so.6: could not read symbols: Invalid operation
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build",Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5),"Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)Environment info
Operating System:
epel-release-6-8.noarch
redhat-release-server-6Server-6.7.0.3.el6.x86_64
Installed version of CUDA and cuDNN:
None
If installed from sources, provide the commit hash:
f8eb1d7
Steps to reproduce

bazel clean
./configure
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

Logs or other output that would be helpful
(If logs are large, please upload as attachment).
ERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'
/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line
/lib64/libm.so.6: could not read symbols: Invalid operation
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build",3
,2528,148,48,1828,"Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):
tf.pow()
tf.exp()
tf.log()

# lower priority?
tf.abs()
tf.neg()
tf.sign()
tf.inv()
tf.square()
tf.round()
tf.sqrt()
tf.ceil()
tf.floor()

and so on.
We'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",SparseTensor: common unary ops,"SparseTensor: common unary opsCurrently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):
tf.pow()
tf.exp()
tf.log()

# lower priority?
tf.abs()
tf.neg()
tf.sign()
tf.inv()
tf.square()
tf.round()
tf.sqrt()
tf.ceil()
tf.floor()

and so on.
We'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",3
,2529,143,49,19731,"hi, all:
I'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.
I tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.
How could I release GPU memory timely to avoid OOM error please?
Thanks!",How to release GPU memory after sess.close()?,"How to release GPU memory after sess.close()?hi, all:
I'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.
I tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.
How could I release GPU memory timely to avoid OOM error please?
Thanks!",3
,2530,143,50,6111,"To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:
Training Epoch 0 ; learning_rate= 0.002 :                                                                                                
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  

However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.
In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.
Below are my system info:
Operating System:
Ubuntu 14.04.5 LTS

Installed version of CUDA and cuDNN:
/usr/local/cuda-8.0
cudnn-8.0-linux-x64-v5.1.tgz

It is installed from binary pip package

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl

The output from python -c ""import tensorflow; print(tensorflow.version)""
xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c ""import tensorflow; print(tensorflow.version)""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0","Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory","Flawed memory management: allow_growth=True consumes more memory, causing out-of-memoryTo prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:
Training Epoch 0 ; learning_rate= 0.002 :                                                                                                
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  

However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.
In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.
Below are my system info:
Operating System:
Ubuntu 14.04.5 LTS

Installed version of CUDA and cuDNN:
/usr/local/cuda-8.0
cudnn-8.0-linux-x64-v5.1.tgz

It is installed from binary pip package

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl

The output from python -c ""import tensorflow; print(tensorflow.version)""
xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c ""import tensorflow; print(tensorflow.version)""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0",3
,2531,148,51,5284,"I tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.
I got the following traceback
traceback.txt
I explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py

When doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].
To sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.","tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatible","tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatibleI tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.
I got the following traceback
traceback.txt
I explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py

When doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].
To sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.",3
,2532,148,52,22249,"System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
not relevant


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:


TensorFlow installed from (source or binary):
1.10.1 (from pip binary)


TensorFlow version (use command below):
tf.VERSION = 1.10.1
tf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1
tf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1


Python version:
Python 3.5.3


Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Nov__3_21:07:56_CDT_2017
Cuda compilation tools, release 9.1, V9.1.85


GPU model and memory:
Tesla P100-PCIE-16GB


Exact command to reproduce:


Describe the problem
It appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)
Moreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)
Why is there this inconsistency?",Inconsistency in supported integer types on GPU,"Inconsistency in supported integer types on GPUSystem information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
not relevant


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:


TensorFlow installed from (source or binary):
1.10.1 (from pip binary)


TensorFlow version (use command below):
tf.VERSION = 1.10.1
tf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1
tf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1


Python version:
Python 3.5.3


Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Nov__3_21:07:56_CDT_2017
Cuda compilation tools, release 9.1, V9.1.85


GPU model and memory:
Tesla P100-PCIE-16GB


Exact command to reproduce:


Describe the problem
It appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)
Moreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)
Why is there this inconsistency?",3
,2533,144,53,8910,"There is a program that defines the loss function as follows:
reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES
weight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')


Running the program raises the following error message

File ""/home/ decoder/kitti_multiloss.py"", line 86, in loss
name='reg_loss')
File ""/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py"", line 1827, in add_n
raise ValueError(""inputs must be a list of at least one Tensor with the ""
ValueError: inputs must be a list of at least one Tensor with the same dtype and shape

I am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shape,"regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shapeThere is a program that defines the loss function as follows:
reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES
weight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')


Running the program raises the following error message

File ""/home/ decoder/kitti_multiloss.py"", line 86, in loss
name='reg_loss')
File ""/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py"", line 1827, in add_n
raise ValueError(""inputs must be a list of at least one Tensor with the ""
ValueError: inputs must be a list of at least one Tensor with the same dtype and shape

I am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",3
,2534,148,54,16995,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): 1.5.0
Python version: 2.7.12
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA-8.0 CuDNN 6.0
GPU model and memory: GTX 1080 8GB
Exact command to reproduce:

Describe the problem
tf.fake_quant_with_min_max_vars returns wrong answer.
Source code / logs
import tensorflow as tf

a =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)


sess = tf.InteractiveSession()
sess.run(tf.initialize_variables([a]))

print a.eval()
print tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()
print tf.reduce_min(a).eval()
print tf.reduce_max(a).eval()
it prints like below
[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253
  0.09277194  0.08704954  0.12734564  0.11479893]
[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124
  0.05249124  0.05249124  0.05249124  0.05249124]
0.0748544
0.127346",tf.fake_quant_with_min_max_vars returns wrong answer,"tf.fake_quant_with_min_max_vars returns wrong answerSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): 1.5.0
Python version: 2.7.12
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA-8.0 CuDNN 6.0
GPU model and memory: GTX 1080 8GB
Exact command to reproduce:

Describe the problem
tf.fake_quant_with_min_max_vars returns wrong answer.
Source code / logs
import tensorflow as tf

a =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)


sess = tf.InteractiveSession()
sess.run(tf.initialize_variables([a]))

print a.eval()
print tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()
print tf.reduce_min(a).eval()
print tf.reduce_max(a).eval()
it prints like below
[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253
  0.09277194  0.08704954  0.12734564  0.11479893]
[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124
  0.05249124  0.05249124  0.05249124  0.05249124]
0.0748544
0.127346",3
,2535,148,55,17121,"After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:
 I->(W1)->C1->(W2)->C2->(W3)->O

I is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?
I know we could get the parameter operators as follows:
W1_op = tf.get_default_graph().get_tensor_by_name('W1')
W1_op = ...

My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).
I know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.
In the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",Feature Request for the back-propagated errors in intermediate layers,"Feature Request for the back-propagated errors in intermediate layersAfter the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:
 I->(W1)->C1->(W2)->C2->(W3)->O

I is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?
I know we could get the parameter operators as follows:
W1_op = tf.get_default_graph().get_tensor_by_name('W1')
W1_op = ...

My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).
I know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.
In the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",3
,2536,148,56,6956,"As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.
Workaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",Error downloading nasm,"Error downloading nasmAs noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.
Workaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",3
,2537,148,57,3439,"I'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.
val_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)
reg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])

(X_val and Y_val are numpy arrays)
The code runs but only the first validation step is done properly, then I get the following message in the logs:
INFO:tensorflow:Input iterator is exhausted.
Any help is welcome!","""tensorflow: Input iterator is exhausted"" when passing numpy arrays in validation monitor","""tensorflow: Input iterator is exhausted"" when passing numpy arrays in validation monitorI'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.
val_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)
reg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])

(X_val and Y_val are numpy arrays)
The code runs but only the first validation step is done properly, then I get the following message in the logs:
INFO:tensorflow:Input iterator is exhausted.
Any help is welcome!",3
,2538,141,58,22357,This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.,Make NVIDIA library versions to TF Version matrix more visible.,Make NVIDIA library versions to TF Version matrix more visible.This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.,3
,2539,148,59,22861,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  v1.11.0-0-gc19e29306c 1.11.0
Python version: Python 3.5.2
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version: 7.2.1
GPU model and memory: GTX 1060 6GB
Exact command to reproduce: Please see the below

Describe the problem
tf.keras.Model makes Variables of its weights, when its build() or call() is called first.
While I found call() makes Variables with the prefix ""MyModel/"", build() make Variables without any prefix.
That is inconvenient to manage non-object based checkpoint.
import tensorflow as tf
import tensorflow.keras as keras

layers = keras.layers

class Model(keras.Model):
    def __init__(self, name=""MyModel""):
        super().__init__(name=name)
        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=""conv1"")
        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=""conv2"")

    def call(self, images):
        featmap = self.conv1(images)
        featmap = self.conv2(featmap)
        return featmap

    def inference(self, images):
        return self.__call__(images)


model = Model()
flags = ""build""

if flags == ""build"":
    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))

    for w in model.weights:
        print (w.op.name)
else:
    dummy = tf.zeros([1, 32, 32, 3])
    model(dummy)

    for w in model.weights:
        print (w.op.name)",Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call(),"Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  v1.11.0-0-gc19e29306c 1.11.0
Python version: Python 3.5.2
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version: 7.2.1
GPU model and memory: GTX 1060 6GB
Exact command to reproduce: Please see the below

Describe the problem
tf.keras.Model makes Variables of its weights, when its build() or call() is called first.
While I found call() makes Variables with the prefix ""MyModel/"", build() make Variables without any prefix.
That is inconvenient to manage non-object based checkpoint.
import tensorflow as tf
import tensorflow.keras as keras

layers = keras.layers

class Model(keras.Model):
    def __init__(self, name=""MyModel""):
        super().__init__(name=name)
        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=""conv1"")
        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=""conv2"")

    def call(self, images):
        featmap = self.conv1(images)
        featmap = self.conv2(featmap)
        return featmap

    def inference(self, images):
        return self.__call__(images)


model = Model()
flags = ""build""

if flags == ""build"":
    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))

    for w in model.weights:
        print (w.op.name)
else:
    dummy = tf.zeros([1, 32, 32, 3])
    model(dummy)

    for w in model.weights:
        print (w.op.name)",3
,2540,143,60,6644,"Hi there,
I'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm
I am getting a ""Data loss"" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.
Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.
W tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable
Traceback (most recent call last):
  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 111, in run_eval
    while ckpt_loader.load_checkpoint():
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 45, in load_checkpoint
    if load_from_checkpoint(self.saver, self.logdir):
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 27, in load_from_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)

tensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]
Caused by op u'save/RestoreV2_15', defined at:  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 98, in run_eval
    saver = tf.train.Saver(model.avg_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1000, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1030, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 624, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 361, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 200, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 441, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

DataLossError (see above for traceback): file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/repl
ica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha
pe_and_slices)]]",Error: Data loss: file is too short to be an sstable,"Error: Data loss: file is too short to be an sstableHi there,
I'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm
I am getting a ""Data loss"" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.
Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.
W tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable
Traceback (most recent call last):
  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 111, in run_eval
    while ckpt_loader.load_checkpoint():
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 45, in load_checkpoint
    if load_from_checkpoint(self.saver, self.logdir):
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 27, in load_from_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)

tensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]
Caused by op u'save/RestoreV2_15', defined at:  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 98, in run_eval
    saver = tf.train.Saver(model.avg_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1000, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1030, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 624, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 361, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 200, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 441, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

DataLossError (see above for traceback): file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/repl
ica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha
pe_and_slices)]]",3
,2541,143,61,12345,"System information
N/A
Describe the problem
Background
PEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.
When adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the ""untyped"" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.
Benefits of Adding Type Annotations

The expected inputs and outputs of functions become much clearer
Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs
Static analysis can uncover latent bugs (case study here[5])

Difficulties/Drawbacks

People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages ""Power Features"" [4] I would argue that striving towards code that is explicit is a similar philosophy
The protobuf compiler would need to be augmented to generate type annotations.
The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.
Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.

Final thoughts
I realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.
[1] https://www.python.org/dev/peps/pep-0484/
[2] http://mypy-lang.org/
[3] https://github.com/python/typeshed
[4] https://google.github.io/styleguide/pyguide.html#Power_Features
[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",PEP 484 Type Annotations (feature request),"PEP 484 Type Annotations (feature request)System information
N/A
Describe the problem
Background
PEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.
When adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the ""untyped"" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.
Benefits of Adding Type Annotations

The expected inputs and outputs of functions become much clearer
Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs
Static analysis can uncover latent bugs (case study here[5])

Difficulties/Drawbacks

People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages ""Power Features"" [4] I would argue that striving towards code that is explicit is a similar philosophy
The protobuf compiler would need to be augmented to generate type annotations.
The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.
Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.

Final thoughts
I realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.
[1] https://www.python.org/dev/peps/pep-0484/
[2] http://mypy-lang.org/
[3] https://github.com/python/typeshed
[4] https://google.github.io/styleguide/pyguide.html#Power_Features
[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",3
,2542,148,62,20096,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary (CPU linux wheel)
TensorFlow version (use command below): 1.8.0
Python version: 3.6.5

Describe the problem
Using Tensorflow 1.8.0, running:
from tensorflow.keras.utils import Progbar
raises an error:
ModuleNotFoundError: No module named 'tensorflow.keras'

Of course, from tensorflow import keras works fine.
This is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result .",ModuleNotFoundError: No module named 'tensorflow.keras',"ModuleNotFoundError: No module named 'tensorflow.keras'System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary (CPU linux wheel)
TensorFlow version (use command below): 1.8.0
Python version: 3.6.5

Describe the problem
Using Tensorflow 1.8.0, running:
from tensorflow.keras.utils import Progbar
raises an error:
ModuleNotFoundError: No module named 'tensorflow.keras'

Of course, from tensorflow import keras works fine.
This is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result .",3
,2543,148,63,14070,"can you implement weight norm ?
I want to use it as follows.
    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)
    x = weight_norm(x)
Is it possible?",Feature request : add weight normalization,"Feature request : add weight normalizationcan you implement weight norm ?
I want to use it as follows.
    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)
    x = weight_norm(x)
Is it possible?",3
,2544,148,64,6769,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
tf.cast()
Environment info
Operating System:
masOSSierra
jupyter notebook
tensforflow v0.12.1
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
No
If installed from binary pip package, provide:

A link to the pip package you installed: pip install tensorflow
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".tensforflow v0.12.1

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py
What other attempted solutions have you tried?
try to cast DataFrame into float32 with
X_train = X_train.astype(np.float32)
try to cast each column with
tf.cast(col, tf.float32)
but after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)
feature_columns dtype just turn to tf.float64
(tried and didn't find attribute from source code that I can change dtype here)
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

TypeError                                 Traceback (most recent call last)
 in ()
----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])
/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
189             _call_location(), decorator_utils.get_qualified_name(func),
190             func.module, arg_name, date, instructions)
--> 191       return func(*args, **kwargs)
192     new_func.doc = _add_deprecated_arg_notice_to_docstring(
193         func.doc, date, instructions)
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
353                              steps=steps,
354                              monitors=monitors,
--> 355                              max_steps=max_steps)
356     logging.info('Loss for final step: %s.', loss)
357     return self
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
697       # cases, but will soon be deleted after the subclasses are updated.
698       # TODO(b/32664904): Update subclasses and delete the else-statement.
--> 699       train_ops = self._get_train_ops(features, labels)
700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature
701         train_op = train_ops.train_op
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
1050       ModelFnOps object.
1051     """"""
-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
1053
1054   def _get_eval_ops(self, features, labels, metrics):
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
1019                                           params=self.params)
1020       else:
-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)
1022     else:
1023       model_fn_results = self._model_fn(features, labels)
 in conv_model(feature, target, mode)
10                                     activation_fn=tf.nn.relu)
11
---> 12         h_pool1 = max_pool_2x2(h_conv1)
13
14     with tf.variable_scope('conv_layer2'):
 in max_pool_2x2(tensor_in)
1 def max_pool_2x2(tensor_in):
----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)
1615                                 padding=padding,
1616                                 data_format=data_format,
-> 1617                                 name=name)
1618
1619
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)
1596   result = _op_def_lib.apply_op(""MaxPool"", input=input, ksize=ksize,
1597                                 strides=strides, padding=padding,
-> 1598                                 data_format=data_format, name=name)
1599   return result
1600
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
580             for base_type in base_types:
581               _SatisfiesTypeConstraint(base_type,
--> 582                                        _Attr(op_def, input_arg.type_attr))
583             attrs[input_arg.type_attr] = attr_value
584             inferred_from[input_arg.type_attr] = input_name
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)
58           ""DataType %s for attr '%s' not in list of allowed values: %s"" %
59           (dtypes.as_dtype(dtype).name, attr_def.name,
---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
61
62
TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16
Many thanks.","TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16","TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
tf.cast()
Environment info
Operating System:
masOSSierra
jupyter notebook
tensforflow v0.12.1
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
No
If installed from binary pip package, provide:

A link to the pip package you installed: pip install tensorflow
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".tensforflow v0.12.1

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py
What other attempted solutions have you tried?
try to cast DataFrame into float32 with
X_train = X_train.astype(np.float32)
try to cast each column with
tf.cast(col, tf.float32)
but after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)
feature_columns dtype just turn to tf.float64
(tried and didn't find attribute from source code that I can change dtype here)
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

TypeError                                 Traceback (most recent call last)
 in ()
----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])
/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
189             _call_location(), decorator_utils.get_qualified_name(func),
190             func.module, arg_name, date, instructions)
--> 191       return func(*args, **kwargs)
192     new_func.doc = _add_deprecated_arg_notice_to_docstring(
193         func.doc, date, instructions)
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
353                              steps=steps,
354                              monitors=monitors,
--> 355                              max_steps=max_steps)
356     logging.info('Loss for final step: %s.', loss)
357     return self
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
697       # cases, but will soon be deleted after the subclasses are updated.
698       # TODO(b/32664904): Update subclasses and delete the else-statement.
--> 699       train_ops = self._get_train_ops(features, labels)
700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature
701         train_op = train_ops.train_op
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
1050       ModelFnOps object.
1051     """"""
-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
1053
1054   def _get_eval_ops(self, features, labels, metrics):
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
1019                                           params=self.params)
1020       else:
-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)
1022     else:
1023       model_fn_results = self._model_fn(features, labels)
 in conv_model(feature, target, mode)
10                                     activation_fn=tf.nn.relu)
11
---> 12         h_pool1 = max_pool_2x2(h_conv1)
13
14     with tf.variable_scope('conv_layer2'):
 in max_pool_2x2(tensor_in)
1 def max_pool_2x2(tensor_in):
----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)
1615                                 padding=padding,
1616                                 data_format=data_format,
-> 1617                                 name=name)
1618
1619
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)
1596   result = _op_def_lib.apply_op(""MaxPool"", input=input, ksize=ksize,
1597                                 strides=strides, padding=padding,
-> 1598                                 data_format=data_format, name=name)
1599   return result
1600
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
580             for base_type in base_types:
581               _SatisfiesTypeConstraint(base_type,
--> 582                                        _Attr(op_def, input_arg.type_attr))
583             attrs[input_arg.type_attr] = attr_value
584             inferred_from[input_arg.type_attr] = input_name
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)
58           ""DataType %s for attr '%s' not in list of allowed values: %s"" %
59           (dtypes.as_dtype(dtype).name, attr_def.name,
---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
61
62
TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16
Many thanks.",3
,2545,145,65,8586,"I upgraded my server from 1.0.0 (ubuntu):
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl
But I get the following discrepancy:
(tensorflow)$ pip list | grep tensorflow
tensorflow (1.0.0)
(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'
1.0.1",TensorFlow upgrade to 1.0.1,"TensorFlow upgrade to 1.0.1I upgraded my server from 1.0.0 (ubuntu):
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl
But I get the following discrepancy:
(tensorflow)$ pip list | grep tensorflow
tensorflow (1.0.0)
(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'
1.0.1",3
,2546,145,66,15323,"@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.
Thank you!",Beam Search Decoder API,"Beam Search Decoder API@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.
Thank you!",3
,2547,148,67,16400,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:
TensorFlow installed from (source or binary) binary:
**TensorFlow version (use command below) 1.5.0rc0 **:
Python version  3.5.1:
Bazel version (if compiling from source) NOT USED:
GCC/Compiler version (if compiling from source) NOT USED:
CUDA/cuDNN version NOT USED:
**GPU model and memory NOT USED **:
Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:

Describe the problem

Link to to ""How to Use t-SNE Effectively"" is broken.
The page link is follows (before junmping)

https://www.tensorflow.org/programmers_guide/embedding
404 page is following URL

https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/




Original page should be follows. (the URL in embedding.md should rewrite to follows)

https://distill.pub/2016/misread-tsne/



Source code / logs

The problem code is follows.

https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123","[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken","[doc] link to ""How to Use t-SNE Effectively"" from embeddings is brokenSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:
TensorFlow installed from (source or binary) binary:
**TensorFlow version (use command below) 1.5.0rc0 **:
Python version  3.5.1:
Bazel version (if compiling from source) NOT USED:
GCC/Compiler version (if compiling from source) NOT USED:
CUDA/cuDNN version NOT USED:
**GPU model and memory NOT USED **:
Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:

Describe the problem

Link to to ""How to Use t-SNE Effectively"" is broken.
The page link is follows (before junmping)

https://www.tensorflow.org/programmers_guide/embedding
404 page is following URL

https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/




Original page should be follows. (the URL in embedding.md should rewrite to follows)

https://distill.pub/2016/misread-tsne/



Source code / logs

The problem code is follows.

https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123",3
,2548,144,68,21985,"After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.

while_loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):
        us_ue = tf.concat([us, ue], axis=1)  # batch 4d
        h, state = cell(inputs=us_ue, state=state)  # batch * d

        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):
            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
        i_start = tf.argmax(alpha, axis=1)  # batch
        i_start = tf.cast(i_start, tf.int32)
        s_idx = tf.stack([idx, i_start], axis=1)
        us = tf.gather_nd(U, s_idx)  # batch 2d

        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):
            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
        i_end = tf.argmax(beta, axis=1)  # batch
        i_end = tf.cast(i_end, tf.int32)
        e_idx = tf.stack([idx, i_end], axis=1)
        ue = tf.gather_nd(U, e_idx)  # batch 2d

        p1s.write(time_step, i_start)
        p2s.write(time_step, i_end)
        alphas.write(time_step, alpha)
        betas.write(time_step, beta)
        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)

    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):
        return tf.less(time_step, 4)


    with tf.variable_scope('dynamic_pointing_decoder'):
        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
        idx = tf.range(0, tf.shape(U)[0], 1)
        s_idx = tf.stack([idx, i_start], axis=1)
        e_idx = tf.stack([idx, i_end], axis=1)
        us = tf.gather_nd(U, s_idx) #batch 2d
        ue = tf.gather_nd(U, e_idx) #batch 2d
        p1s = tf.TensorArray(tf.int32, size=4)
        p2s = tf.TensorArray(tf.int32, size=4)
        alphas = tf.TensorArray(tf.float32, size=4)
        betas = tf.TensorArray(tf.float32, size=4)
        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
        #time_step, p1s, p2s, us, ue, state
        time_step = 0
        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,
                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),
                                                                          maximum_iterations=4)
        self.p1s = tf.transpose(p1s.stack()) #batch*4
        self.p2s = tf.transpose(p2s.stack())
        print(
            ""p1s  shape : {0}"".format(np.shape(self.p1s))
        )
        self.p1 = tf.unstack(self.p1s,axis=0)[-1]
        print(""p1 shape : {0}"".format(np.shape(self.p1)))
        self.p2 = tf.unstack(self.p2s, axis=0)[-1]
        alphas = tf.unstack(alphas.stack(), axis=0)
        betas = tf.unstack(betas.stack(), axis=0)
        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))
        return alphas, betas

#no while loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
with tf.variable_scope('dynamic_pointing_decoder'):
#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)
#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])
#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)
cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
idx = tf.range(0, tf.shape(U)[0], 1)
s_idx = tf.stack([idx, i_start], axis=1)
e_idx = tf.stack([idx, i_end], axis=1)
us = tf.gather_nd(U, s_idx) #batch 2d
ue = tf.gather_nd(U, e_idx) #batch 2d
alphas, betas = [], []
state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
for time_step in range(4):
if time_step >= 1:
tf.get_variable_scope().reuse_variables()
us_ue = tf.concat([us,ue], axis=1) #batch 4d
h, state = cell(inputs=us_ue, state=state) #batch * d
            with tf.variable_scope('alpha_HMN'):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
            i_start = tf.argmax(alpha, axis=1) #batch
            i_start = tf.cast(i_start, tf.int32)
            s_idx = tf.stack([idx, i_start], axis=1)
            us = tf.gather_nd(U, s_idx) #batch 2d

            with tf.variable_scope('betas_HMN'):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
            i_end = tf.argmax(alpha, axis=1) #batch
            i_end = tf.cast(i_end, tf.int32)
            e_idx = tf.stack([idx, i_end], axis=1)
            ue = tf.gather_nd(U, e_idx) #batch 2d

            alphas.append(alpha)
            betas.append(beta)
            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):
            #    break
            #else:
            #    pre_start = i_start
            #    pre_end = i_end
        return alpha, beta

anyone can help me? Thank you very much",While loop no gradients provided for any variable,"While loop no gradients provided for any variableAfter i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.

while_loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):
        us_ue = tf.concat([us, ue], axis=1)  # batch 4d
        h, state = cell(inputs=us_ue, state=state)  # batch * d

        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):
            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
        i_start = tf.argmax(alpha, axis=1)  # batch
        i_start = tf.cast(i_start, tf.int32)
        s_idx = tf.stack([idx, i_start], axis=1)
        us = tf.gather_nd(U, s_idx)  # batch 2d

        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):
            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
        i_end = tf.argmax(beta, axis=1)  # batch
        i_end = tf.cast(i_end, tf.int32)
        e_idx = tf.stack([idx, i_end], axis=1)
        ue = tf.gather_nd(U, e_idx)  # batch 2d

        p1s.write(time_step, i_start)
        p2s.write(time_step, i_end)
        alphas.write(time_step, alpha)
        betas.write(time_step, beta)
        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)

    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):
        return tf.less(time_step, 4)


    with tf.variable_scope('dynamic_pointing_decoder'):
        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
        idx = tf.range(0, tf.shape(U)[0], 1)
        s_idx = tf.stack([idx, i_start], axis=1)
        e_idx = tf.stack([idx, i_end], axis=1)
        us = tf.gather_nd(U, s_idx) #batch 2d
        ue = tf.gather_nd(U, e_idx) #batch 2d
        p1s = tf.TensorArray(tf.int32, size=4)
        p2s = tf.TensorArray(tf.int32, size=4)
        alphas = tf.TensorArray(tf.float32, size=4)
        betas = tf.TensorArray(tf.float32, size=4)
        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
        #time_step, p1s, p2s, us, ue, state
        time_step = 0
        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,
                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),
                                                                          maximum_iterations=4)
        self.p1s = tf.transpose(p1s.stack()) #batch*4
        self.p2s = tf.transpose(p2s.stack())
        print(
            ""p1s  shape : {0}"".format(np.shape(self.p1s))
        )
        self.p1 = tf.unstack(self.p1s,axis=0)[-1]
        print(""p1 shape : {0}"".format(np.shape(self.p1)))
        self.p2 = tf.unstack(self.p2s, axis=0)[-1]
        alphas = tf.unstack(alphas.stack(), axis=0)
        betas = tf.unstack(betas.stack(), axis=0)
        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))
        return alphas, betas

#no while loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
with tf.variable_scope('dynamic_pointing_decoder'):
#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)
#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])
#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)
cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
idx = tf.range(0, tf.shape(U)[0], 1)
s_idx = tf.stack([idx, i_start], axis=1)
e_idx = tf.stack([idx, i_end], axis=1)
us = tf.gather_nd(U, s_idx) #batch 2d
ue = tf.gather_nd(U, e_idx) #batch 2d
alphas, betas = [], []
state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
for time_step in range(4):
if time_step >= 1:
tf.get_variable_scope().reuse_variables()
us_ue = tf.concat([us,ue], axis=1) #batch 4d
h, state = cell(inputs=us_ue, state=state) #batch * d
            with tf.variable_scope('alpha_HMN'):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
            i_start = tf.argmax(alpha, axis=1) #batch
            i_start = tf.cast(i_start, tf.int32)
            s_idx = tf.stack([idx, i_start], axis=1)
            us = tf.gather_nd(U, s_idx) #batch 2d

            with tf.variable_scope('betas_HMN'):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
            i_end = tf.argmax(alpha, axis=1) #batch
            i_end = tf.cast(i_end, tf.int32)
            e_idx = tf.stack([idx, i_end], axis=1)
            ue = tf.gather_nd(U, e_idx) #batch 2d

            alphas.append(alpha)
            betas.append(beta)
            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):
            #    break
            #else:
            #    pre_start = i_start
            #    pre_end = i_end
        return alpha, beta

anyone can help me? Thank you very much",3
,2549,148,69,4027,"Somehow the re2 dependency does not get build correctly.
I think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",[cmake] Build error in dependency re2,"[cmake] Build error in dependency re2Somehow the re2 dependency does not get build correctly.
I think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",3
,2550,144,70,14797,"This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:
ERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':
this rule is missing dependency declarations for the following files included by 'graph.cc':
  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'
  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'
  'tensorflow/core/platform/macros.h'
  '/tensorflow/core/platform/types.h'
  '/tensorflow/core/platform/platform.h'
  '/tensorflow/core/platform/default/integral_types.h'
  '/tensorflow/compiler/xla/executable_run_options.h'
graph.cc pretty much just does #include ""graph.h"" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.
This is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):
load(""@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(
  name = ""graph"",
  cpp_class = ""Graph"",
  graph = ""graph.pb"",
  config = ""graph.config.pb"",
)

cc_binary(
  name = ""model"",
  srcs = [""graph.cc""],
  deps = ["":graph"", ""//third_party/eigen3""],
  linkopts = [""-lpthread""]
)
I'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule,"XLA AOT tfcompile failure due to undeclared inclusions in cc_binary ruleThis happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:
ERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':
this rule is missing dependency declarations for the following files included by 'graph.cc':
  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'
  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'
  'tensorflow/core/platform/macros.h'
  '/tensorflow/core/platform/types.h'
  '/tensorflow/core/platform/platform.h'
  '/tensorflow/core/platform/default/integral_types.h'
  '/tensorflow/compiler/xla/executable_run_options.h'
graph.cc pretty much just does #include ""graph.h"" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.
This is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):
load(""@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(
  name = ""graph"",
  cpp_class = ""Graph"",
  graph = ""graph.pb"",
  config = ""graph.config.pb"",
)

cc_binary(
  name = ""model"",
  srcs = [""graph.cc""],
  deps = ["":graph"", ""//third_party/eigen3""],
  linkopts = [""-lpthread""]
)
I'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",3
,2551,148,71,7251,"Environment info
Operating System: Windows 10
Installed version of CUDA and cuDNN: 8.0, 5105
tensorflow release 0.12.1
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import tensorflow as tf
from tensorflow.python.client.timeline import Timeline

with tf.device(""/gpu:0""):
    x = tf.ones(100)
    idxs = tf.range(100)

    for _ in range(10):
        y = tf.identity(x)
        x = tf.dynamic_stitch([idxs, idxs], [x, y])
        # x = tf.gather(y, idxs)

with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
    metadata = tf.RunMetadata()
    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
             run_metadata=metadata)

timeline = Timeline(metadata.step_stats)
with open(""profile.json"", ""w"") as f:
    f.write(timeline.generate_chrome_trace_format())
The log_device_placement output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch.  This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.
Is this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",Native GPU version of `tf.dynamic_stitch`,"Native GPU version of `tf.dynamic_stitch`Environment info
Operating System: Windows 10
Installed version of CUDA and cuDNN: 8.0, 5105
tensorflow release 0.12.1
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import tensorflow as tf
from tensorflow.python.client.timeline import Timeline

with tf.device(""/gpu:0""):
    x = tf.ones(100)
    idxs = tf.range(100)

    for _ in range(10):
        y = tf.identity(x)
        x = tf.dynamic_stitch([idxs, idxs], [x, y])
        # x = tf.gather(y, idxs)

with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
    metadata = tf.RunMetadata()
    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
             run_metadata=metadata)

timeline = Timeline(metadata.step_stats)
with open(""profile.json"", ""w"") as f:
    f.write(timeline.generate_chrome_trace_format())
The log_device_placement output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch.  This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.
Is this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",3
,2552,145,72,10019,"from Tensor.java:
non-scalar DataType.STRING tensors are not supported yet
Is there a plan for adding them for the Java interface?",Java Api String tensors support,"Java Api String tensors supportfrom Tensor.java:
non-scalar DataType.STRING tensors are not supported yet
Is there a plan for adding them for the Java interface?",3
,2553,146,73,19814,"Hi, all
I want parameter distribution analysis script for pretrained models.
I do not want special script for each model, just want single program to do it.
Some person advised me to use the summary graph.
tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc
I check the code and fell that the code does not support extracting parameters from pb file.
I wrote a draft code to analyze;
https://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py
Any suggestion is welcome, and I am beginner, please explain softly.
Best,
Syouyu",How to make statistics script using summary?,"How to make statistics script using summary?Hi, all
I want parameter distribution analysis script for pretrained models.
I do not want special script for each model, just want single program to do it.
Some person advised me to use the summary graph.
tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc
I check the code and fell that the code does not support extracting parameters from pb file.
I wrote a draft code to analyze;
https://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py
Any suggestion is welcome, and I am beginner, please explain softly.
Best,
Syouyu",3
,2554,148,74,16946,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10(64bit)
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below): tensorflow 1.5.0
Python version: Python 2.7/3.6
Bazel version (if compiling from source):  bazel 0.9.0
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: No GPU model

Describe the problem
I try to build the toco that is tensorflow lite converter.
But I can not success to build. please see below for the details.
Source code / logs
C:\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco
The following error message appears.

ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
INFO: Elapsed time: 27.852s
FAILED: Build did NOT complete successfully (0 packages loaded)
currently loading: tensorflow/contrib/lite/toco


plus info.
The following message appears when I input like this in command line. (for test)
patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch


patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.

patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary

patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).
Hunk #2 succeeded at 825 (offset 169 lines).
Hunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).

I don't know how to add --binary option to script...
ref. #10435",tensorflow lite converter(toco) build error ,"tensorflow lite converter(toco) build error System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10(64bit)
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below): tensorflow 1.5.0
Python version: Python 2.7/3.6
Bazel version (if compiling from source):  bazel 0.9.0
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: No GPU model

Describe the problem
I try to build the toco that is tensorflow lite converter.
But I can not success to build. please see below for the details.
Source code / logs
C:\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco
The following error message appears.

ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
INFO: Elapsed time: 27.852s
FAILED: Build did NOT complete successfully (0 packages loaded)
currently loading: tensorflow/contrib/lite/toco


plus info.
The following message appears when I input like this in command line. (for test)
patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch


patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.

patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary

patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).
Hunk #2 succeeded at 825 (offset 169 lines).
Hunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).

I don't know how to add --binary option to script...
ref. #10435",3
,2555,144,75,7959,"I tried run cnn_mnist.py and I got the following error.
Traceback (most recent call last):
File "" cnn_mnist.py"", line 13, in 
from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
ImportError: cannot import name model_fn
cuda veriosn
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
tensorflow version
tensorflow (0.10.0)
python version
Python 2.7.12",ImportError: cannot import name model_fn,"ImportError: cannot import name model_fnI tried run cnn_mnist.py and I got the following error.
Traceback (most recent call last):
File "" cnn_mnist.py"", line 13, in 
from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
ImportError: cannot import name model_fn
cuda veriosn
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
tensorflow version
tensorflow (0.10.0)
python version
Python 2.7.12",3
,2556,143,76,1962,"Environment info
Operating System: Ubuntu 15.10
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a
If installed from binary pip package, provide:

Which pip package you installed.
The output from python -c ""import tensorflow; print(tensorflow.version)"".

0.8.0rc0
Relevant code:
Unfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:
http://pastebin.com/5QEJWqtm
After running for some time, I save a checkpoint:
saver.save(sess, checkpoint_path, global_step=step)
Which sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.
Didn't have any issues in 0.7.",Killed during checkpoint save (v0.8),"Killed during checkpoint save (v0.8)Environment info
Operating System: Ubuntu 15.10
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a
If installed from binary pip package, provide:

Which pip package you installed.
The output from python -c ""import tensorflow; print(tensorflow.version)"".

0.8.0rc0
Relevant code:
Unfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:
http://pastebin.com/5QEJWqtm
After running for some time, I save a checkpoint:
saver.save(sess, checkpoint_path, global_step=step)
Which sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.
Didn't have any issues in 0.7.",3
,2557,148,77,2062,"tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).
To illustrate:
import tensorflow as tf

a = tf.Variable(0)
incr = a.count_up_to(1)

def todo_if_true():
  with tf.control_dependencies([incr]):
    return tf.identity(a)
def todo_if_false():
  return tf.identity(a)

g = tf.cond(tf.constant(False), todo_if_true, todo_if_false)
init = tf.initialize_all_variables()

with tf.Session() as sess:
  sess.run(init)
  print(sess.run(g))

Output:
1 #But should be 0",tf.cond not working with depedencies,"tf.cond not working with depedenciestf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).
To illustrate:
import tensorflow as tf

a = tf.Variable(0)
incr = a.count_up_to(1)

def todo_if_true():
  with tf.control_dependencies([incr]):
    return tf.identity(a)
def todo_if_false():
  return tf.identity(a)

g = tf.cond(tf.constant(False), todo_if_true, todo_if_false)
init = tf.initialize_all_variables()

with tf.Session() as sess:
  sess.run(init)
  print(sess.run(g))

Output:
1 #But should be 0",3
,2558,142,78,16226,"Describe the problem
This is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.
Note, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.
https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NA
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):tensorflow/tensorflow:1.3.0 docker container
TensorFlow installed from (source or binary):docker container
TensorFlow version (use command below):1.3.0
Python version: 2.7.12
Bazel version (if compiling from source):NA
GCC/Compiler version (if compiling from source):NA
CUDA/cuDNN version:NA
GPU model and memory:NA
Exact command to reproduce:netstat",Include netstat in the tensorflow docker container,"Include netstat in the tensorflow docker containerDescribe the problem
This is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.
Note, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.
https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NA
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):tensorflow/tensorflow:1.3.0 docker container
TensorFlow installed from (source or binary):docker container
TensorFlow version (use command below):1.3.0
Python version: 2.7.12
Bazel version (if compiling from source):NA
GCC/Compiler version (if compiling from source):NA
CUDA/cuDNN version:NA
GPU model and memory:NA
Exact command to reproduce:netstat",3
,2559,146,79,8633,"The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.
https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch
Although it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by ""training from scratch"" in the same way.
Any example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",Please provide an example how to use a model trained from scratch for image classification,"Please provide an example how to use a model trained from scratch for image classificationThe following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.
https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch
Although it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by ""training from scratch"" in the same way.
Any example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",3
,2560,145,80,16584,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)
with tf.device(tf.train.replica_device_setter(...):
      model = ##create model by keras
      clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling
 clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op
Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",TensorFlow op to copy weights of Keras model,"TensorFlow op to copy weights of Keras modelI am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)
with tf.device(tf.train.replica_device_setter(...):
      model = ##create model by keras
      clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling
 clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op
Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",3
,2561,143,81,23165,"System information*

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.8.0
Python version: 2.7.12
Bazel version (if compiling from source): 0.18.0
GCC/Compiler version (if compiling from source): gcc5
CUDA/cuDNN version: 9.0/7.1.5
GPU model and memory: Jetson tx2 8GB

You can collect some of this information using our environment capture script
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the current behavior:
I have frozen a graph that contains the following operations:

tf.layers.conv2d(*args, **kwargs)
tf.layers.batch_normalization(net, **batch_norm)

I will try to provide a minimal graph that have this problem.
Describe the expected behavior:
I create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:
2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            
2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     
Aborted (core dumped)

Any guideline on how to provide a better log issue?
Thanks",TensorRT engine binding error,"TensorRT engine binding errorSystem information*

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.8.0
Python version: 2.7.12
Bazel version (if compiling from source): 0.18.0
GCC/Compiler version (if compiling from source): gcc5
CUDA/cuDNN version: 9.0/7.1.5
GPU model and memory: Jetson tx2 8GB

You can collect some of this information using our environment capture script
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the current behavior:
I have frozen a graph that contains the following operations:

tf.layers.conv2d(*args, **kwargs)
tf.layers.batch_normalization(net, **batch_norm)

I will try to provide a minimal graph that have this problem.
Describe the expected behavior:
I create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:
2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            
2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     
Aborted (core dumped)

Any guideline on how to provide a better log issue?
Thanks",3
,2562,148,82,8138,"Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).
For instance, the following lines of code run without issue:
 with graph.gradient_override_map({""nonsense"": ""more_nonsense""}): input = tf.sign(input)
A more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:
 with graph.gradient_override_map({""sign"": ""Identity""}): input = tf.sign(input)
(""sign"" should be ""Sign"").
Seems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",gradient_override_map should raise an error if passed invalid gradient names.,"gradient_override_map should raise an error if passed invalid gradient names.Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).
For instance, the following lines of code run without issue:
 with graph.gradient_override_map({""nonsense"": ""more_nonsense""}): input = tf.sign(input)
A more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:
 with graph.gradient_override_map({""sign"": ""Identity""}): input = tf.sign(input)
(""sign"" should be ""Sign"").
Seems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",3
,2563,143,83,2106,"The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.
The way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",[ distribution ] How to use multiple GPU on each replica ?,"[ distribution ] How to use multiple GPU on each replica ?The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.
The way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",3
,2564,143,84,5122,"What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing
Environment info
Operating System:
$ uname -a
Linux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$ ls -l /appl/cuda/8.0/lib64/libcud*
-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a

If installed from source, provide

The commit hash (git rev-parse HEAD) 5a5a25e
The output of bazel version

$ bazel version
Build label: 0.3.2- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 21 15:09:04 2016 (1477062544)
Build timestamp: 1477062544
Build timestamp as int: 1477062544

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Using the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op

compile example

export TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include

nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \
-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC

g++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \
cuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart


edit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.

What other attempted solutions have you tried?

I tried a non CUDA example, worked fine.
I tried a diffrent cuda kernel (square operator) also failed.
I added printf to the kernel launcher and made sure it was executed.

Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
CUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.25GiB
Free memory: 11.15GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version
not equal where =  (array([0, 1, 2, 3, 4]),)
not equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]
not equal rhs =  [6 5 4 3 2]
F.
======================================================================
FAIL: test (__main__.AddOneTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""cuda_op_test.py"", line 31, in test
    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py"", line 499, in assertAllEqual
    np.testing.assert_array_equal(a, b)
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 813, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 739, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 100.0%)
 x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)
 y: array([6, 5, 4, 3, 2])

----------------------------------------------------------------------
Ran 2 tests in 0.213s

FAILED (failures=1)


It looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",custom CUDA op example returns random values,"custom CUDA op example returns random valuesWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing
Environment info
Operating System:
$ uname -a
Linux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$ ls -l /appl/cuda/8.0/lib64/libcud*
-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a

If installed from source, provide

The commit hash (git rev-parse HEAD) 5a5a25e
The output of bazel version

$ bazel version
Build label: 0.3.2- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 21 15:09:04 2016 (1477062544)
Build timestamp: 1477062544
Build timestamp as int: 1477062544

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Using the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op

compile example

export TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include

nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \
-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC

g++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \
cuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart


edit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.

What other attempted solutions have you tried?

I tried a non CUDA example, worked fine.
I tried a diffrent cuda kernel (square operator) also failed.
I added printf to the kernel launcher and made sure it was executed.

Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
CUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.25GiB
Free memory: 11.15GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version
not equal where =  (array([0, 1, 2, 3, 4]),)
not equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]
not equal rhs =  [6 5 4 3 2]
F.
======================================================================
FAIL: test (__main__.AddOneTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""cuda_op_test.py"", line 31, in test
    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py"", line 499, in assertAllEqual
    np.testing.assert_array_equal(a, b)
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 813, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 739, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 100.0%)
 x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)
 y: array([6, 5, 4, 3, 2])

----------------------------------------------------------------------
Ran 2 tests in 0.213s

FAILED (failures=1)


It looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",3
,2565,148,85,12078,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3.0-rc2
Python version:  2.7.12
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: no
GPU model and memory: no
Exact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done

Describe the problem
The following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.
Source code / logs
mnist_softmax_device_issue.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf
import threading
import numpy as np
import json
import os
import time

FLAGS = None

INTER_OP_PARALLELISM = 76
INTRA_OP_PARALLELISM = 1
BATCH_SIZE = 100
ITERATIONS = 1000
TRAINING_THREADS = 46

threads = [None] * TRAINING_THREADS

def train_function(thread_idx, mnist, sess, train_step, x, y_, y):
  iterations = int(ITERATIONS/TRAINING_THREADS)
  for i in range(iterations):
    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

def main(_):
  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  x = tf.placeholder(tf.float32, [None, 784])
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, W) + b

  y_ = tf.placeholder(tf.float32, [None, 10])

  cross_entropy = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)

  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))
  sess.run(tf.global_variables_initializer())

  for i in range(TRAINING_THREADS):
      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])

  for thread in threads:
      thread.start()
  for thread in threads:
      thread.join()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--data_dir', type=str, default='mnist-data',
                      help='Directory for storing input data')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)


Traceback
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",Assert randomly fails when training with multiple threads,"Assert randomly fails when training with multiple threadsSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3.0-rc2
Python version:  2.7.12
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: no
GPU model and memory: no
Exact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done

Describe the problem
The following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.
Source code / logs
mnist_softmax_device_issue.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf
import threading
import numpy as np
import json
import os
import time

FLAGS = None

INTER_OP_PARALLELISM = 76
INTRA_OP_PARALLELISM = 1
BATCH_SIZE = 100
ITERATIONS = 1000
TRAINING_THREADS = 46

threads = [None] * TRAINING_THREADS

def train_function(thread_idx, mnist, sess, train_step, x, y_, y):
  iterations = int(ITERATIONS/TRAINING_THREADS)
  for i in range(iterations):
    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

def main(_):
  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  x = tf.placeholder(tf.float32, [None, 784])
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, W) + b

  y_ = tf.placeholder(tf.float32, [None, 10])

  cross_entropy = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)

  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))
  sess.run(tf.global_variables_initializer())

  for i in range(TRAINING_THREADS):
      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])

  for thread in threads:
      thread.start()
  for thread in threads:
      thread.join()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--data_dir', type=str, default='mnist-data',
                      help='Directory for storing input data')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)


Traceback
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",3
,2566,148,86,21200,"Environment
OS: Ubuntu 16.04
Tensorflow-gpu: 1.8
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(
        num_units=self.config.num_lstm_units, state_is_tuple=True)
    if self.mode == ""train"":
      lstm_cell = tf.contrib.rnn.DropoutWrapper(
          lstm_cell,
          input_keep_prob=self.config.lstm_dropout_keep_prob,
          output_keep_prob=self.config.lstm_dropout_keep_prob)

    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:
      # Feed the image embeddings to set the initial LSTM state.
      zero_state = lstm_cell.zero_state(
          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)

      # Allow the LSTM variables to be reused.
      #lstm_scope.reuse_variables()

      ........

      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=""scores"")

      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=""M"")
      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))

      lstm_input_size = 14
      zk_size = 4096

      hidden = zero_state

      for k in range(0, K+1):
          .......

          lstm_outputs, hidden = lstm_cell(f_k, hidden) 

M and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module",the weights of tf.contrib.rnn.BasicLSTMCell can't be updated,"the weights of tf.contrib.rnn.BasicLSTMCell can't be updatedEnvironment
OS: Ubuntu 16.04
Tensorflow-gpu: 1.8
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(
        num_units=self.config.num_lstm_units, state_is_tuple=True)
    if self.mode == ""train"":
      lstm_cell = tf.contrib.rnn.DropoutWrapper(
          lstm_cell,
          input_keep_prob=self.config.lstm_dropout_keep_prob,
          output_keep_prob=self.config.lstm_dropout_keep_prob)

    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:
      # Feed the image embeddings to set the initial LSTM state.
      zero_state = lstm_cell.zero_state(
          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)

      # Allow the LSTM variables to be reused.
      #lstm_scope.reuse_variables()

      ........

      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=""scores"")

      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=""M"")
      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))

      lstm_input_size = 14
      zk_size = 4096

      hidden = zero_state

      for k in range(0, K+1):
          .......

          lstm_outputs, hidden = lstm_cell(f_k, hidden) 

M and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module",3
,2567,145,87,13207,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3.0 from master branch
Python version: Python 2.7.12
Bazel version (if compiling from source): 0.5.4
CUDA/cuDNN version: null
GPU model and memory: null
Exact command to reproduce:


Fetch all external dependencies by docker image with internet access.

bazel fetch //tensorflow/tools/pip_package:build_pip_package


Complie tensorflow offline without internet access with --fetch=false.

bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package

Describe the problem
I fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.
#bazel fetch //tensorflow/tools/pip_package:build_pip_package
INFO: All external dependencies fetched successfully.

After that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error ""no such package '@xxx'"", as follows:
#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
ERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
INFO: Elapsed time: 0.349s

In fact, be sure that the package eigen_archive was existing in the container, as below:
#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive
eigen_archive

And the ps stdout of bazel process was as follows:
#ps aux | grep bazel
root        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=

So, please help for that and let me know if something wrong with my operation, thanks.",Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel,"Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazelSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3.0 from master branch
Python version: Python 2.7.12
Bazel version (if compiling from source): 0.5.4
CUDA/cuDNN version: null
GPU model and memory: null
Exact command to reproduce:


Fetch all external dependencies by docker image with internet access.

bazel fetch //tensorflow/tools/pip_package:build_pip_package


Complie tensorflow offline without internet access with --fetch=false.

bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package

Describe the problem
I fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.
#bazel fetch //tensorflow/tools/pip_package:build_pip_package
INFO: All external dependencies fetched successfully.

After that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error ""no such package '@xxx'"", as follows:
#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
ERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
INFO: Elapsed time: 0.349s

In fact, be sure that the package eigen_archive was existing in the container, as below:
#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive
eigen_archive

And the ps stdout of bazel process was as follows:
#ps aux | grep bazel
root        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=

So, please help for that and let me know if something wrong with my operation, thanks.",3
,2568,145,88,22954,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.11
Python version:3.6.6
Bazel version (if compiling from source): 0.17.2
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

Describe the problem
I can't build from source as it gives me the error
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
Perhaps because my CPU doesnt have AVX instructions set
on my CPU Supported Instructions sets	MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T
Lack of AVX don't allow me to pip install tf>1.5
My question is how to install from source without AVX instructions set?
Source code / logs

C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 74.199s, Critical Path: 2.64s
INFO: 42 processes: 42 local.
FAILED: Build did NOT complete successfully",Error Building from source on Windows / my CPU doesn't have AVX ,"Error Building from source on Windows / my CPU doesn't have AVX System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.11
Python version:3.6.6
Bazel version (if compiling from source): 0.17.2
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

Describe the problem
I can't build from source as it gives me the error
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
Perhaps because my CPU doesnt have AVX instructions set
on my CPU Supported Instructions sets	MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T
Lack of AVX don't allow me to pip install tf>1.5
My question is how to install from source without AVX instructions set?
Source code / logs

C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 74.199s, Critical Path: 2.64s
INFO: 42 processes: 42 local.
FAILED: Build did NOT complete successfully",3
,2569,145,89,21429,"Have I written custom code: No
OS Platform and Distribution: Mac
TensorFlow installed from: pip
TensorFlow version: 1.10-rc1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)
Mobile device: N/A

This is required to convert models that use custom layers or loss functions
this is the fix (will submit a PR soon):
From 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001
From: Ophir Yoktan <ophir@ziprecruiter.com>
Date: Wed, 1 Aug 2018 10:16:10 +0300
Subject: [PATCH] add support for custom_objects when loading keras model for
 conversion

---
 tensorflow/contrib/lite/python/lite.py | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py
index 2f9b9d469a2..c12617df061 100644
--- a/tensorflow/contrib/lite/python/lite.py
+++ b/tensorflow/contrib/lite/python/lite.py
@@ -274,7 +274,8 @@ def from_keras_model_file(cls,
                             model_file,
                             input_arrays=None,
                             input_shapes=None,
-                            output_arrays=None):
+                            output_arrays=None,
+                            custom_objects=None):
     """"""Creates a TocoConverter class from a tf.keras model file.
 
     Args:
@@ -293,7 +294,7 @@ def from_keras_model_file(cls,
     """"""
     _keras.backend.clear_session()
     _keras.backend.set_learning_phase(False)
-    keras_model = _keras.models.load_model(model_file)
+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)
     sess = _keras.backend.get_session()
 
     # Get input and output tensors.",TocoConvertor: converting keras models to tflite doesn't support custom objects,"TocoConvertor: converting keras models to tflite doesn't support custom objectsHave I written custom code: No
OS Platform and Distribution: Mac
TensorFlow installed from: pip
TensorFlow version: 1.10-rc1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)
Mobile device: N/A

This is required to convert models that use custom layers or loss functions
this is the fix (will submit a PR soon):
From 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001
From: Ophir Yoktan <ophir@ziprecruiter.com>
Date: Wed, 1 Aug 2018 10:16:10 +0300
Subject: [PATCH] add support for custom_objects when loading keras model for
 conversion

---
 tensorflow/contrib/lite/python/lite.py | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py
index 2f9b9d469a2..c12617df061 100644
--- a/tensorflow/contrib/lite/python/lite.py
+++ b/tensorflow/contrib/lite/python/lite.py
@@ -274,7 +274,8 @@ def from_keras_model_file(cls,
                             model_file,
                             input_arrays=None,
                             input_shapes=None,
-                            output_arrays=None):
+                            output_arrays=None,
+                            custom_objects=None):
     """"""Creates a TocoConverter class from a tf.keras model file.
 
     Args:
@@ -293,7 +294,7 @@ def from_keras_model_file(cls,
     """"""
     _keras.backend.clear_session()
     _keras.backend.set_learning_phase(False)
-    keras_model = _keras.models.load_model(model_file)
+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)
     sess = _keras.backend.get_session()
 
     # Get input and output tensors.",3
,2570,144,90,5179,"I am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a ""system"" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.
Any help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.
Thanks,
Hemal
My setup is the following:
Docker image from ubuntu:16.04 using gcc5.
Bazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)
I matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.
The following is in my Dockerfile to build and ""deploy"" Tensorflow:
RUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \
&& cd /tmp/tensorflow && git checkout r0.11 \
&& yes '' | ./configure \
&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \
&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \
&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \
&& bazel build -c opt //tensorflow:libtensorflow_cc.so \
&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party
164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command
(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && 
exec env - 
/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.so,"Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.soI am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a ""system"" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.
Any help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.
Thanks,
Hemal
My setup is the following:
Docker image from ubuntu:16.04 using gcc5.
Bazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)
I matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.
The following is in my Dockerfile to build and ""deploy"" Tensorflow:
RUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \
&& cd /tmp/tensorflow && git checkout r0.11 \
&& yes '' | ./configure \
&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \
&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \
&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \
&& bazel build -c opt //tensorflow:libtensorflow_cc.so \
&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party
164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command
(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && 
exec env - 
/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",3
,2571,148,91,2233,"I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.
when I use ""with sv.managed_session(server.target) as sess:"", it shows error:
AttributeError: 'Supervisor' object has no attribute 'managed_session'
So I follow the code of ""Inception"":
with sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :
Then it starts to run, but hangs immediately after reporting the following error:
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864
E tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800
Would you please help me on this?
Thanks a lot in advance!",protobuf message overflow on trying distributed ,"protobuf message overflow on trying distributed I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.
when I use ""with sv.managed_session(server.target) as sess:"", it shows error:
AttributeError: 'Supervisor' object has no attribute 'managed_session'
So I follow the code of ""Inception"":
with sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :
Then it starts to run, but hangs immediately after reporting the following error:
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864
E tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800
Would you please help me on this?
Thanks a lot in advance!",3
,2572,142,92,2510,"Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.
For example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say ""What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size"" or ""Oh yeah I used my other dataset instead.""
Obviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",Tensorboard feature request - Text summary,"Tensorboard feature request - Text summaryWould it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.
For example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say ""What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size"" or ""Oh yeah I used my other dataset instead.""
Obviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",3
,2573,143,93,16669,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below):  v1.5.0-0-g37aa430d84
Python version: 3.5
Bazel version (if compiling from source): 0.10.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: 9.1 / 7
GPU model and memory: TITAN Xp, 12196MiB
Exact command to reproduce: -

Describe the problem
When I enable the memory optimizer in grappler, it fails with the following errors:
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.

My network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.
Is this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.,"grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below):  v1.5.0-0-g37aa430d84
Python version: 3.5
Bazel version (if compiling from source): 0.10.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: 9.1 / 7
GPU model and memory: TITAN Xp, 12196MiB
Exact command to reproduce: -

Describe the problem
When I enable the memory optimizer in grappler, it fails with the following errors:
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.

My network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.
Is this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",3
,2574,148,94,13458,"I want to limit the total memory of each GPU in mnist,
https://www.tensorflow.org/tutorials/using_gpu
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)
and I added the above code to the mnis.py
https://github.com/tensorflow/models/tree/master/official/mnist
here is the modified code in mnis.py :
def main(unused_argv):
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
mnist_classifier = tf.estimator.Estimator(
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
but I get the below error:
Traceback (most recent call last):
File ""mnist.py"", line 231, in 
tf.app.run()
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""mnist.py"", line 206, in main
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 142, in init
config)
ValueError: config must be an instance of RunConfig, but provided gpu_options {
per_process_gpu_memory_fraction: 0.4
}
.
My question is :
How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config),"How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)I want to limit the total memory of each GPU in mnist,
https://www.tensorflow.org/tutorials/using_gpu
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)
and I added the above code to the mnis.py
https://github.com/tensorflow/models/tree/master/official/mnist
here is the modified code in mnis.py :
def main(unused_argv):
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
mnist_classifier = tf.estimator.Estimator(
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
but I get the below error:
Traceback (most recent call last):
File ""mnist.py"", line 231, in 
tf.app.run()
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""mnist.py"", line 206, in main
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 142, in init
config)
ValueError: config must be an instance of RunConfig, but provided gpu_options {
per_process_gpu_memory_fraction: 0.4
}
.
My question is :
How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",3
,2575,141,95,6314,"The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows
I'm getting an HTTP Error 404.
I found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",Error 404 when downloading Tensorflow on Windows,"Error 404 when downloading Tensorflow on WindowsThe links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows
I'm getting an HTTP Error 404.
I found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",3
,2576,142,96,4474,"Hello there,
Feature
I consider a lot but finally decide put this request here.
I know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().
But what I want is extract the patches given several locations.
Input Tensor:
         batch image: [batch_size, image_height, image_width, image_channel]
         batch locations: [batch_size, num_patches, 2]
Output Tensor:
        [batch_size, num_patches, patch_height, patch_width, image_channel]
or 
        [batch_size * num_patches, patch_height, patch_width, image_channel]
Reference:
Georgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.
https://github.com/trigeorgis/tensorflow

But when I use it in v0.10.0, it requires me to define a shape function.
I really want to use it in the future, so I am glad it can be added as a new feature.
Other solution I tried
I have tried use extract_glimpse() instead.
output_list = [[] for _ in range(batch_size)]   # create n_patch list
locations = locations/image_size  # normalize the location
for j in range(n_patch):
    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)
    for i in range(batch_size):
          output_list[i].append(patch_one[j])  # add tensor to each list
patches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]",Feature request: Patch extracting given location implementation needed.,"Feature request: Patch extracting given location implementation needed.Hello there,
Feature
I consider a lot but finally decide put this request here.
I know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().
But what I want is extract the patches given several locations.
Input Tensor:
         batch image: [batch_size, image_height, image_width, image_channel]
         batch locations: [batch_size, num_patches, 2]
Output Tensor:
        [batch_size, num_patches, patch_height, patch_width, image_channel]
or 
        [batch_size * num_patches, patch_height, patch_width, image_channel]
Reference:
Georgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.
https://github.com/trigeorgis/tensorflow

But when I use it in v0.10.0, it requires me to define a shape function.
I really want to use it in the future, so I am glad it can be added as a new feature.
Other solution I tried
I have tried use extract_glimpse() instead.
output_list = [[] for _ in range(batch_size)]   # create n_patch list
locations = locations/image_size  # normalize the location
for j in range(n_patch):
    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)
    for i in range(batch_size):
          output_list[i].append(patch_one[j])  # add tensor to each list
patches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]",3
,2577,146,97,4675,"tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit
effectively returns
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit
Estimator.fit calls:
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model
which has these lines:
      features, targets = input_fn()
      self._check_inputs(features, targets)
      train_op, loss_op = self._get_train_ops(features, targets)

tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs
    if self._features_info is not None:
      ...
    else:
      self._features_info = tensor_signature.create_signatures(features)

So we get to a point where features, as derived from the input_fn, is treated as our feature columns set.
In pseudo code:
def input_function()
    return [foo, bar, baz], quux

lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
lc.fit(input_function) # run fit on out input function
lc._feature_columns # repr _feature_columns on the instantiated classifier
    [foo]
lc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator
    [foo, bar, baz]

The issue is:
Although this line appears to indicate that we will be making an estimation based on the feature columns supplied:
lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
What happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.
Am I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?
That when we instantiate the classifier we are setting the feature_columns we expect to be used?
The work around for this is simply to only return the columns you need from your input function however I found this misleading.
Point in the tutorial:
Either the code or the tutorial need to be changed.
https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model
Error raised:
WARNING:tensorflow:Setting feature info to
as per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",LinearClassifier feature_columns overwritten in LinearClassifier.fit,"LinearClassifier feature_columns overwritten in LinearClassifier.fittensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit
effectively returns
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit
Estimator.fit calls:
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model
which has these lines:
      features, targets = input_fn()
      self._check_inputs(features, targets)
      train_op, loss_op = self._get_train_ops(features, targets)

tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs
    if self._features_info is not None:
      ...
    else:
      self._features_info = tensor_signature.create_signatures(features)

So we get to a point where features, as derived from the input_fn, is treated as our feature columns set.
In pseudo code:
def input_function()
    return [foo, bar, baz], quux

lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
lc.fit(input_function) # run fit on out input function
lc._feature_columns # repr _feature_columns on the instantiated classifier
    [foo]
lc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator
    [foo, bar, baz]

The issue is:
Although this line appears to indicate that we will be making an estimation based on the feature columns supplied:
lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
What happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.
Am I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?
That when we instantiate the classifier we are setting the feature_columns we expect to be used?
The work around for this is simply to only return the columns you need from your input function however I found this misleading.
Point in the tutorial:
Either the code or the tutorial need to be changed.
https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model
Error raised:
WARNING:tensorflow:Setting feature info to
as per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",3
,2578,145,98,23344,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0
Python version: 3.6.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0.176/7.1.3
GPU model and memory: NVIDIA Quadro M2200

Describe the current behavior
TensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.
This throws the InvalidArgumentError as seen below.
The problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error.  The only related (closed) issue I could find: #19497 ""NHWC convolution sometimes incorrectly considered NCHW""
 InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=""NCHW"", ksize=[1, 1, 3, 3], padding=""SAME"", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container="""", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Describe the expected behavior
To not throw the error as was the case for TensorFlow 1.8.
Code to reproduce the issue
import tensorflow as tf
import numpy as np

dataset_np = {'height_or_width': 512, 
              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),
              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}
dataset = tf.data.Dataset.from_tensors(dataset_np)

def densify(element):
    label_image_sparse = tf.SparseTensor(indices = element['indices'], 
                                         values = element['values'], 
                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],
                                                                         element['height_or_width']]),tf.int64))
    label_image = tf.sparse_tensor_to_dense(label_image_sparse, 
                                            default_value=-1, 
                                            validate_indices=False, 
                                            name='label_image')
    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], 
                                                     data_format=""NHWC"", 
                                                     ksize= [1,3,3,1], 
                                                     strides = [1,1,1,1], 
                                                     padding='SAME', 
                                                     name='label_image_dilated'),[0,-1])
    return {'label_image_dilated':label_image_dilated}

dataset = dataset.map(densify)
element = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    result = sess.run(element)
    print(result)
Other info / logs",LayoutOptimizer optimizes to unsupported data_format for max_pool on CPU,"LayoutOptimizer optimizes to unsupported data_format for max_pool on CPUSystem information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0
Python version: 3.6.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0.176/7.1.3
GPU model and memory: NVIDIA Quadro M2200

Describe the current behavior
TensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.
This throws the InvalidArgumentError as seen below.
The problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error.  The only related (closed) issue I could find: #19497 ""NHWC convolution sometimes incorrectly considered NCHW""
 InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=""NCHW"", ksize=[1, 1, 3, 3], padding=""SAME"", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container="""", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Describe the expected behavior
To not throw the error as was the case for TensorFlow 1.8.
Code to reproduce the issue
import tensorflow as tf
import numpy as np

dataset_np = {'height_or_width': 512, 
              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),
              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}
dataset = tf.data.Dataset.from_tensors(dataset_np)

def densify(element):
    label_image_sparse = tf.SparseTensor(indices = element['indices'], 
                                         values = element['values'], 
                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],
                                                                         element['height_or_width']]),tf.int64))
    label_image = tf.sparse_tensor_to_dense(label_image_sparse, 
                                            default_value=-1, 
                                            validate_indices=False, 
                                            name='label_image')
    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], 
                                                     data_format=""NHWC"", 
                                                     ksize= [1,3,3,1], 
                                                     strides = [1,1,1,1], 
                                                     padding='SAME', 
                                                     name='label_image_dilated'),[0,-1])
    return {'label_image_dilated':label_image_dilated}

dataset = dataset.map(densify)
element = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    result = sess.run(element)
    print(result)
Other info / logs",3
,2579,148,99,15103,"I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:
It seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

with tf.device('/gpu:0'):
  g = tf.spectral.rfft(tf.ones(64))
  
  tf.exp(g)

which results in
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
 [Op:Exp]

a more practical example that would lead to this same error:
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)

frame_length=256
frame_step=64
n_mels = 64
sr=16000
filename = 'path/to/a.wav'

some_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), 
                                     file_format='wav', 
                                     samples_per_second=16000, 
                                     channel_count=1)

with tf.device('/gpu:0'):
  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, 
                                  frame_step=frame_step, fft_length=frame_length)

  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(
      n_mels, 1+frame_length//2, sr)

  magnitude_spectrograms = tf.abs(stft)
  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,
                                           linear_to_mel_weight_matrix, 
                                           axes = [[2], [0]]))

  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)

Keeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",No GPU OpKernel for tf.exp() operation for Complex64,"No GPU OpKernel for tf.exp() operation for Complex64I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:
It seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

with tf.device('/gpu:0'):
  g = tf.spectral.rfft(tf.ones(64))
  
  tf.exp(g)

which results in
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
 [Op:Exp]

a more practical example that would lead to this same error:
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)

frame_length=256
frame_step=64
n_mels = 64
sr=16000
filename = 'path/to/a.wav'

some_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), 
                                     file_format='wav', 
                                     samples_per_second=16000, 
                                     channel_count=1)

with tf.device('/gpu:0'):
  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, 
                                  frame_step=frame_step, fft_length=frame_length)

  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(
      n_mels, 1+frame_length//2, sr)

  magnitude_spectrograms = tf.abs(stft)
  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,
                                           linear_to_mel_weight_matrix, 
                                           axes = [[2], [0]]))

  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)

Keeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",3
,2580,145,0,13565,"From @eparis's comment in the Cinder PR: #13367 (comment)
PR #13367, Cinder Volume Plugin introduced a dependency on

github.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes
github.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach

which in turn has a dependency on github.com/golang/go/src/testing
which declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).
Which means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binaries,"Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binariesFrom @eparis's comment in the Cinder PR: #13367 (comment)
PR #13367, Cinder Volume Plugin introduced a dependency on

github.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes
github.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach

which in turn has a dependency on github.com/golang/go/src/testing
which declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).
Which means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",3
,2581,145,1,28318,"The steps to reproduce the issue, on a working gce cluster

Choose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)
Create a pod and make sure it is assigned to the node you chose in step 1
kill kubelet process (kubelet process will be restarted automatically after > 5mins )
Check pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)",Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserver,"Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserverThe steps to reproduce the issue, on a working gce cluster

Choose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)
Create a pod and make sure it is assigned to the node you chose in step 1
kill kubelet process (kubelet process will be restarted automatically after > 5mins )
Check pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)",3
,2582,145,2,4128,"This is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:

create networking bridge called cbr0
config docker run with ""--bridge cbr0 --iptables=false""
add default route for master node (on GCE). Need to figure out other cloud providers.",Configure master node same as slave node,"Configure master node same as slave nodeThis is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:

create networking bridge called cbr0
config docker run with ""--bridge cbr0 --iptables=false""
add default route for master node (on GCE). Need to figure out other cloud providers.",3
,2583,141,3,23062,"The software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew.  Please add an alert.
Also, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).","Alert someone when kubernetes-test-history stops running, and add report date","Alert someone when kubernetes-test-history stops running, and add report dateThe software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew.  Please add an alert.
Also, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).",3
,2584,148,4,132,"Hi,
I'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.
I've read the design doc, shell scripts and salt states and this is how I deployed it:

kubelet -config /etc/kubelet.conf -address=0.0.0.0
etcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/
apiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115
controller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001

Kubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):
2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}
2014/06/17 13:01:12 Existing:
[]string{} Desired: map[string]bool{}

apiserver prints nothing at all.
controller-manger logs:
etcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=false&sorted=false]
etcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false  | method  GET]
etcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
etcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
etcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]
etcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]
2014/06/17 13:13:17 Synchronization error &etcd.EtcdError{ErrorCode:100, Message:""Key not found"", Cause:""/registry"", Index:0x2}

Now running cloudcfg fails:
./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx
2014/06/17 15:16:03 Error: &errors.errorString{s:""request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found""}

If I try to create a pod I get:
./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods
2014/06/17 15:17:14 Failed to print: &json.SyntaxError{msg:""invalid character 'N' looking for beginning of value"", Offset:1}",Add generic documentation,"Add generic documentationHi,
I'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.
I've read the design doc, shell scripts and salt states and this is how I deployed it:

kubelet -config /etc/kubelet.conf -address=0.0.0.0
etcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/
apiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115
controller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001

Kubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):
2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}
2014/06/17 13:01:12 Existing:
[]string{} Desired: map[string]bool{}

apiserver prints nothing at all.
controller-manger logs:
etcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=false&sorted=false]
etcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false  | method  GET]
etcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]
etcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
etcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
etcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]
etcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]
2014/06/17 13:13:17 Synchronization error &etcd.EtcdError{ErrorCode:100, Message:""Key not found"", Cause:""/registry"", Index:0x2}

Now running cloudcfg fails:
./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx
2014/06/17 15:16:03 Error: &errors.errorString{s:""request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found""}

If I try to create a pod I get:
./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods
2014/06/17 15:17:14 Failed to print: &json.SyntaxError{msg:""invalid character 'N' looking for beginning of value"", Offset:1}",3
,2585,148,5,14281,"Per a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.
Right now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",`rolling-update` requires a unique name when specifying a manifest,"`rolling-update` requires a unique name when specifying a manifestPer a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.
Right now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",3
,2586,144,6,15116,"This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?
For example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):

Kubernetes e2e suite.Services should check NodePort out-of-range

Expected
    <string>: Service ""nodeport-range-test"" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range
to equal
    <string>: Service ""nodeport-range-test"" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range


Kubernetes e2e suite.Services should prevent NodePort collisions

Expected
    <string>: Service ""nodeport-collision2"" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated
to equal
    <string>: Service ""nodeport-collision2"" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated

@ixdy @quinton-hoole Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.
For now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",What do we do with 1.0 tests that fail when run against 1.1?,"What do we do with 1.0 tests that fail when run against 1.1?This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?
For example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):

Kubernetes e2e suite.Services should check NodePort out-of-range

Expected
    <string>: Service ""nodeport-range-test"" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range
to equal
    <string>: Service ""nodeport-range-test"" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range


Kubernetes e2e suite.Services should prevent NodePort collisions

Expected
    <string>: Service ""nodeport-collision2"" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated
to equal
    <string>: Service ""nodeport-collision2"" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated

@ixdy @quinton-hoole Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.
For now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",3
,2587,144,7,14237,"Setting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.
In the tutorial we set up the nginx service with a nodePort like so
kubectl expose rc my-nginx --target-port=80 --type=NodePort

Of all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to
curl (node external ip):(node_port)

was the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.
Not sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses
Next, create a Container Engine service which exposes this nginx Pod on each node in your cluster:",GKE cluster running nginx seems to break nodePort,"GKE cluster running nginx seems to break nodePortSetting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.
In the tutorial we set up the nginx service with a nodePort like so
kubectl expose rc my-nginx --target-port=80 --type=NodePort

Of all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to
curl (node external ip):(node_port)

was the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.
Not sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses
Next, create a Container Engine service which exposes this nginx Pod on each node in your cluster:",3
,2588,148,8,10774,"Please add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.
The SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.",Docker k8s setup instructions to have DNS option,"Docker k8s setup instructions to have DNS optionPlease add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.
The SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.",3
,2589,142,9,11732,"The golang docker image is existed
$ docker images docker.io/golang
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
docker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB

But it always say ""You don't have a local copy of the golang docker image"".
$ sudo make release
build/release.sh
+++ [0723 10:58:17] Verifying Prerequisites....
You don't have a local copy of the golang docker image. This image is 450MB.
Download it now? [y/n] n

#11284 fixes this.",Check local copy of the golang docker image is always failed.,"Check local copy of the golang docker image is always failed.The golang docker image is existed
$ docker images docker.io/golang
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
docker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB

But it always say ""You don't have a local copy of the golang docker image"".
$ sudo make release
build/release.sh
+++ [0723 10:58:17] Verifying Prerequisites....
You don't have a local copy of the golang docker image. This image is 450MB.
Download it now? [y/n] n

#11284 fixes this.",3
,2590,146,10,15412,"error: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout

https://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614
First time I've seen this. Documenting for searchability.",[mesos/docker] Flakey Smoke Test - TLS handshake timeout,"[mesos/docker] Flakey Smoke Test - TLS handshake timeouterror: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout

https://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614
First time I've seen this. Documenting for searchability.",3
,2591,142,11,4695,"Should it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.
Here are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash
And here is what lead me to investigate:
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service ""redis"": stopping it
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service ""redis""
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service ""redis"" at 119.9.235.183:6379/TCP (local :0)
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service ""redis"" on TCP port 37841
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection

The Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).
The thing of note in the logs is that in the ""Setting services"" message both redis instances are present where-as in the ""Recieved update"" message, only the project2 instance is present.
I believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214
Assuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",Services with the same name in different namespaces cause kube-proxy to bug out,"Services with the same name in different namespaces cause kube-proxy to bug outShould it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.
Here are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash
And here is what lead me to investigate:
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]

Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service ""redis"": stopping it
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service ""redis""
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service ""redis"" at 119.9.235.183:6379/TCP (local :0)
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service ""redis"" on TCP port 37841
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection
Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection

The Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).
The thing of note in the logs is that in the ""Setting services"" message both redis instances are present where-as in the ""Recieved update"" message, only the project2 instance is present.
I believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214
Assuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",3
,2592,146,12,7047,"If you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do ""create or update"" in a single command.",Want create-or-update command for kubectl,"Want create-or-update command for kubectlIf you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do ""create or update"" in a single command.",3
,2593,148,13,9937,"Example usage (details to be decided):
$ manifest-query -f mypod.yaml  -t {{.metadata.name}}

golang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)
The tool will be useful for:

bash scripts that have to manipulate yamls (e.g. for fixing #9849)
examples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy

cc. @zmerlynn",Create a command-line tool to query yaml and json files,"Create a command-line tool to query yaml and json filesExample usage (details to be decided):
$ manifest-query -f mypod.yaml  -t {{.metadata.name}}

golang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)
The tool will be useful for:

bash scripts that have to manipulate yamls (e.g. for fixing #9849)
examples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy

cc. @zmerlynn",3
,2594,148,14,9459,"Hello,
I was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:
==> master: Installing, enabling prerequisites
==> master:
==> master:
==> master:  One of the configured repositories failed (Fedora 21 - x86_64),
==> master:  and yum doesn't have enough cached data to continue. At this point the only
==> master:  safe thing yum can do is fail. There are a few ways to work ""fix"" this:
==> master:
==> master:      1. Contact the upstream for the repository and get them to fix the problem.
==> master:
==> master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working
==> master:         upstream. This is most often useful if you are using a newer
==> master:         distribution release than is supported by the repository (and the
==> master:         packages for the previous distribution release still work).
==> master:
==> master:      3. Disable the repository, so yum won't use it by default. Yum will then
==> master:         just ignore the repository until you permanently enable it again or use
==> master:         --enablerepo for temporary usage:
==> master:
==> master:             yum-config-manager --disable fedora
==> master:
==> master:      4. Configure the failing repository to be skipped, if it is unavailable.
==> master:         Note that yum will try to contact the repo. when it runs most commands,
==> master:         so will have to try and fail each time (and thus. yum will be be much
==> master:         slower). If it is a very temporary problem though, this is often a nice
==> master:         compromise:
==> master:
==> master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true
==> master:
==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again
Any idea?",Vagrant:Fedora 21 repo failing,"Vagrant:Fedora 21 repo failingHello,
I was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:
==> master: Installing, enabling prerequisites
==> master:
==> master:
==> master:  One of the configured repositories failed (Fedora 21 - x86_64),
==> master:  and yum doesn't have enough cached data to continue. At this point the only
==> master:  safe thing yum can do is fail. There are a few ways to work ""fix"" this:
==> master:
==> master:      1. Contact the upstream for the repository and get them to fix the problem.
==> master:
==> master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working
==> master:         upstream. This is most often useful if you are using a newer
==> master:         distribution release than is supported by the repository (and the
==> master:         packages for the previous distribution release still work).
==> master:
==> master:      3. Disable the repository, so yum won't use it by default. Yum will then
==> master:         just ignore the repository until you permanently enable it again or use
==> master:         --enablerepo for temporary usage:
==> master:
==> master:             yum-config-manager --disable fedora
==> master:
==> master:      4. Configure the failing repository to be skipped, if it is unavailable.
==> master:         Note that yum will try to contact the repo. when it runs most commands,
==> master:         so will have to try and fail each time (and thus. yum will be be much
==> master:         slower). If it is a very temporary problem though, this is often a nice
==> master:         compromise:
==> master:
==> master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true
==> master:
==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again
Any idea?",3
,2595,148,15,14528,"I created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.
This is not expected.
The running pods are on nodes that have existed for 3 or 4 days.
The pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",Too many daemons created by DaemonSet,"Too many daemons created by DaemonSetI created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.
This is not expected.
The running pods are on nodes that have existed for 3 or 4 days.
The pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",3
,2596,148,16,14743,"I'm getting an error while running the cluster/kube-up.sh with
export KUBERNETES_PROVIDER=vagrant
export VAGRANT_DEFAULT_PROVIDER=virtualbox
export NUM_MINIONS=1

error:
==> master: /srv/salt-overlay/salt/kube-apiserver/basic_auth.csv -> /srv/salt-new/salt/kube-apiserver/basic_auth.csv
==> master: /srv/salt-overlay/pillar/cluster-params.sls -> /srv/salt-new/pillar/cluster-params.sls
==> master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz
==> master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes
==> master: tar: Exiting with failure status due to previous errors
The SSH command responded with a non-zero exit status. Vagrant
assumes that this means the command failed. The output for this command
should be in the log above. Please read the output to determine what
went wrong.

am I missing something?",kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes,"kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytesI'm getting an error while running the cluster/kube-up.sh with
export KUBERNETES_PROVIDER=vagrant
export VAGRANT_DEFAULT_PROVIDER=virtualbox
export NUM_MINIONS=1

error:
==> master: /srv/salt-overlay/salt/kube-apiserver/basic_auth.csv -> /srv/salt-new/salt/kube-apiserver/basic_auth.csv
==> master: /srv/salt-overlay/pillar/cluster-params.sls -> /srv/salt-new/pillar/cluster-params.sls
==> master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz
==> master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes
==> master: tar: Exiting with failure status due to previous errors
The SSH command responded with a non-zero exit status. Vagrant
assumes that this means the command failed. The output for this command
should be in the log above. Please read the output to determine what
went wrong.

am I missing something?",3
,2597,146,17,21635,"I may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a ""Click here to submit feedback"", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!
E.g.
From here:
http://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service
From here:
http://kubernetes.io/v1.1/examples/https-nginx/README.html
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md
Recommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.
Alternatively, do you just accept kubernetes.io references in github issues?",Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetes,"Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetesI may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a ""Click here to submit feedback"", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!
E.g.
From here:
http://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service
From here:
http://kubernetes.io/v1.1/examples/https-nginx/README.html
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md
Recommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.
Alternatively, do you just accept kubernetes.io references in github issues?",3
,2598,145,18,17600,http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/,Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance],Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance]http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/,3
,2599,148,19,8943,"Without --api-prefix, kubectl proxy works just fine,
$ curl localhost:8001/api/
{
  ""versions"": [
    ""v1beta1"",
    ""v1beta2"",
    ""v1beta3""
  ]
}

With '--api-prefix=xxx-api', it stops work:
$ curl localhost:8001/xxx-api/
404 page not found

My kubectl version:
$ kubectl version
Client Version: version.Info{Major:""0"", Minor:""17+"", GitVersion:""v0.17.1-878-g851f6b754241c0-dirty"", GitCommit:""851f6b754241c01f30c43cddd2f9ca8c7c3f42f5"", GitTreeState:""dirty""}
Server Version: version.Info{Major:""0"", Minor:""17+"", GitVersion:""v0.17.1-893-g58b683fe296da8-dirty"", GitCommit:""58b683fe296da870fdbc6eb1a32dcf50ed94a8e3"", GitTreeState:""dirty""}

@jlowdermilk, could you take a look? Thanks.",'--api-prefix' option in 'kubectl proxy' documentation is wrong,"'--api-prefix' option in 'kubectl proxy' documentation is wrongWithout --api-prefix, kubectl proxy works just fine,
$ curl localhost:8001/api/
{
  ""versions"": [
    ""v1beta1"",
    ""v1beta2"",
    ""v1beta3""
  ]
}

With '--api-prefix=xxx-api', it stops work:
$ curl localhost:8001/xxx-api/
404 page not found

My kubectl version:
$ kubectl version
Client Version: version.Info{Major:""0"", Minor:""17+"", GitVersion:""v0.17.1-878-g851f6b754241c0-dirty"", GitCommit:""851f6b754241c01f30c43cddd2f9ca8c7c3f42f5"", GitTreeState:""dirty""}
Server Version: version.Info{Major:""0"", Minor:""17+"", GitVersion:""v0.17.1-893-g58b683fe296da8-dirty"", GitCommit:""58b683fe296da870fdbc6eb1a32dcf50ed94a8e3"", GitTreeState:""dirty""}

@jlowdermilk, could you take a look? Thanks.",3
,2600,148,20,21330,"We provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.
If we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",AllowUnconditionalUpdate is very frightening,"AllowUnconditionalUpdate is very frighteningWe provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.
If we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",3
,2601,148,21,24139,"I've enabled kubectl bash completion as per the instructions:
source ./contrib/completions/bash/kubectl
Now autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.

kubectl create -f ./ser
Tab to autocomplete directory name
kubectl create -f ./ser-bash: _filedir: command not found

I am using OSX.",Bash completion fails to autocomplete files or directories,"Bash completion fails to autocomplete files or directoriesI've enabled kubectl bash completion as per the instructions:
source ./contrib/completions/bash/kubectl
Now autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.

kubectl create -f ./ser
Tab to autocomplete directory name
kubectl create -f ./ser-bash: _filedir: command not found

I am using OSX.",3
,2602,141,22,30338,"I believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz
A few challenges with that:

It's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)
We should probably bundle it instead with the k8s version with which that k8s version is tested
I'm not entirely sure how this tar file was built

It would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.",Document / rationalize CNI plugin distribution,"Document / rationalize CNI plugin distributionI believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz
A few challenges with that:

It's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)
We should probably bundle it instead with the k8s version with which that k8s version is tested
I'm not entirely sure how this tar file was built

It would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.",3
,2603,148,23,24534,"It seems like v1.3.0-alpha.2 binaries are not pushed.
$ curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl
HTTP/1.1 404 Not Found
X-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ
Content-Type: application/xml; charset=UTF-8
Content-Length: 127
Date: Wed, 20 Apr 2016 16:01:18 GMT
Expires: Wed, 20 Apr 2016 16:01:18 GMT
Cache-Control: private, max-age=0
Server: UploadServer
Alternate-Protocol: 443:quic
Alt-Svc: quic="":443""; ma=2592000; v=""32,31,30,29,28,27,26,25""
@david-mcmahon @bgrant0607 @ixdy",v1.3.0-alpha.2 binaries not pushed,"v1.3.0-alpha.2 binaries not pushedIt seems like v1.3.0-alpha.2 binaries are not pushed.
$ curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl
HTTP/1.1 404 Not Found
X-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ
Content-Type: application/xml; charset=UTF-8
Content-Length: 127
Date: Wed, 20 Apr 2016 16:01:18 GMT
Expires: Wed, 20 Apr 2016 16:01:18 GMT
Cache-Control: private, max-age=0
Server: UploadServer
Alternate-Protocol: 443:quic
Alt-Svc: quic="":443""; ma=2592000; v=""32,31,30,29,28,27,26,25""
@david-mcmahon @bgrant0607 @ixdy",3
,2604,146,24,3338,"e.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen
The primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",Investigate alternative JSON parsers,"Investigate alternative JSON parserse.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen
The primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",3
,2605,144,25,32237,"https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/
Failed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
Sep  7 14:58:43.497: Couldn't delete ns: ""e2e-tests-thirdparty-dn7z5"": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:""company.com"", Version:""v1""}:(*errors.StatusError)(0xc820cece80)}})
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338",ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite},"ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/
Failed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134
Sep  7 14:58:43.497: Couldn't delete ns: ""e2e-tests-thirdparty-dn7z5"": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:""company.com"", Version:""v1""}:(*errors.StatusError)(0xc820cece80)}})
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338",3
,2606,148,26,7965,"The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:

there are the debug handlers /exec/ and /run/ that run code in any container on the host
these debug handlers are enabled by default
the code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)",Secure kubelet port 10250,"Secure kubelet port 10250The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:

there are the debug handlers /exec/ and /run/ that run code in any container on the host
these debug handlers are enabled by default
the code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)",3
,2607,143,27,33289,"In rescheduler e2e test, for It(""should ensure that critical pod is scheduled in case there is no resources available""), kube-dns is used as the critical pod target and being scaled out and in.
However, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.
So probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.
@piosz  @thockin",Rescheduler e2e should not scale kube-dns pods.,"Rescheduler e2e should not scale kube-dns pods.In rescheduler e2e test, for It(""should ensure that critical pod is scheduled in case there is no resources available""), kube-dns is used as the critical pod target and being scaled out and in.
However, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.
So probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.
@piosz  @thockin",3
,2608,143,28,7989,"In my last PR @brendanburns duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.

lets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places.    There are also other examples of this online, (i.e.  https://code.google.com/p/go-wiki/wiki/Timeouts)
while we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created",E2E: Audit density.go iterator and other timeout iterators,"E2E: Audit density.go iterator and other timeout iteratorsIn my last PR @brendanburns duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.

lets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places.    There are also other examples of this online, (i.e.  https://code.google.com/p/go-wiki/wiki/Timeouts)
while we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created",3
,2609,145,29,30069,"The Google Cloud API client libraries for Go are making some breaking changes:

The import paths are changing from google.golang.org/cloud/... to
cloud.google.com/go/.... For example, if your code imports the BigQuery client
it currently reads
import ""google.golang.org/cloud/bigquery""
It should be changed to
import ""cloud.google.com/go/bigquery""
Client options are also moving, from google.golang.org/cloud to
google.golang.org/api/option. Two have also been renamed:

WithBaseGRPC is now WithGRPCConn
WithBaseHTTP is now WithHTTPClient


The cloud.WithContext and cloud.NewContext methods are gone, as are the
deprecated pubsub and container functions that required them. Use the Client
methods of these packages instead.

You should make these changes before September 12, 2016, when the packages at
google.golang.org/cloud will go away.",update Google Cloud API client import paths and more,"update Google Cloud API client import paths and moreThe Google Cloud API client libraries for Go are making some breaking changes:

The import paths are changing from google.golang.org/cloud/... to
cloud.google.com/go/.... For example, if your code imports the BigQuery client
it currently reads
import ""google.golang.org/cloud/bigquery""
It should be changed to
import ""cloud.google.com/go/bigquery""
Client options are also moving, from google.golang.org/cloud to
google.golang.org/api/option. Two have also been renamed:

WithBaseGRPC is now WithGRPCConn
WithBaseHTTP is now WithHTTPClient


The cloud.WithContext and cloud.NewContext methods are gone, as are the
deprecated pubsub and container functions that required them. Use the Client
methods of these packages instead.

You should make these changes before September 12, 2016, when the packages at
google.golang.org/cloud will go away.",3
,2610,148,30,16938,"They should have been in github in the first place.
cc @mansoorj",Copy relevant useful docs from https://cloud.google.com/container-engine/docs/,"Copy relevant useful docs from https://cloud.google.com/container-engine/docs/They should have been in github in the first place.
cc @mansoorj",3
,2611,143,31,5184,"While debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.
It's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.
I can see few solutions of such problem:

Get rid off HOST:PORT stuff and start using DNS service instead.
Add ability to wait for such information being propagated.
Injecting environment variables to container somehow(?).",Missing service environment variables while starting pod,"Missing service environment variables while starting podWhile debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.
It's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.
I can see few solutions of such problem:

Get rid off HOST:PORT stuff and start using DNS service instead.
Add ability to wait for such information being propagated.
Injecting environment variables to container somehow(?).",3
,2612,148,32,32224,"#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.
I think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.
cc @kubernetes/sig-api-machinery",registered.EnabledVersions returns all registered versions,"registered.EnabledVersions returns all registered versions#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.
I think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.
cc @kubernetes/sig-api-machinery",3
,2613,148,33,24614,"shellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.
/cc @zmerlynn",Add a `shellcheck` based pre-submit,"Add a `shellcheck` based pre-submitshellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.
/cc @zmerlynn",3
,2614,148,34,21932,"Scale is a sub-resource, we shouldn't generate a typed client for it.
cc @lavalamp @madhusudancs",Remove the generated client for Scale,"Remove the generated client for ScaleScale is a sub-resource, we shouldn't generate a typed client for it.
cc @lavalamp @madhusudancs",3
,2615,144,35,22907,It was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?,AWS: Check that dynamic volumes are deleted,AWS: Check that dynamic volumes are deletedIt was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?,3
,2616,142,36,12053,"We should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.
We may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.
For an example
#12039",Merge NamespaceExists and NamespaceLifecycle admission controllers,"Merge NamespaceExists and NamespaceLifecycle admission controllersWe should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.
We may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.
For an example
#12039",3
,2617,145,37,3345,"Clearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.
Possible solutions:

Segment by namespace
Require predeclarations

@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",Scaling clusters: 1000's of services in env vars,"Scaling clusters: 1000's of services in env varsClearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.
Possible solutions:

Segment by namespace
Require predeclarations

@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",3
,2618,143,38,2385,"One thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:
NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS
influx-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending
heapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running
synthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running
elasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending
kibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed

i.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:
NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS
influx-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending
heapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running
synthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running
elasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running
kibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running
kubectl.sh get services

but even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.
It would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:
apiVersion: v1beta1
kind: Pod
id: kibana-pod
desiredState:
  manifest:
    version: v1beta1
    id: kibana-server
    containers:
      - name: kibana-image
        image: kubernetes/kibana:latest
        ports:
          - name: kibana-port
            containerPort: 80
        dependencies: [elasticsearch]
labels:
  app: kibana-viewer

This would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",Pod dependencies on services,"Pod dependencies on servicesOne thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:
NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS
influx-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending
heapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running
synthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running
elasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending
kibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed

i.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:
NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS
influx-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending
heapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running
synthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running
elasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running
kibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running
kubectl.sh get services

but even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.
It would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:
apiVersion: v1beta1
kind: Pod
id: kibana-pod
desiredState:
  manifest:
    version: v1beta1
    id: kibana-server
    containers:
      - name: kibana-image
        image: kubernetes/kibana:latest
        ports:
          - name: kibana-port
            containerPort: 80
        dependencies: [elasticsearch]
labels:
  app: kibana-viewer

This would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",3
,2619,148,39,20556,"hi
i'm trying to run with k8s version 1.1.7
i updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)
after that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success
in  kube-controller-manager.log  i saw the following error
unable to create pod replica: Pod ""kube-ui-v4-"" is forbidden: service account kube-system/default was not found, retry after the service account is created
i googled it  and  found the following  solution

Generate a signing key:
openssl genrsa -out /tmp/serviceaccount.key 2048
Update /etc/kubernetes/apiserver:
KUBE_API_ARGS=""--service_account_key_file=/tmp/serviceaccount.key""
Update /etc/kubernetes/controller-manager:
KUBE_CONTROLLER_MANAGER_ARGS=""--service_account_private_key_file=/etc/kubernetes/serviceaccount.key""

but  i couldn't found the keys in the config files
i tried to update util.sh
and set the following flags (thats the only places that i found those keys)
--tls-private-key-file=/etc/kubernetes/serviceaccount.key""
--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \
note: doing kubectl serviceaccount for default namespace returns 1 entry
BUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!
i'm really desperate :) does anyone have a  clue how to fix this issue
tks a lot",service account kube-system/default was not found in ubuntu,"service account kube-system/default was not found in ubuntuhi
i'm trying to run with k8s version 1.1.7
i updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)
after that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success
in  kube-controller-manager.log  i saw the following error
unable to create pod replica: Pod ""kube-ui-v4-"" is forbidden: service account kube-system/default was not found, retry after the service account is created
i googled it  and  found the following  solution

Generate a signing key:
openssl genrsa -out /tmp/serviceaccount.key 2048
Update /etc/kubernetes/apiserver:
KUBE_API_ARGS=""--service_account_key_file=/tmp/serviceaccount.key""
Update /etc/kubernetes/controller-manager:
KUBE_CONTROLLER_MANAGER_ARGS=""--service_account_private_key_file=/etc/kubernetes/serviceaccount.key""

but  i couldn't found the keys in the config files
i tried to update util.sh
and set the following flags (thats the only places that i found those keys)
--tls-private-key-file=/etc/kubernetes/serviceaccount.key""
--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \
note: doing kubectl serviceaccount for default namespace returns 1 entry
BUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!
i'm really desperate :) does anyone have a  clue how to fix this issue
tks a lot",3
,2620,148,40,4024,"kubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.
Also, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",etcd arguments are different for systemd vs non-systemd,"etcd arguments are different for systemd vs non-systemdkubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.
Also, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",3
,2621,144,41,13060,"Hello,
For a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.
Regards.
Ron",Incorrect Login For Master,"Incorrect Login For MasterHello,
For a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.
Regards.
Ron",3
,2622,145,42,23389,"Recent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead
Jenkins jobs.
kubernetes-pull-build-test-e2e-gce/33368/
Stacktrace

/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463
Expected error:
    <*errors.errorString | 0xc208266480>: {
        s: ""Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n{\""metadata\"":{\""annotations\"":{\""kubectl.kubernetes.io/last-applied-configuration\"":\""{\\\""kind\\\"":\\\""ReplicationController\\\"",\\\""apiVersion\\\"":\\\""v1\\\"",\\\""metadata\\\"":{\\\""name\\\"":\\\""redis-master\\\"",\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""replicas\\\"":1,\\\""selector\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""},\\\""template\\\"":{\\\""metadata\\\"":{\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""containers\\\"":[{\\\""name\\\"":\\\""redis-master\\\"",\\\""image\\\"":\\\""redis\\\"",\\\""ports\\\"":[{\\\""name\\\"":\\\""redis-server\\\"",\\\""containerPort\\\"":6379}],\\\""resources\\\"":{}}]}}},\\\""status\\\"":{\\\""replicas\\\"":0}}\""},\""creationTimestamp\"":null,\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}},\""spec\"":{\""selector\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""},\""template\"":{\""metadata\"":{\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}}}}}\nto:\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\nfor: \""STDIN\"": replicationControllers \""redis-master\"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\nCommand stdout:\n\nstderr:\nError from server: error when applying patch:\n{\""metadata\"":{\""annotations\"":{\""kubectl.kubernetes.io/last-applied-configuration\"":\""{\\\""kind\\\"":\\\""ReplicationController\\\"",\\\""apiVersion\\\"":\\\""v1\\\"",\\\""metadata\\\"":{\\\""name\\\"":\\\""redis-master\\\"",\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""replicas\\\"":1,\\\""selector\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""},\\\""template\\\"":{\\\""metadata\\\"":{\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""containers\\\"":[{\\\""name\\\"":\\\""redis-master\\\"",\\\""image\\\"":\\\""redis\\\"",\\\""ports\\\"":[{\\\""name\\\"":\\\""redis-server\\\"",\\\""containerPort\\\"":6379}],\\\""resources\\\"":{}}]}}},\\\""status\\\"":{\\\""replicas\\\"":0}}\""},\""creationTimestamp\"":null,\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}},\""spec\"":{\""selector\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""},\""template\"":{\""metadata\"":{\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}}}}}\nto:\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\nfor: \""STDIN\"": replicationControllers \""redis-master\"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n"",
    }
    Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:
    {""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""kind\"":\""ReplicationController\"",\""apiVersion\"":\""v1\"",\""metadata\"":{\""name\"":\""redis-master\"",\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""replicas\"":1,\""selector\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""},\""template\"":{\""metadata\"":{\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""containers\"":[{\""name\"":\""redis-master\"",\""image\"":\""redis\"",\""ports\"":[{\""name\"":\""redis-server\"",\""containerPort\"":6379}],\""resources\"":{}}]}}},\""status\"":{\""replicas\"":0}}""},""creationTimestamp"":null,""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}},""spec"":{""selector"":{""kubectl.kubernetes.io/apply-test"":""ADDED""},""template"":{""metadata"":{""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}}}}}
    to:
    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}
    for: ""STDIN"": replicationControllers ""redis-master"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again
     [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:
    Command stdout:

    stderr:
    Error from server: error when applying patch:
    {""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""kind\"":\""ReplicationController\"",\""apiVersion\"":\""v1\"",\""metadata\"":{\""name\"":\""redis-master\"",\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""replicas\"":1,\""selector\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""},\""template\"":{\""metadata\"":{\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""containers\"":[{\""name\"":\""redis-master\"",\""image\"":\""redis\"",\""ports\"":[{\""name\"":\""redis-server\"",\""containerPort\"":6379}],\""resources\"":{}}]}}},\""status\"":{\""replicas\"":0}}""},""creationTimestamp"":null,""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}},""spec"":{""selector"":{""kubectl.kubernetes.io/apply-test"":""ADDED""},""template"":{""metadata"":{""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}}}}}
    to:
    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}
    for: ""STDIN"": replicationControllers ""redis-master"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again


not to have occurred

@spxtr to delegate.",e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RC,"e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RCRecent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead
Jenkins jobs.
kubernetes-pull-build-test-e2e-gce/33368/
Stacktrace

/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463
Expected error:
    <*errors.errorString | 0xc208266480>: {
        s: ""Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n{\""metadata\"":{\""annotations\"":{\""kubectl.kubernetes.io/last-applied-configuration\"":\""{\\\""kind\\\"":\\\""ReplicationController\\\"",\\\""apiVersion\\\"":\\\""v1\\\"",\\\""metadata\\\"":{\\\""name\\\"":\\\""redis-master\\\"",\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""replicas\\\"":1,\\\""selector\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""},\\\""template\\\"":{\\\""metadata\\\"":{\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""containers\\\"":[{\\\""name\\\"":\\\""redis-master\\\"",\\\""image\\\"":\\\""redis\\\"",\\\""ports\\\"":[{\\\""name\\\"":\\\""redis-server\\\"",\\\""containerPort\\\"":6379}],\\\""resources\\\"":{}}]}}},\\\""status\\\"":{\\\""replicas\\\"":0}}\""},\""creationTimestamp\"":null,\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}},\""spec\"":{\""selector\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""},\""template\"":{\""metadata\"":{\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}}}}}\nto:\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\nfor: \""STDIN\"": replicationControllers \""redis-master\"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\nCommand stdout:\n\nstderr:\nError from server: error when applying patch:\n{\""metadata\"":{\""annotations\"":{\""kubectl.kubernetes.io/last-applied-configuration\"":\""{\\\""kind\\\"":\\\""ReplicationController\\\"",\\\""apiVersion\\\"":\\\""v1\\\"",\\\""metadata\\\"":{\\\""name\\\"":\\\""redis-master\\\"",\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""replicas\\\"":1,\\\""selector\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""},\\\""template\\\"":{\\\""metadata\\\"":{\\\""creationTimestamp\\\"":null,\\\""labels\\\"":{\\\""app\\\"":\\\""redis\\\"",\\\""kubectl.kubernetes.io/apply-test\\\"":\\\""ADDED\\\"",\\\""role\\\"":\\\""master\\\""}},\\\""spec\\\"":{\\\""containers\\\"":[{\\\""name\\\"":\\\""redis-master\\\"",\\\""image\\\"":\\\""redis\\\"",\\\""ports\\\"":[{\\\""name\\\"":\\\""redis-server\\\"",\\\""containerPort\\\"":6379}],\\\""resources\\\"":{}}]}}},\\\""status\\\"":{\\\""replicas\\\"":0}}\""},\""creationTimestamp\"":null,\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}},\""spec\"":{\""selector\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""},\""template\"":{\""metadata\"":{\""labels\"":{\""kubectl.kubernetes.io/apply-test\"":\""ADDED\""}}}}}\nto:\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\nfor: \""STDIN\"": replicationControllers \""redis-master\"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n"",
    }
    Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:
    {""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""kind\"":\""ReplicationController\"",\""apiVersion\"":\""v1\"",\""metadata\"":{\""name\"":\""redis-master\"",\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""replicas\"":1,\""selector\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""},\""template\"":{\""metadata\"":{\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""containers\"":[{\""name\"":\""redis-master\"",\""image\"":\""redis\"",\""ports\"":[{\""name\"":\""redis-server\"",\""containerPort\"":6379}],\""resources\"":{}}]}}},\""status\"":{\""replicas\"":0}}""},""creationTimestamp"":null,""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}},""spec"":{""selector"":{""kubectl.kubernetes.io/apply-test"":""ADDED""},""template"":{""metadata"":{""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}}}}}
    to:
    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}
    for: ""STDIN"": replicationControllers ""redis-master"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again
     [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:
    Command stdout:

    stderr:
    Error from server: error when applying patch:
    {""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""kind\"":\""ReplicationController\"",\""apiVersion\"":\""v1\"",\""metadata\"":{\""name\"":\""redis-master\"",\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""replicas\"":1,\""selector\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""},\""template\"":{\""metadata\"":{\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""redis\"",\""kubectl.kubernetes.io/apply-test\"":\""ADDED\"",\""role\"":\""master\""}},\""spec\"":{\""containers\"":[{\""name\"":\""redis-master\"",\""image\"":\""redis\"",\""ports\"":[{\""name\"":\""redis-server\"",\""containerPort\"":6379}],\""resources\"":{}}]}}},\""status\"":{\""replicas\"":0}}""},""creationTimestamp"":null,""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}},""spec"":{""selector"":{""kubectl.kubernetes.io/apply-test"":""ADDED""},""template"":{""metadata"":{""labels"":{""kubectl.kubernetes.io/apply-test"":""ADDED""}}}}}
    to:
    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}
    for: ""STDIN"": replicationControllers ""redis-master"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again


not to have occurred

@spxtr to delegate.",3
,2623,148,43,19494,"As rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.
cc @derekparker @sjpotter @jonboulle",rkt: retrieve image size from rkt api service,"rkt: retrieve image size from rkt api serviceAs rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.
cc @derekparker @sjpotter @jonboulle",3
,2624,145,44,23170,"We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",Pods Killed Every 5 Minutes After Upgrading to 1.2,"Pods Killed Every 5 Minutes After Upgrading to 1.2We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",3
,2625,148,45,12137,"Hi,
I have the following service definition for use on GCE:
apiVersion: v1
kind: Service
metadata:
  name: auth
  labels:
    name: auth
spec:
  type: LoadBalancer
  ports:
  - name: api
    protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    name: auth

When i do a SRV lookup to get the port (as per instructions here) for that service using:
dig SRV _api._TCP.auth.default.cluster.local.
I get:
root@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.

; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV _api._tcp.auth.default.cluster.local.
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11979
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;_api._tcp.auth.default.cluster.local. IN SRV

;; AUTHORITY SECTION:
cluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60

;; Query time: 5 msec
;; SERVER: 10.111.240.10#53(10.111.240.10)
;; WHEN: Mon Aug 03 05:44:55 UTC 2015
;; MSG SIZE  rcvd: 123

The service does get registered properly and is responding. I can look up the service if I use:
dig SRV default.cluster.local.
I get:
; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV default.cluster.local.
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54040
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2

;; QUESTION SECTION:
;default.cluster.local.     IN  SRV

;; ANSWER SECTION:
default.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.
default.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.

;; ADDITIONAL SECTION:
kubernetes.default.cluster.local. 30 IN A   10.111.240.1
auth.default.cluster.local. 30 IN A 10.111.247.242

;; Query time: 7 msec
;; SERVER: 10.111.240.10#53(10.111.240.10)
;; WHEN: Mon Aug 03 05:46:53 UTC 2015
;; MSG SIZE  rcvd: 201

Why doesn't the port number get returned when I lookup the named port via an SRV record?",Named Ports not creating _name._protocol.<service> SRV entries,"Named Ports not creating _name._protocol.<service> SRV entriesHi,
I have the following service definition for use on GCE:
apiVersion: v1
kind: Service
metadata:
  name: auth
  labels:
    name: auth
spec:
  type: LoadBalancer
  ports:
  - name: api
    protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    name: auth

When i do a SRV lookup to get the port (as per instructions here) for that service using:
dig SRV _api._TCP.auth.default.cluster.local.
I get:
root@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.

; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV _api._tcp.auth.default.cluster.local.
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11979
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;_api._tcp.auth.default.cluster.local. IN SRV

;; AUTHORITY SECTION:
cluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60

;; Query time: 5 msec
;; SERVER: 10.111.240.10#53(10.111.240.10)
;; WHEN: Mon Aug 03 05:44:55 UTC 2015
;; MSG SIZE  rcvd: 123

The service does get registered properly and is responding. I can look up the service if I use:
dig SRV default.cluster.local.
I get:
; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV default.cluster.local.
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54040
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2

;; QUESTION SECTION:
;default.cluster.local.     IN  SRV

;; ANSWER SECTION:
default.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.
default.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.

;; ADDITIONAL SECTION:
kubernetes.default.cluster.local. 30 IN A   10.111.240.1
auth.default.cluster.local. 30 IN A 10.111.247.242

;; Query time: 7 msec
;; SERVER: 10.111.240.10#53(10.111.240.10)
;; WHEN: Mon Aug 03 05:46:53 UTC 2015
;; MSG SIZE  rcvd: 201

Why doesn't the port number get returned when I lookup the named port via an SRV record?",3
,2626,146,46,12828,"Something like kubectl run --env=""VAR=value"" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",Support setting env vars in kubectl run,"Support setting env vars in kubectl runSomething like kubectl run --env=""VAR=value"" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",3
,2627,146,47,26220,It would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere,Node e2e reporting,Node e2e reportingIt would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere,3
,2628,148,48,2728,"When I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'
Then I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover
However in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?
Best Regards
Simon",code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/cover,"code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/coverWhen I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'
Then I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover
However in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?
Best Regards
Simon",3
,2629,146,49,6132,"After making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that",kubectl should return more information on failure,"kubectl should return more information on failureAfter making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that",3
,2630,141,50,8753,"Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:
root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions
I0524 08:52:29.291509   19049 selector.go:53] Unable to list ""minions"": the server could not find the requested resource

Indeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1beta3"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""the server could not find the requested resource"",
  ""reason"": ""NotFound"",
  ""details"": {},
  ""code"": 404
}
Specifying older api version works though:
root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions
NAME         LABELS    STATUS
10.243.0.1   <none>    NotReady",/minions not available for api v1beta3,"/minions not available for api v1beta3Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:
root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions
I0524 08:52:29.291509   19049 selector.go:53] Unable to list ""minions"": the server could not find the requested resource

Indeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1beta3"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""the server could not find the requested resource"",
  ""reason"": ""NotFound"",
  ""details"": {},
  ""code"": 404
}
Specifying older api version works though:
root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions
NAME         LABELS    STATUS
10.243.0.1   <none>    NotReady",3
,2631,144,51,33202,"This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.
The relevant bit of output is as follows:
./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable
Kubernetes binaries at /Users/jon/kubernetes/cluster/
You may want to add this directory to your PATH in $HOME/.profile
Installation successful!

A viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable),"kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable)This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.
The relevant bit of output is as follows:
./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable
Kubernetes binaries at /Users/jon/kubernetes/cluster/
You may want to add this directory to your PATH in $HOME/.profile
Installation successful!

A viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",3
,2632,148,52,31488,"Kubernetes version (use kubectl version):
Client Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.4"", GitCommit:""dd6b458ef8dbf24aff55795baa68f83383c9b3a9"", GitTreeState:""clean"", BuildDate:""2016-08-01T16:45:16Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.5"", GitCommit:""b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5"", GitTreeState:""clean"", BuildDate:""2016-08-11T20:21:58Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}

Environment:

Cloud provider or hardware configuration: GCP VM & GCP container cluster
OS (e.g. from /etc/os-release):

NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""


Kernel (e.g. uname -a):

Linux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

What happened:
In a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.
On Mac, I can run ""kubectl proxy --port=8080"" or ""sudo kubectl proxy --port=8080"" without problem.
Run as a service, failed:
$ sudo systemctl restart kubectlproxy
Job for kubectlproxy.service failed because the control process exited with error code. See ""systemctl status kubectlproxy.service"" and ""journalctl -xe"" for details.

$ sudo systemctl -l status kubectlproxy.service
 kubectlproxy.service - kubectl proxy Service
   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)
   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago
  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)
 Main PID: 1982 (code=exited, status=1/FAILURE)

Aug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...
Aug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?
Aug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE
Aug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.
Aug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.
Aug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.

Run as sudo, failed:
$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080
The connection to the server localhost:8080 was refused - did you specify the right host or port?

Run as current user, succeeded:
$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080
Starting to serve on 127.0.0.1:8080

Below is the systemd service file:
[Unit]
Description=kubectl proxy Service
After=network.target

[Service]
Type=notify
User=root
ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p
ort=8080
Restart=on-abort

[Install]
WantedBy=multi-user.target",kubectl proxy failed to run as a service or sudo in GCE,"kubectl proxy failed to run as a service or sudo in GCEKubernetes version (use kubectl version):
Client Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.4"", GitCommit:""dd6b458ef8dbf24aff55795baa68f83383c9b3a9"", GitTreeState:""clean"", BuildDate:""2016-08-01T16:45:16Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.5"", GitCommit:""b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5"", GitTreeState:""clean"", BuildDate:""2016-08-11T20:21:58Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}

Environment:

Cloud provider or hardware configuration: GCP VM & GCP container cluster
OS (e.g. from /etc/os-release):

NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""


Kernel (e.g. uname -a):

Linux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

What happened:
In a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.
On Mac, I can run ""kubectl proxy --port=8080"" or ""sudo kubectl proxy --port=8080"" without problem.
Run as a service, failed:
$ sudo systemctl restart kubectlproxy
Job for kubectlproxy.service failed because the control process exited with error code. See ""systemctl status kubectlproxy.service"" and ""journalctl -xe"" for details.

$ sudo systemctl -l status kubectlproxy.service
 kubectlproxy.service - kubectl proxy Service
   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)
   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago
  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)
 Main PID: 1982 (code=exited, status=1/FAILURE)

Aug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...
Aug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?
Aug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE
Aug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.
Aug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.
Aug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.

Run as sudo, failed:
$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080
The connection to the server localhost:8080 was refused - did you specify the right host or port?

Run as current user, succeeded:
$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080
Starting to serve on 127.0.0.1:8080

Below is the systemd service file:
[Unit]
Description=kubectl proxy Service
After=network.target

[Service]
Type=notify
User=root
ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p
ort=8080
Restart=on-abort

[Install]
WantedBy=multi-user.target",3
,2633,141,53,4235,"Most applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:

User sets up cluster in a region in a way that minions are spread evenly across all available zones
User creates replication controller for the application with size > 1
Scheduler spreads pods within the same replication controller across available zones

That way user gets regional availability for free.
AFAIU this will mostly require changes in how we create cluster and how we schedule pods.
cc @wojtek-t",Kubernetes should support cross-zone cluster,"Kubernetes should support cross-zone clusterMost applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:

User sets up cluster in a region in a way that minions are spread evenly across all available zones
User creates replication controller for the application with size > 1
Scheduler spreads pods within the same replication controller across available zones

That way user gets regional availability for free.
AFAIU this will mostly require changes in how we create cluster and how we schedule pods.
cc @wojtek-t",3
,2634,148,54,23397,"Docker v1.11 rcs are starting to show up. Its time to get started with the validation.
https://github.com/docker/docker/releases/tag/v1.11.0-rc1
cc @kubernetes/sig-node
EDIT(timstclair):
TASKS:

 e2e tests pass
 performance analysis
 startup tests (bootstrap, restart)
 live upgrade successful
 1 week soak tests",Validate Docker v1.11,"Validate Docker v1.11Docker v1.11 rcs are starting to show up. Its time to get started with the validation.
https://github.com/docker/docker/releases/tag/v1.11.0-rc1
cc @kubernetes/sig-node
EDIT(timstclair):
TASKS:

 e2e tests pass
 performance analysis
 startup tests (bootstrap, restart)
 live upgrade successful
 1 week soak tests",3
,2635,142,55,24343,"A large monorepo works for Google, but not on github.
We hit the ceiling of achievable velocity of a single github repo in early 2015:
https://github.com/kubernetes/kubernetes/graphs/contributors
There are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.
We're chipping away at these issues, but we need more than incremental improvement.
We've discussed moving a number of things to other repos:

Kubelet: #444
Generic API infrastructure: #2742
Client libraries: #5660
Misc. utilities: #24156
kubectl
scheduler
examples
cloud providers + cluster code + ""getting-started guides""
auth plugins
network and storage plugins?
e2e tests
contributor documentation

We need to seriously think about how to do this.
Known issues that need to be addressed:

We need to get sprawl and package dependencies under control: #4851
We need to make most components ordinary clients of the API: #20193
We need to figure out dependency management and integration testing

An example of a Go project on github with good repo hygiene:
https://github.com/deis
I have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....
Speaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762
@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",Figure out how to handle code in multiple repos,"Figure out how to handle code in multiple reposA large monorepo works for Google, but not on github.
We hit the ceiling of achievable velocity of a single github repo in early 2015:
https://github.com/kubernetes/kubernetes/graphs/contributors
There are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.
We're chipping away at these issues, but we need more than incremental improvement.
We've discussed moving a number of things to other repos:

Kubelet: #444
Generic API infrastructure: #2742
Client libraries: #5660
Misc. utilities: #24156
kubectl
scheduler
examples
cloud providers + cluster code + ""getting-started guides""
auth plugins
network and storage plugins?
e2e tests
contributor documentation

We need to seriously think about how to do this.
Known issues that need to be addressed:

We need to get sprawl and package dependencies under control: #4851
We need to make most components ordinary clients of the API: #20193
We need to figure out dependency management and integration testing

An example of a Go project on github with good repo hygiene:
https://github.com/deis
I have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....
Speaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762
@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",3
,2636,142,56,20885,"PR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.
In order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",Modify E2E tests to use the GCE API instead of gcloud exec,"Modify E2E tests to use the GCE API instead of gcloud execPR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.
In order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",3
,2637,148,57,10130,"We need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the ""how"" info we provide today to improve/clarify what the specific steps.
(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).
Some details we should make clear:

When do i want containers to consume info about the system without coupling to k8s client or REST API?
Do we say ""use the downward API when your containers need access to information about the cluster in which it resides""?
We should also tie together/link/mention the other content we have on using environment variables and the downward api example.","Add the ""when"" and ""why"" to use the Downward API","Add the ""when"" and ""why"" to use the Downward APIWe need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the ""how"" info we provide today to improve/clarify what the specific steps.
(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).
Some details we should make clear:

When do i want containers to consume info about the system without coupling to k8s client or REST API?
Do we say ""use the downward API when your containers need access to information about the cluster in which it resides""?
We should also tie together/link/mention the other content we have on using environment variables and the downward api example.",3
,2638,144,58,26816,"A pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).
The pod also does not show up in kubectl get pods in any state, though it can still be deleted.
The latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.

Should the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?
1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)
Pods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.

Example failing pod
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: mount-dn
  name: mount-dne
spec:
  restartPolicy: Never
  volumes:
  - name: dne
    hostPath:
      path: /does/not/exist
  containers:
    - name: exit
      image: busybox
      command: [""sh"", ""-c"", ""ls /test; sleep 60""]
      volumeMounts:
      - mountPath: /test
        name: dne
cc @yifan-gu @tmrts",rkt: hostPath mounts to non-existent directories fail,"rkt: hostPath mounts to non-existent directories failA pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).
The pod also does not show up in kubectl get pods in any state, though it can still be deleted.
The latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.

Should the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?
1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)
Pods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.

Example failing pod
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: mount-dn
  name: mount-dne
spec:
  restartPolicy: Never
  volumes:
  - name: dne
    hostPath:
      path: /does/not/exist
  containers:
    - name: exit
      image: busybox
      command: [""sh"", ""-c"", ""ls /test; sleep 60""]
      volumeMounts:
      - mountPath: /test
        name: dne
cc @yifan-gu @tmrts",3
,2639,148,59,22485,"In the getting-started-guides matches the detection of the docker version not with a version 1.1x.
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",fixing detecting docker version,"fixing detecting docker versionIn the getting-started-guides matches the detection of the docker version not with a version 1.1x.
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",3
,2640,144,60,20154,"Encountered on #18901
!!! Error in ./hack/test-cmd.sh:203
  '[ ""$(kubectl get nodes -o go-template='{{ .apiVersion }}' ""${kube_flags[@]}"")"" == ""v1"" ]' exited with status 1
Call stack:
  1: ./hack/test-cmd.sh:203 runTests(...)
  2: ./hack/test-cmd.sh:1352 main(...)
Exiting with status 1

cc @kargakis",test-cmd flake: first kubectl command (get nodes) fails,"test-cmd flake: first kubectl command (get nodes) failsEncountered on #18901
!!! Error in ./hack/test-cmd.sh:203
  '[ ""$(kubectl get nodes -o go-template='{{ .apiVersion }}' ""${kube_flags[@]}"")"" == ""v1"" ]' exited with status 1
Call stack:
  1: ./hack/test-cmd.sh:203 runTests(...)
  2: ./hack/test-cmd.sh:1352 main(...)
Exiting with status 1

cc @kargakis",3
,2641,148,61,11965,"Limit outbound and inbound bandwidth, using the tc tool, see:
https://www.iplocation.net/traffic-control",Use bandwidth shaping on the master,"Use bandwidth shaping on the masterLimit outbound and inbound bandwidth, using the tc tool, see:
https://www.iplocation.net/traffic-control",3
,2642,146,62,1988,"I find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.","Add an optional ""why"" clause to ValidationError","Add an optional ""why"" clause to ValidationErrorI find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.",3
,2643,148,63,22524,"We'd like to stay vet clean, and aren't too far away.",Add go_vet to presubmit,"Add go_vet to presubmitWe'd like to stay vet clean, and aren't too far away.",3
,2644,148,64,33382,"cc: @derekwaynecarr (how can we get devicemapper support for this?)
when a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.
This is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.
Progress:

 Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).
 Modify cadvisor api and kubernetes api to include per-container inode usage
 Modify cadvisor to publish per-container inode usage
 Modify kubelet eviction manager to take per-container inode usage into account when evicting",Per-Container Inode Accounting,"Per-Container Inode Accountingcc: @derekwaynecarr (how can we get devicemapper support for this?)
when a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.
This is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.
Progress:

 Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).
 Modify cadvisor api and kubernetes api to include per-container inode usage
 Modify cadvisor to publish per-container inode usage
 Modify kubelet eviction manager to take per-container inode usage into account when evicting",3
,2645,140,65,20820,"With docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.
For containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.
/cc @stephenR",[docker 1.10] create syscall filters for k8s-supplied components,"[docker 1.10] create syscall filters for k8s-supplied componentsWith docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.
For containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.
/cc @stephenR",3
,2646,140,66,7493,"Hi,
iam has a few roles. Is it ""Grant API access to SAML providers""?
Thank you.
skwok",What kind of aws roles do I need to prepare for kubernetes,"What kind of aws roles do I need to prepare for kubernetesHi,
iam has a few roles. Is it ""Grant API access to SAML providers""?
Thank you.
skwok",3
,2647,148,67,3894,"A user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following.  I verified similar behavior in kubectl v0.9.1.
I am using Kubernetes v0.7.0, when I run ""kubectl"" to create pod, I found:
$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml
I0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --> this is to stderr
mariadb    --> this is to stdout
I do not know why the message ""...Waiting for completion ..."" is considered as an error, I think it should be to stdout too.",kubectl: rationalize which messages to stderr/stdout,"kubectl: rationalize which messages to stderr/stdoutA user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following.  I verified similar behavior in kubectl v0.9.1.
I am using Kubernetes v0.7.0, when I run ""kubectl"" to create pod, I found:
$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml
I0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --> this is to stderr
mariadb    --> this is to stdout
I do not know why the message ""...Waiting for completion ..."" is considered as an error, I think it should be to stdout too.",3
,2648,145,68,22938,cc: @piosz,Version is missing in Heapster configuration,Version is missing in Heapster configurationcc: @piosz,3
,2649,143,69,7906,"I have some reports of a Node reporting a a resource capacity for cpu as a negative value.
osc describe node `hostname -f`
Name:                   <omitted>
Labels:                 <none>
CreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400
Conditions:
  Type          Status  LastHeartbeatTime
LastTransitionTime                      Reason
        Message
  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07
May 2015 09:51:54 -0400         kubelet is posting ready status
Addresses:      <omitted>
Capacity:
 cpu:           -1
 memory:        1884432Ki
Version:
 Kernel Version:                3.10.0-229.el7.x86_64
 OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)
 Container Runtime Version:     docker://1.6.0
 Kubelet Version:               v0.14.1-582-gb12d75d
 Kube-Proxy Version:            v0.14.1-582-gb12d75d
ExternalID:                     <omitted>
Pods:                           (0 in total)
No events.

Any idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.
@dchen1107 @vmarmol - any ideas?",Kubelet is reporting Node cpu capacity as negative value,"Kubelet is reporting Node cpu capacity as negative valueI have some reports of a Node reporting a a resource capacity for cpu as a negative value.
osc describe node `hostname -f`
Name:                   <omitted>
Labels:                 <none>
CreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400
Conditions:
  Type          Status  LastHeartbeatTime
LastTransitionTime                      Reason
        Message
  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07
May 2015 09:51:54 -0400         kubelet is posting ready status
Addresses:      <omitted>
Capacity:
 cpu:           -1
 memory:        1884432Ki
Version:
 Kernel Version:                3.10.0-229.el7.x86_64
 OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)
 Container Runtime Version:     docker://1.6.0
 Kubelet Version:               v0.14.1-582-gb12d75d
 Kube-Proxy Version:            v0.14.1-582-gb12d75d
ExternalID:                     <omitted>
Pods:                           (0 in total)
No events.

Any idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.
@dchen1107 @vmarmol - any ideas?",3
,2650,141,70,20070,"Hello everyone!
I noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:
spec:
  ports:
  - port: 80
    name: foo
    targetPort: 3000
$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""no endpoints available for service \""gogs\"""",
  ""reason"": ""ServiceUnavailable"",
  ""code"": 503
}
But when I deleted the Service.spec.ports[*].name field, it worked as it should:
spec:
  ports:
  - port: 80
    targetPort: 3000
$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar
OK
Is this a known bug? Or is it made this way ""by design""?
This also makes services with two or more ports inaccessible via apiserver proxy
@ArtfulCoder @thockin",apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specified,"apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specifiedHello everyone!
I noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:
spec:
  ports:
  - port: 80
    name: foo
    targetPort: 3000
$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""no endpoints available for service \""gogs\"""",
  ""reason"": ""ServiceUnavailable"",
  ""code"": 503
}
But when I deleted the Service.spec.ports[*].name field, it worked as it should:
spec:
  ports:
  - port: 80
    targetPort: 3000
$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar
OK
Is this a known bug? Or is it made this way ""by design""?
This also makes services with two or more ports inaccessible via apiserver proxy
@ArtfulCoder @thockin",3
,2651,144,71,15474,"We need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).
The tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",Implement e2e tests for horizontal pod autoscaling of deployments,"Implement e2e tests for horizontal pod autoscaling of deploymentsWe need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).
The tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",3
,2652,146,72,26513,"1 Is webhook plugin stable?
2 How webhook works?
3 How can I use webhook and where can find detailed information.
Thank you very much!","How Webhook works, where can find the details? thanks","How Webhook works, where can find the details? thanks1 Is webhook plugin stable?
2 How webhook works?
3 How can I use webhook and where can find detailed information.
Thank you very much!",3
,2653,148,73,19167,"It's bad. kubertest-test-go suite failed 5 consecutive times because of it.
@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick
Known Issues:

 pkg/master #19141 (fixed by @wojtek-t in #19195)
 pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)
 pkg/storage/etcd, pkg/registry/generic/etcd #18928
 pkg/apiserver #19176
 pkg/client/record/event_test.go #19151
 k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186
 pkg/util/wait: #19223
 pkg/client/unversioned/portforward #19230
 plugin/pkg/scheduler/factory #19229
 pkg/storage #19254
 pkg/client/record #19268",Unit tests are EXTREMELY flaky lately,"Unit tests are EXTREMELY flaky latelyIt's bad. kubertest-test-go suite failed 5 consecutive times because of it.
@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick
Known Issues:

 pkg/master #19141 (fixed by @wojtek-t in #19195)
 pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)
 pkg/storage/etcd, pkg/registry/generic/etcd #18928
 pkg/apiserver #19176
 pkg/client/record/event_test.go #19151
 k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186
 pkg/util/wait: #19223
 pkg/client/unversioned/portforward #19230
 plugin/pkg/scheduler/factory #19229
 pkg/storage #19254
 pkg/client/record #19268",3
,2654,148,74,17805,"Observed odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",Fix l7 controller watch on namespaces,"Fix l7 controller watch on namespacesObserved odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",3
,2655,146,75,424,"When exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.
    $ sudo docker run --rm --name web2 --link db:db training/webapp env
    . . .
    DB_NAME=/web2/db
    DB_PORT=tcp://172.17.0.5:5432
    DB_PORT_5000_TCP=tcp://172.17.0.5:5432
    DB_PORT_5000_TCP_PROTO=tcp
    DB_PORT_5000_TCP_PORT=5432
    DB_PORT_5000_TCP_ADDR=172.17.0.5

The code writing the env var from service definition is here:
https://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46
I believe it could be easily adapted to follow the same standard defined by docker.",support for docker links env variables format,"support for docker links env variables formatWhen exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.
    $ sudo docker run --rm --name web2 --link db:db training/webapp env
    . . .
    DB_NAME=/web2/db
    DB_PORT=tcp://172.17.0.5:5432
    DB_PORT_5000_TCP=tcp://172.17.0.5:5432
    DB_PORT_5000_TCP_PROTO=tcp
    DB_PORT_5000_TCP_PORT=5432
    DB_PORT_5000_TCP_ADDR=172.17.0.5

The code writing the env var from service definition is here:
https://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46
I believe it could be easily adapted to follow the same standard defined by docker.",3
,2656,144,76,4783,"We ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",Add e2e test for etcd failures handling,"Add e2e test for etcd failures handlingWe ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",3
,2657,140,77,3800,"Since the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.
If .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.
This would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.
/cc @jlowdermilk @smarterclayton @liggitt",Remove --auth-path from kubectl,"Remove --auth-path from kubectlSince the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.
If .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.
This would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.
/cc @jlowdermilk @smarterclayton @liggitt",3
,2658,148,78,13709,"EDIT: kubernetes version 1.0.3
One of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.

It stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.
iptables:
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL
KUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
KUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */
DOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL
KUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.16.70.0/24       anywhere

Chain DOCKER (2 references)
target     prot opt source               destination

Chain KUBE-NODEPORT-CONTAINER (1 references)
target     prot opt source               destination
REDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768

Chain KUBE-NODEPORT-HOST (1 references)
target     prot opt source               destination
DNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768

Chain KUBE-PORTALS-CONTAINER (1 references)
target     prot opt source               destination
REDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726
REDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087
REDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046
REDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397
REDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270
REDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069
REDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624
REDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381
REDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223
REDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521
REDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287
REDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104
REDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337
REDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768

Chain KUBE-PORTALS-HOST (1 references)
target     prot opt source               destination
DNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726
DNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087
DNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046
DNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397
DNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270
DNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069
DNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624
DNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381
DNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223
DNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521
DNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287
DNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104
DNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337
DNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768

Nothing interesting in logs. So I just skip them.",kube-proxy's udp proxy dead,"kube-proxy's udp proxy deadEDIT: kubernetes version 1.0.3
One of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.

It stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.
iptables:
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL
KUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
KUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */
DOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL
KUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.16.70.0/24       anywhere

Chain DOCKER (2 references)
target     prot opt source               destination

Chain KUBE-NODEPORT-CONTAINER (1 references)
target     prot opt source               destination
REDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337
REDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768

Chain KUBE-NODEPORT-HOST (1 references)
target     prot opt source               destination
DNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337
DNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768

Chain KUBE-PORTALS-CONTAINER (1 references)
target     prot opt source               destination
REDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726
REDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087
REDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046
REDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397
REDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270
REDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069
REDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624
REDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381
REDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223
REDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521
REDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287
REDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104
REDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337
REDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768

Chain KUBE-PORTALS-HOST (1 references)
target     prot opt source               destination
DNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726
DNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087
DNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046
DNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397
DNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270
DNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069
DNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624
DNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381
DNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223
DNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521
DNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287
DNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104
DNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337
DNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768

Nothing interesting in logs. So I just skip them.",3
,2659,144,79,30962,"Failed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/
Run so broken it didn't make JUnit output!",kubernetes-e2e-gke-staging: broken test run,"kubernetes-e2e-gke-staging: broken test runFailed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/
Run so broken it didn't make JUnit output!",3
,2660,148,80,8792,"when building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:
(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test
build/make-clean.sh
+++ [0525 20:52:03] Verifying Prerequisites....
+++ [0525 20:52:03] Cleaning out local _output directory
rm -rf _output
rm -rf Godeps/_workspace/pkg
hack/build-go.sh test/e2e/e2e.test
+++ [0525 20:52:04] Building go targets for darwin/amd64:
test/e2e/e2e.test
/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory
!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361
'pushd ""$(dirname ${outfile})"" > /dev/null' exited with status 1
Call stack:
1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)
2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)
3: hack/build-go.sh:26 main(...)
Exiting with status 1
make: *** [all] Error 1
However 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works","make clean ; make WHAT=test/e2e/e2e.test"" fails","make clean ; make WHAT=test/e2e/e2e.test"" failswhen building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:
(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test
build/make-clean.sh
+++ [0525 20:52:03] Verifying Prerequisites....
+++ [0525 20:52:03] Cleaning out local _output directory
rm -rf _output
rm -rf Godeps/_workspace/pkg
hack/build-go.sh test/e2e/e2e.test
+++ [0525 20:52:04] Building go targets for darwin/amd64:
test/e2e/e2e.test
/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory
!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361
'pushd ""$(dirname ${outfile})"" > /dev/null' exited with status 1
Call stack:
1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)
2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)
3: hack/build-go.sh:26 main(...)
Exiting with status 1
make: *** [all] Error 1
However 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works",3
,2661,148,81,14023,"I've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.
I've taking a look at both the following similar issues and they don't appear to be the cause
#4891
#3185
Cluster is running v1.0.3
Here's an example of the state
docker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide
NAME                READY     STATUS    RESTARTS   AGE       NODE
kube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35
kube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34

get events
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e

scheduler log
I0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:""Pod"", Namespace:""kube-system"", Name:""kube-dns-v8-jti2e"", UID:""c1cafebe-5c3b-11e5-b3c4-020443b6797d"", APIVersion:""v1"", ResourceVersion:""670117"", FieldPath:""""}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34
I0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:""Pod"", Namespace:""kube-system"", Name:""kube-dns-v8-i0yac"", UID:""c1cafc69-5c3b-11e5-b3c4-020443b6797d"", APIVersion:""v1"", ResourceVersion:""670118"", FieldPath:""""}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35

tailing kubelet log file during pod create
tail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668
I0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}

kubelet process
root@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet
root     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem

node list
docker run --rm -it lachie83/kubectl:prod get nodes
NAME            LABELS                                             STATUS
10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready
10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready
10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready
10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready
10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready
10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready",Pods hang in pending state indefinitely,"Pods hang in pending state indefinitelyI've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.
I've taking a look at both the following similar issues and they don't appear to be the cause
#4891
#3185
Cluster is running v1.0.3
Here's an example of the state
docker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide
NAME                READY     STATUS    RESTARTS   AGE       NODE
kube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35
kube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34

get events
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34
Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e

scheduler log
I0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:""Pod"", Namespace:""kube-system"", Name:""kube-dns-v8-jti2e"", UID:""c1cafebe-5c3b-11e5-b3c4-020443b6797d"", APIVersion:""v1"", ResourceVersion:""670117"", FieldPath:""""}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34
I0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:""Pod"", Namespace:""kube-system"", Name:""kube-dns-v8-i0yac"", UID:""c1cafc69-5c3b-11e5-b3c4-020443b6797d"", APIVersion:""v1"", ResourceVersion:""670118"", FieldPath:""""}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35

tailing kubelet log file during pod create
tail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668
I0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}
I0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}

kubelet process
root@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet
root     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem

node list
docker run --rm -it lachie83/kubectl:prod get nodes
NAME            LABELS                                             STATUS
10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready
10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready
10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready
10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready
10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready
10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready",3
,2662,148,82,26304,"In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the ""run this code and hope it doesn't hang"" variety.
Follow-up issue.","Fix wait.Until tests to actually verify something other than ""not hanging""","Fix wait.Until tests to actually verify something other than ""not hanging""In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the ""run this code and hope it doesn't hang"" variety.
Follow-up issue.",3
,2663,146,83,16626,"AFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.
cc @mikedanese @timstclair @lavalamp",Document how to add a new API group,"Document how to add a new API groupAFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.
cc @mikedanese @timstclair @lavalamp",3
,2664,146,84,16124,,Add a user guide readme for deployments,Add a user guide readme for deployments,3
,2665,148,85,5585,"@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.
Start two containers on the same machine, each a shell script that traps SIGTERM and prints something like I received SIGTERM and quits. Then do docker stop and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",Investigate possibly broken Docker SIGTERM delivery,"Investigate possibly broken Docker SIGTERM delivery@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.
Start two containers on the same machine, each a shell script that traps SIGTERM and prints something like I received SIGTERM and quits. Then do docker stop and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",3
,2666,148,86,4628,"From #4486
Proposed:

label values: restrict to [A-Za-z0-9_-.]*
annotation values: no restriction
qualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of
what we allow today.  We can say that convention is to use dns-compatible
domain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that ""bar/bat"" means (bar, bat) while ""foo.com/bar/bat"" means (foo.com, bar/bat).

@smarterclayton does that give you enough freedom?
@bgrant0607 does that give you enough consistency?
@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",Loosen label and annotation validation and related tests,"Loosen label and annotation validation and related testsFrom #4486
Proposed:

label values: restrict to [A-Za-z0-9_-.]*
annotation values: no restriction
qualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of
what we allow today.  We can say that convention is to use dns-compatible
domain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that ""bar/bat"" means (bar, bat) while ""foo.com/bar/bat"" means (foo.com, bar/bat).

@smarterclayton does that give you enough freedom?
@bgrant0607 does that give you enough consistency?
@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",3
,2667,142,87,5946,"We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?,"`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",3
,2668,148,88,20514,"Following the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a ""no cloud provider specified"" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it
I'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.
Here is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )

default
docker-compose file:
master:
  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6
  net: host
  pid: host
  privileged: true
  volumes:
    - /:/rootfs:ro
    - /sys:/sys:ro
    - /dev:/dev
    - /var/lib/docker/:/var/lib/docker:rw
    - /var/lib/kubelet/:/var/lib/kubelet:rw
    - /var/run:/var/run:rw
  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']

logs:
Attaching to aiok8s_master_1
[36mmaster_1 | [0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)
[36mmaster_1 | [0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)
[36mmaster_1 | [0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.
[36mmaster_1 | [0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
[36mmaster_1 | [0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.
[36mmaster_1 | [0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: """" from the config file: """"
[36mmaster_1 | [0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: ""/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e""
[36mmaster_1 | [0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]
[36mmaster_1 | [0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in ""/rootfs/etc/machine-id,/var/lib/dbus/machine-id""
[36mmaster_1 | [0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}
[36mmaster_1 | [0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}
[36mmaster_1 | [0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet
[36mmaster_1 | [0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.
[36mmaster_1 | [0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests
[36mmaster_1 | [0mI0202 21:15:54.496911    6562 file.go:47] Watching path ""etc/kubernetes/manifests""
[36mmaster_1 | [0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver
[36mmaster_1 | [0mI0202 21:15:54.497132    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/etcd.json""
[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/services?resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0  in 1 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.502225    6562 common.go:52] Generated UID ""e171033ec56ced6f29a4417f8b61cbf0"" pod ""k8s-etcd"" from etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.502880    6562 common.go:56] Generated Name ""k8s-etcd-127.0.0.1"" for UID ""e171033ec56ced6f29a4417f8b61cbf0"" from URL etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.502892    6562 common.go:61] Using namespace ""default"" for pod ""k8s-etcd-127.0.0.1"" from etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.503109    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/kube-proxy.json""
[36mmaster_1 | [0mI0202 21:15:54.503393    6562 common.go:52] Generated UID ""e513e62ea641585218de8b3495fc7a21"" pod ""k8s-proxy"" from etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503419    6562 common.go:56] Generated Name ""k8s-proxy-127.0.0.1"" for UID ""e513e62ea641585218de8b3495fc7a21"" from URL etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503429    6562 common.go:61] Using namespace ""default"" for pod ""k8s-proxy-127.0.0.1"" from etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503514    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/master.json""
[36mmaster_1 | [0mI0202 21:15:54.503983    6562 common.go:52] Generated UID ""eaf8ef5e21f965406a198841c5faa403"" pod ""k8s-master"" from etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504018    6562 common.go:56] Generated Name ""k8s-master-127.0.0.1"" for UID ""eaf8ef5e21f965406a198841c5faa403"" from URL etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504029    6562 common.go:61] Using namespace ""default"" for pod ""k8s-master-127.0.0.1"" from etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file
[36mmaster_1 | [0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)""
[36mmaster_1 | [0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)""
[36mmaster_1 | [0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0  in 3 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker
[36mmaster_1 | [0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg
[36mmaster_1 | [0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/aws-ebs""
[36mmaster_1 | [0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/empty-dir""
[36mmaster_1 | [0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/gce-pd""
[36mmaster_1 | [0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/git-repo""
[36mmaster_1 | [0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/host-path""
[36mmaster_1 | [0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/nfs""
[36mmaster_1 | [0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/secret""
[36mmaster_1 | [0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/iscsi""
[36mmaster_1 | [0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/glusterfs""
[36mmaster_1 | [0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/persistent-claim""
[36mmaster_1 | [0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/rbd""
[36mmaster_1 | [0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/cinder""
[36mmaster_1 | [0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/cephfs""
[36mmaster_1 | [0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/downward-api""
[36mmaster_1 | [0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/fc""
[36mmaster_1 | [0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/flocker""
[36mmaster_1 | [0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet
[36mmaster_1 | [0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250
[36mmaster_1 | [0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /
[36mmaster_1 | [0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {""kind"":""Event"",""apiVersion"":""v1"",""metadata"":{""name"":""127.0.0.1.142f3c86e792e0e6"",""namespace"":""default"",""creationTimestamp"":null},""involvedObject"":{""kind"":""Node"",""name"":""127.0.0.1"",""uid"":""127.0.0.1""},""reason"":""Starting"",""message"":""Starting kubelet."",""source"":{""component"":""kubelet"",""host"":""127.0.0.1""},""firstTimestamp"":""2016-02-02T21:15:54Z"",""lastTimestamp"":""2016-02-02T21:15:54Z"",""count"":1,""type"":""Normal""}
[36mmaster_1 | [0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" -H ""Content-Type: application/json"" http://localhost:8080/api/v1/namespaces/default/events
[36mmaster_1 | [0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255
[36mmaster_1 | [0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:""Node"", Namespace:"""", Name:""127.0.0.1"", UID:""127.0.0.1"", APIVersion:"""", ResourceVersion:"""", FieldPath:""""}): type: 'Normal' reason: 'Starting' Starting kubelet.
[36mmaster_1 | [0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
[36mmaster_1 | [0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container ""/kubelet""
[36mmaster_1 | [0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331
[36mmaster_1 | [0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver
[36mmaster_1 | [0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.
[36mmaster_1 | [0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting
[36mmaster_1 | [0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, ""file""): """"
[36mmaster_1 | [0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f""}
[36mmaster_1 | [0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, ""file""): ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18""}
[36mmaster_1 | [0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)""
[36mmaster_1 | [0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)""
[36mmaster_1 | [0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-proxy-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default"",""uid"":""e513e62ea641585218de8b3495fc7a21"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""e513e62ea641585218de8b3495fc7a21"",""kubernetes.io/config.mirror"":""e513e62ea641585218de8b3495fc7a21"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504500123Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""containers"":[{""name"":""kube-proxy"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""proxy"",""--master=http://127.0.0.1:8080"",""--v=2"",""--resource-container=\""\""""],""resources"":{},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent"",""securityContext"":{""privileged"":true}}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-etcd-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default"",""uid"":""e171033ec56ced6f29a4417f8b61cbf0"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""e171033ec56ced6f29a4417f8b61cbf0"",""kubernetes.io/config.mirror"":""e171033ec56ced6f29a4417f8b61cbf0"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504450902Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""volumes"":[{""name"":""varetcd"",""emptyDir"":{}}],""containers"":[{""name"":""etcd"",""image"":""gcr.io/google_containers/etcd:2.2.1"",""command"":[""/usr/local/bin/etcd"",""--listen-client-urls=http://127.0.0.1:4001"",""--advertise-client-urls=http://127.0.0.1:4001"",""--data-dir=/var/etcd/data""],""resources"":{},""volumeMounts"":[{""name"":""varetcd"",""mountPath"":""/var/etcd""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" -H ""Content-Type: application/json"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-master-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-master-127.0.0.1/default"",""uid"":""eaf8ef5e21f965406a198841c5faa403"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""eaf8ef5e21f965406a198841c5faa403"",""kubernetes.io/config.mirror"":""eaf8ef5e21f965406a198841c5faa403"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504513882Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""volumes"":[{""name"":""data"",""emptyDir"":{}}],""containers"":[{""name"":""controller-manager"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""controller-manager"",""--master=127.0.0.1:8080"",""--min-resync-period=3m"",""--service-account-private-key-file=/srv/kubernetes/server.key"",""--root-ca-file=/srv/kubernetes/ca.crt"",""--v=2""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/srv/kubernetes""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""apiserver"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""apiserver"",""--service-cluster-ip-range=10.0.0.1/24"",""--insecure-bind-address=127.0.0.1"",""--etcd-servers=http://127.0.0.1:4001"",""--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota"",""--client-ca-file=/srv/kubernetes/ca.crt"",""--basic-auth-file=/srv/kubernetes/basic_auth.csv"",""--min-request-timeout=300"",""--tls-cert-file=/srv/kubernetes/server.cert"",""--tls-private-key-file=/srv/kubernetes/server.key"",""--token-auth-file=/srv/kubernetes/known_tokens.csv"",""--allow-privileged=true"",""--v=4""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/srv/kubernetes""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""scheduler"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""scheduler"",""--master=127.0.0.1:8080"",""--v=2""],""resources"":{},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""setup"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/setup-files.sh""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/data""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""Content-Type: application/json"" -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""Content-Type: application/json"" -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin ""kubernetes.io/empty-dir"" for varetcd
[36mmaster_1 | [0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]
[36mmaster_1 | [0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1
[36mmaster_1 | [0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin ""kubernetes.io/empty-dir"" for data
[36mmaster_1 | [0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]
[36mmaster_1 | [0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1
[36mmaster_1 | [0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05""}
[36mmaster_1 | [0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:<nil> NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=""""] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28""}
[36mmaster_1 | [0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:<nil> NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)"": &{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=""""] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:<nil> ReadinessProbe:<nil> Lifecycle:<nil> TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:<nil> ContainerStatuses:[]}}


with nsenter modification
docker-compose file:
master:
  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6
  net: host
  pid: host
  privileged: true
  volumes:
    - /sys:/sys:ro
    - /dev:/dev
    - /var/lib/docker/:/var/lib/docker:rw
    - /var/lib/kubelet/:/var/lib/kubelet:rw
    - /var/run:/var/run:rw
  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']

logs:
Attaching to aiok8s_master_1
server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.
server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
plugins.go:71] No cloud provider specified.
manager.go:128] cAdvisor running in container: ""/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177""
fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]
machine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory
manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}
manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}
server.go:636] Adding manifest file: etc/kubernetes/manifests
server.go:646] Watching apiserver
manager.go:191] Setting dockerRoot to /var/lib/docker
plugins.go:56] Registering credential provider: .dockercfg
server.go:608] Started kubelet
kubelet.go:868] Image garbage collection failed: unable to find data for container /
server.go:104] Starting to listen on 0.0.0.0:10250
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
kubelet.go:898] Running in container ""/kubelet""
manager.go:124] Starting to sync pod status with apiserver
kubelet.go:2246] Starting kubelet main sync loop.
kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
factory.go:245] Registering Docker factory
factory.go:94] Registering Raw factory
manager.go:1005] Started watching for new ooms in manager
oomparser.go:198] OOM parser using kernel log file: ""/var/log/kern.log""
manager.go:249] Starting recovery of all containers
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:254] Recovery completed
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available",Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserver,"Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserverFollowing the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a ""no cloud provider specified"" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it
I'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.
Here is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )

default
docker-compose file:
master:
  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6
  net: host
  pid: host
  privileged: true
  volumes:
    - /:/rootfs:ro
    - /sys:/sys:ro
    - /dev:/dev
    - /var/lib/docker/:/var/lib/docker:rw
    - /var/lib/kubelet/:/var/lib/kubelet:rw
    - /var/run:/var/run:rw
  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']

logs:
Attaching to aiok8s_master_1
[36mmaster_1 | [0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)
[36mmaster_1 | [0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)
[36mmaster_1 | [0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.
[36mmaster_1 | [0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
[36mmaster_1 | [0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.
[36mmaster_1 | [0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: """" from the config file: """"
[36mmaster_1 | [0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: ""/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e""
[36mmaster_1 | [0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]
[36mmaster_1 | [0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in ""/rootfs/etc/machine-id,/var/lib/dbus/machine-id""
[36mmaster_1 | [0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}
[36mmaster_1 | [0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}
[36mmaster_1 | [0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet
[36mmaster_1 | [0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.
[36mmaster_1 | [0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests
[36mmaster_1 | [0mI0202 21:15:54.496911    6562 file.go:47] Watching path ""etc/kubernetes/manifests""
[36mmaster_1 | [0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver
[36mmaster_1 | [0mI0202 21:15:54.497132    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/etcd.json""
[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/services?resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0
[36mmaster_1 | [0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0  in 1 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.502225    6562 common.go:52] Generated UID ""e171033ec56ced6f29a4417f8b61cbf0"" pod ""k8s-etcd"" from etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.502880    6562 common.go:56] Generated Name ""k8s-etcd-127.0.0.1"" for UID ""e171033ec56ced6f29a4417f8b61cbf0"" from URL etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.502892    6562 common.go:61] Using namespace ""default"" for pod ""k8s-etcd-127.0.0.1"" from etc/kubernetes/manifests/etcd.json
[36mmaster_1 | [0mI0202 21:15:54.503109    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/kube-proxy.json""
[36mmaster_1 | [0mI0202 21:15:54.503393    6562 common.go:52] Generated UID ""e513e62ea641585218de8b3495fc7a21"" pod ""k8s-proxy"" from etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503419    6562 common.go:56] Generated Name ""k8s-proxy-127.0.0.1"" for UID ""e513e62ea641585218de8b3495fc7a21"" from URL etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503429    6562 common.go:61] Using namespace ""default"" for pod ""k8s-proxy-127.0.0.1"" from etc/kubernetes/manifests/kube-proxy.json
[36mmaster_1 | [0mI0202 21:15:54.503514    6562 file.go:135] Reading config file ""etc/kubernetes/manifests/master.json""
[36mmaster_1 | [0mI0202 21:15:54.503983    6562 common.go:52] Generated UID ""eaf8ef5e21f965406a198841c5faa403"" pod ""k8s-master"" from etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504018    6562 common.go:56] Generated Name ""k8s-master-127.0.0.1"" for UID ""eaf8ef5e21f965406a198841c5faa403"" from URL etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504029    6562 common.go:61] Using namespace ""default"" for pod ""k8s-master-127.0.0.1"" from etc/kubernetes/manifests/master.json
[36mmaster_1 | [0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file
[36mmaster_1 | [0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)""
[36mmaster_1 | [0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)""
[36mmaster_1 | [0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0  in 3 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker
[36mmaster_1 | [0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg
[36mmaster_1 | [0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/aws-ebs""
[36mmaster_1 | [0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/empty-dir""
[36mmaster_1 | [0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/gce-pd""
[36mmaster_1 | [0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/git-repo""
[36mmaster_1 | [0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/host-path""
[36mmaster_1 | [0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/nfs""
[36mmaster_1 | [0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/secret""
[36mmaster_1 | [0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/iscsi""
[36mmaster_1 | [0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/glusterfs""
[36mmaster_1 | [0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/persistent-claim""
[36mmaster_1 | [0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/rbd""
[36mmaster_1 | [0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/cinder""
[36mmaster_1 | [0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/cephfs""
[36mmaster_1 | [0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/downward-api""
[36mmaster_1 | [0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/fc""
[36mmaster_1 | [0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin ""kubernetes.io/flocker""
[36mmaster_1 | [0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet
[36mmaster_1 | [0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250
[36mmaster_1 | [0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /
[36mmaster_1 | [0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {""kind"":""Event"",""apiVersion"":""v1"",""metadata"":{""name"":""127.0.0.1.142f3c86e792e0e6"",""namespace"":""default"",""creationTimestamp"":null},""involvedObject"":{""kind"":""Node"",""name"":""127.0.0.1"",""uid"":""127.0.0.1""},""reason"":""Starting"",""message"":""Starting kubelet."",""source"":{""component"":""kubelet"",""host"":""127.0.0.1""},""firstTimestamp"":""2016-02-02T21:15:54Z"",""lastTimestamp"":""2016-02-02T21:15:54Z"",""count"":1,""type"":""Normal""}
[36mmaster_1 | [0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" -H ""Content-Type: application/json"" http://localhost:8080/api/v1/namespaces/default/events
[36mmaster_1 | [0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255
[36mmaster_1 | [0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:""Node"", Namespace:"""", Name:""127.0.0.1"", UID:""127.0.0.1"", APIVersion:"""", ResourceVersion:"""", FieldPath:""""}): type: 'Normal' reason: 'Starting' Starting kubelet.
[36mmaster_1 | [0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
[36mmaster_1 | [0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container ""/kubelet""
[36mmaster_1 | [0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331
[36mmaster_1 | [0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver
[36mmaster_1 | [0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.
[36mmaster_1 | [0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting
[36mmaster_1 | [0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, ""file""): """"
[36mmaster_1 | [0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -> running
[36mmaster_1 | [0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -> exited
[36mmaster_1 | [0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f""}
[36mmaster_1 | [0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, ""file""): ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18""}
[36mmaster_1 | [0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)""
[36mmaster_1 | [0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)""
[36mmaster_1 | [0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)""
[36mmaster_1 | [0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-proxy-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default"",""uid"":""e513e62ea641585218de8b3495fc7a21"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""e513e62ea641585218de8b3495fc7a21"",""kubernetes.io/config.mirror"":""e513e62ea641585218de8b3495fc7a21"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504500123Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""containers"":[{""name"":""kube-proxy"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""proxy"",""--master=http://127.0.0.1:8080"",""--v=2"",""--resource-container=\""\""""],""resources"":{},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent"",""securityContext"":{""privileged"":true}}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-etcd-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default"",""uid"":""e171033ec56ced6f29a4417f8b61cbf0"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""e171033ec56ced6f29a4417f8b61cbf0"",""kubernetes.io/config.mirror"":""e171033ec56ced6f29a4417f8b61cbf0"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504450902Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""volumes"":[{""name"":""varetcd"",""emptyDir"":{}}],""containers"":[{""name"":""etcd"",""image"":""gcr.io/google_containers/etcd:2.2.1"",""command"":[""/usr/local/bin/etcd"",""--listen-client-urls=http://127.0.0.1:4001"",""--advertise-client-urls=http://127.0.0.1:4001"",""--data-dir=/var/etcd/data""],""resources"":{},""volumeMounts"":[{""name"":""varetcd"",""mountPath"":""/var/etcd""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" -H ""Content-Type: application/json"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""k8s-master-127.0.0.1"",""namespace"":""default"",""selfLink"":""/api/v1/pods/namespaces/k8s-master-127.0.0.1/default"",""uid"":""eaf8ef5e21f965406a198841c5faa403"",""creationTimestamp"":null,""annotations"":{""kubernetes.io/config.hash"":""eaf8ef5e21f965406a198841c5faa403"",""kubernetes.io/config.mirror"":""eaf8ef5e21f965406a198841c5faa403"",""kubernetes.io/config.seen"":""2016-02-02T21:15:54.504513882Z"",""kubernetes.io/config.source"":""file""}},""spec"":{""volumes"":[{""name"":""data"",""emptyDir"":{}}],""containers"":[{""name"":""controller-manager"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""controller-manager"",""--master=127.0.0.1:8080"",""--min-resync-period=3m"",""--service-account-private-key-file=/srv/kubernetes/server.key"",""--root-ca-file=/srv/kubernetes/ca.crt"",""--v=2""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/srv/kubernetes""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""apiserver"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""apiserver"",""--service-cluster-ip-range=10.0.0.1/24"",""--insecure-bind-address=127.0.0.1"",""--etcd-servers=http://127.0.0.1:4001"",""--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota"",""--client-ca-file=/srv/kubernetes/ca.crt"",""--basic-auth-file=/srv/kubernetes/basic_auth.csv"",""--min-request-timeout=300"",""--tls-cert-file=/srv/kubernetes/server.cert"",""--tls-private-key-file=/srv/kubernetes/server.key"",""--token-auth-file=/srv/kubernetes/known_tokens.csv"",""--allow-privileged=true"",""--v=4""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/srv/kubernetes""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""scheduler"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/hyperkube"",""scheduler"",""--master=127.0.0.1:8080"",""--v=2""],""resources"":{},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""},{""name"":""setup"",""image"":""gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6"",""command"":[""/setup-files.sh""],""resources"":{},""volumeMounts"":[{""name"":""data"",""mountPath"":""/data""}],""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""IfNotPresent""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeName"":""127.0.0.1"",""hostNetwork"":true,""securityContext"":{}},""status"":{}}
[36mmaster_1 | [0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""Content-Type: application/json"" -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H ""Content-Type: application/json"" -H ""User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af"" http://localhost:8080/api/v1/namespaces/default/pods
[36mmaster_1 | [0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds
[36mmaster_1 | [0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:
[36mmaster_1 | [0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
[36mmaster_1 | [0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available
[36mmaster_1 | [0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin ""kubernetes.io/empty-dir"" for varetcd
[36mmaster_1 | [0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]
[36mmaster_1 | [0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1
[36mmaster_1 | [0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin ""kubernetes.io/empty-dir"" for data
[36mmaster_1 | [0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]
[36mmaster_1 | [0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1
[36mmaster_1 | [0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05""}
[36mmaster_1 | [0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:<nil> NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=""""] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.
[36mmaster_1 | [0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"", event: &pleg.PodLifecycleEvent{ID:""eaf8ef5e21f965406a198841c5faa403"", Type:""ContainerDied"", Data:""1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28""}
[36mmaster_1 | [0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:<nil> NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}
[36mmaster_1 | [0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod ""k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)"": &{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=""""] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:<nil> ReadinessProbe:<nil> Lifecycle:<nil> TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:<nil> ContainerStatuses:[]}}


with nsenter modification
docker-compose file:
master:
  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6
  net: host
  pid: host
  privileged: true
  volumes:
    - /sys:/sys:ro
    - /dev:/dev
    - /var/lib/docker/:/var/lib/docker:rw
    - /var/lib/kubelet/:/var/lib/kubelet:rw
    - /var/run:/var/run:rw
  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']

logs:
Attaching to aiok8s_master_1
server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.
server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
plugins.go:71] No cloud provider specified.
manager.go:128] cAdvisor running in container: ""/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177""
fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]
machine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory
manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}
manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}
server.go:636] Adding manifest file: etc/kubernetes/manifests
server.go:646] Watching apiserver
manager.go:191] Setting dockerRoot to /var/lib/docker
plugins.go:56] Registering credential provider: .dockercfg
server.go:608] Started kubelet
kubelet.go:868] Image garbage collection failed: unable to find data for container /
server.go:104] Starting to listen on 0.0.0.0:10250
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
kubelet.go:898] Running in container ""/kubelet""
manager.go:124] Starting to sync pod status with apiserver
kubelet.go:2246] Starting kubelet main sync loop.
kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
factory.go:245] Registering Docker factory
factory.go:94] Registering Raw factory
manager.go:1005] Started watching for new ooms in manager
oomparser.go:198] OOM parser using kernel log file: ""/var/log/kern.log""
manager.go:249] Starting recovery of all containers
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:254] Recovery completed
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available
manager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
manager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)
pod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
manager.go:390] Failed to update status for pod ""_()"": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1621] Failed creating a mirror pod for ""k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)"": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused
kubelet.go:1626] Mirror pod not available",3
,2669,144,89,15910,"For ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.
Links work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).
Looks like a bug in htmlpreview. Need to figure out a way to fix this.
cc @caesarxuchao",Cross links dont work in api reference docs when viewed using htmlpreview,"Cross links dont work in api reference docs when viewed using htmlpreviewFor ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.
Links work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).
Looks like a bug in htmlpreview. Need to figure out a way to fix this.
cc @caesarxuchao",3
,2670,148,90,19416,Part of #19413,Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zone,Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zonePart of #19413,3
,2671,148,91,6265,For max consistency.  Some String() methods might hash collide.,DeepHashObject should probably set DisableMethods and DisablePointerMethods for spew,DeepHashObject should probably set DisableMethods and DisablePointerMethods for spewFor max consistency.  Some String() methods might hash collide.,3
,2672,148,92,6667,"Hi There
I have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.
E.g

When PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)
When redis slave try to synch with master using redis-master , it doesnt work.
When I try and ping redis-master from inside the container, it comes back with unknown host error.

Some logs to show you the problem
[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379
[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379
Temporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141
I do see Kube-Proxy is making the right iptable entries.
I0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.84:8000
I0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.84:8000
Do you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.
What might be going wrong here ?",Service Name Resolution is not working,"Service Name Resolution is not workingHi There
I have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.
E.g

When PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)
When redis slave try to synch with master using redis-master , it doesnt work.
When I try and ping redis-master from inside the container, it comes back with unknown host error.

Some logs to show you the problem
[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379
[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379
Temporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141
I do see Kube-Proxy is making the right iptable entries.
I0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.84:8000
I0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.84:8000
Do you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.
What might be going wrong here ?",3
,2673,148,93,13633,"I took the last version from github several days ago, installed without any problems
host machine
[root@linux-c56a cluster]$ ./kubectl.sh version
Client Version: version.Info{Major:""1"", Minor:""1+"", GitVersion:""v1.1.0-alpha.1"", GitCommit:""9bae6636d5fcf436a3b14d219ced195e10e21fe8"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""1+"", GitVersion:""v1.1.0-alpha.1"", GitCommit:""9bae6636d5fcf436a3b14d219ced195e10e21fe8"", GitTreeState:""clean""}

minion-1, everything is fine
[root@kubernetes-minion-1 vagrant]# docker ps
CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES
6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    ""/kuisp -p 8080 -c /o""   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681
c8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   ""/run.sh""                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b
097522fb0214        gcr.io/google_containers/kube2sky:1.11            ""/kube2sky -domain=cl""   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6
cd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               ""/usr/local/bin/etcd ""   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01
b2065f47fd00        gcr.io/google_containers/exechealthz:1.0          ""/exechealthz '-cmd=n""   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982
e328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    ""/skydns -machines=ht""   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84
336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         ""/heapster --source=k""   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb
f9584b644df5        gcr.io/google_containers/kube-ui:v1.1             ""/kube-ui""               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76
ebbe49b70250        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes       0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395
81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1
bc84d79785ae        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc
ec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f

minion-2. Completely empty
[root@kubernetes-minion-2 vagrant]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

minion-3. Completely empty
[root@kubernetes-minion-3 vagrant]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

ok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting
[root@linux-c56a cluster]$ ./kubectl.sh get pods
NAME                   READY     STATUS    RESTARTS   AGE
redis-1btiw            0/1       Pending   0          48s
redis-1lfi0            0/1       Pending   0          4m
redis-333g6            0/1       Pending   0          3m
redis-5hpg8            0/1       Pending   0          7m
redis-5lsy5            0/1       Pending   0          17s
redis-7be1q            0/1       Pending   0          4m
redis-9jjwo            0/1       Pending   0          3m
redis-aospw            0/1       Pending   0          48s
redis-c96tg            0/1       Pending   0          14m
redis-eoffa            0/1       Pending   0          7m
redis-exgmi            0/1       Pending   0          11m
redis-fkp2a            0/1       Pending   0          48s
redis-fosf3            0/1       Pending   0          17s
redis-l483e            0/1       Pending   0          3m
redis-pc3bl            0/1       Pending   0          8m
redis-sentinel-4bdy5   0/1       Pending   0          10m
redis-sentinel-4ch8m   0/1       Pending   0          3m
redis-sentinel-59x65   0/1       Pending   0          14m
redis-sentinel-97a10   0/1       Pending   0          3m
redis-sentinel-abw3f   0/1       Pending   0          11m
redis-sentinel-bjyth   0/1       Pending   0          7m
redis-sentinel-c94br   0/1       Pending   0          3m
redis-sentinel-chkmu   0/1       Pending   0          7m
redis-sentinel-dtc89   0/1       Pending   0          7m
redis-sentinel-fxzrq   0/1       Pending   0          17s
redis-sentinel-h4ixm   0/1       Pending   0          11m
redis-sentinel-hph84   0/1       Pending   0          11m
redis-sentinel-j1svl   0/1       Pending   0          7m
redis-sentinel-kfx09   0/1       Pending   0          17s
redis-sentinel-mp18j   0/1       Pending   0          7m
redis-sentinel-t9t0v   0/1       Pending   0          10m
redis-sentinel-tkyh9   0/1       Pending   0          3m
redis-sentinel-udu4k   0/1       Pending   0          17s
redis-sentinel-ukv95   0/1       Pending   0          10m
redis-sentinel-xrniw   0/1       Pending   0          7m
redis-sentinel-yhwad   0/1       Pending   0          3m
redis-sentinel-ysawg   0/1       Pending   0          3m
redis-slcg7            0/1       Pending   0          7m
redis-spati            0/1       Pending   0          17s
redis-taasu            0/1       Pending   0          11m
redis-v6vif            0/1       Pending   0          8m
redis-yqlbl            0/1       Pending   0          4m
redis-yvc2u            0/1       Pending   0          11m
redis-zkktb            0/1       Pending   0          8m",vagrant-based cluster is broken,"vagrant-based cluster is brokenI took the last version from github several days ago, installed without any problems
host machine
[root@linux-c56a cluster]$ ./kubectl.sh version
Client Version: version.Info{Major:""1"", Minor:""1+"", GitVersion:""v1.1.0-alpha.1"", GitCommit:""9bae6636d5fcf436a3b14d219ced195e10e21fe8"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""1+"", GitVersion:""v1.1.0-alpha.1"", GitCommit:""9bae6636d5fcf436a3b14d219ced195e10e21fe8"", GitTreeState:""clean""}

minion-1, everything is fine
[root@kubernetes-minion-1 vagrant]# docker ps
CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES
6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    ""/kuisp -p 8080 -c /o""   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681
c8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   ""/run.sh""                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b
097522fb0214        gcr.io/google_containers/kube2sky:1.11            ""/kube2sky -domain=cl""   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6
cd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               ""/usr/local/bin/etcd ""   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01
b2065f47fd00        gcr.io/google_containers/exechealthz:1.0          ""/exechealthz '-cmd=n""   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982
e328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    ""/skydns -machines=ht""   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84
336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         ""/heapster --source=k""   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb
f9584b644df5        gcr.io/google_containers/kube-ui:v1.1             ""/kube-ui""               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76
ebbe49b70250        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes       0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395
81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1
bc84d79785ae        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc
ec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              ""/pause""                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f

minion-2. Completely empty
[root@kubernetes-minion-2 vagrant]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

minion-3. Completely empty
[root@kubernetes-minion-3 vagrant]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

ok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting
[root@linux-c56a cluster]$ ./kubectl.sh get pods
NAME                   READY     STATUS    RESTARTS   AGE
redis-1btiw            0/1       Pending   0          48s
redis-1lfi0            0/1       Pending   0          4m
redis-333g6            0/1       Pending   0          3m
redis-5hpg8            0/1       Pending   0          7m
redis-5lsy5            0/1       Pending   0          17s
redis-7be1q            0/1       Pending   0          4m
redis-9jjwo            0/1       Pending   0          3m
redis-aospw            0/1       Pending   0          48s
redis-c96tg            0/1       Pending   0          14m
redis-eoffa            0/1       Pending   0          7m
redis-exgmi            0/1       Pending   0          11m
redis-fkp2a            0/1       Pending   0          48s
redis-fosf3            0/1       Pending   0          17s
redis-l483e            0/1       Pending   0          3m
redis-pc3bl            0/1       Pending   0          8m
redis-sentinel-4bdy5   0/1       Pending   0          10m
redis-sentinel-4ch8m   0/1       Pending   0          3m
redis-sentinel-59x65   0/1       Pending   0          14m
redis-sentinel-97a10   0/1       Pending   0          3m
redis-sentinel-abw3f   0/1       Pending   0          11m
redis-sentinel-bjyth   0/1       Pending   0          7m
redis-sentinel-c94br   0/1       Pending   0          3m
redis-sentinel-chkmu   0/1       Pending   0          7m
redis-sentinel-dtc89   0/1       Pending   0          7m
redis-sentinel-fxzrq   0/1       Pending   0          17s
redis-sentinel-h4ixm   0/1       Pending   0          11m
redis-sentinel-hph84   0/1       Pending   0          11m
redis-sentinel-j1svl   0/1       Pending   0          7m
redis-sentinel-kfx09   0/1       Pending   0          17s
redis-sentinel-mp18j   0/1       Pending   0          7m
redis-sentinel-t9t0v   0/1       Pending   0          10m
redis-sentinel-tkyh9   0/1       Pending   0          3m
redis-sentinel-udu4k   0/1       Pending   0          17s
redis-sentinel-ukv95   0/1       Pending   0          10m
redis-sentinel-xrniw   0/1       Pending   0          7m
redis-sentinel-yhwad   0/1       Pending   0          3m
redis-sentinel-ysawg   0/1       Pending   0          3m
redis-slcg7            0/1       Pending   0          7m
redis-spati            0/1       Pending   0          17s
redis-taasu            0/1       Pending   0          11m
redis-v6vif            0/1       Pending   0          8m
redis-yqlbl            0/1       Pending   0          4m
redis-yvc2u            0/1       Pending   0          11m
redis-zkktb            0/1       Pending   0          8m",3
,2674,148,94,25792,"I am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar
After setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.

You now need to edit the docker configuration to activate new flags. Again, this is system specific.
This may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.
Regardless, you need to add the following to the docker command line:
--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}

And after I ran
ps aux | grep docker

I found my docker is already started with the right options:
root     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972

But when I tried to run hello-world procedure with the following command
sudo docker run hello-world

The error occurs
docker: Error response from daemon: Container command '/hello' not found or does not exist..

BTW, I can run hello-world find with --bip & --mtu removed.
Any kind of help will be appreciated :)",failed to set up cluster via docker,"failed to set up cluster via dockerI am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar
After setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.

You now need to edit the docker configuration to activate new flags. Again, this is system specific.
This may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.
Regardless, you need to add the following to the docker command line:
--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}

And after I ran
ps aux | grep docker

I found my docker is already started with the right options:
root     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972

But when I tried to run hello-world procedure with the following command
sudo docker run hello-world

The error occurs
docker: Error response from daemon: Container command '/hello' not found or does not exist..

BTW, I can run hello-world find with --bip & --mtu removed.
Any kind of help will be appreciated :)",3
,2675,142,95,32991,"Problem: the following codes just hang and never returns
 clientset, err := kubernetes.NewForConfig(config)
 pods, err := clientset.Core().Pods("""").List(api.ListOptions{Watch:true})

while codes like:
watch, err := clientset.Core().Pods("""").Watch(api.ListOptions{})

work as expected.
IMHO,  the first use is kind of misleading.

Is this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
What keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):

Is this a BUG REPORT or FEATURE REQUEST? (choose one):

Kubernetes version (use kubectl version):
Environment:

Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release):
Kernel (e.g. uname -a):
Install tools:
Others:

What happened:
What you expected to happen:
How to reproduce it (as minimally and precisely as possible):
Anything else do we need to know:","when using client-go library to communicate with master, client list pods with watch hang forever","when using client-go library to communicate with master, client list pods with watch hang foreverProblem: the following codes just hang and never returns
 clientset, err := kubernetes.NewForConfig(config)
 pods, err := clientset.Core().Pods("""").List(api.ListOptions{Watch:true})

while codes like:
watch, err := clientset.Core().Pods("""").Watch(api.ListOptions{})

work as expected.
IMHO,  the first use is kind of misleading.

Is this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
What keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):

Is this a BUG REPORT or FEATURE REQUEST? (choose one):

Kubernetes version (use kubectl version):
Environment:

Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release):
Kernel (e.g. uname -a):
Install tools:
Others:

What happened:
What you expected to happen:
How to reproduce it (as minimally and precisely as possible):
Anything else do we need to know:",3
,2676,148,96,10983,I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570  but I am not finding enough resources if it can.,--cpuset-cpus supported in kubernetes?,--cpuset-cpus supported in kubernetes?I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570  but I am not finding enough resources if it can.,3
,2677,144,97,22842,"I tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.
Here is my environment setting:
export KUBERNETES_PROVIDER=gce
export KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud
export KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304
export KUBE_OS_DISTRIBUTION=trusty
./cluster/kube-up.sh

Am I missing something? CC @dchen1107
By the way, this problem has also been confirmed by @Random-Liu .",Cluster failed to initialize on GCE,"Cluster failed to initialize on GCEI tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.
Here is my environment setting:
export KUBERNETES_PROVIDER=gce
export KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud
export KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304
export KUBE_OS_DISTRIBUTION=trusty
./cluster/kube-up.sh

Am I missing something? CC @dchen1107
By the way, this problem has also been confirmed by @Random-Liu .",3
,2678,146,98,14385,"Currently a job's .spec.parallelism defaults to 2.  @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).
The current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from .spec.completions.  And people will ask why it was
I don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",Default Job Parallelism from Completions,"Default Job Parallelism from CompletionsCurrently a job's .spec.parallelism defaults to 2.  @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).
The current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from .spec.completions.  And people will ask why it was
I don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",3
,2679,141,99,28144,"I am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?
Thank you!
service ip

slave container in ping master service ip:

master container in ping slave service ip:

new start service","About redis from the main issue, I can not connect master service ip connection pod slave","About redis from the main issue, I can not connect master service ip connection pod slaveI am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?
Thank you!
service ip

slave container in ping master service ip:

master container in ping slave service ip:

new start service",3
,2680,144,0,14452,"Steps to Reproduce
The end goal is to create a scrolling list that automatically ""snaps"" to a given location based on the actual scroll position at which the user stops scrolling. This may not be the best solution (I'm new to Flutter), but here's the process by which I get the crash:
Wrap a ListView within a NotificationListener. Upon receiving a UserScrollNotification with ScrollDirection.idle, call the ScrollController's animateTo() method to scroll to another location (in the example here, I arbitrarily scroll to 10000.0 pixels).
Sample code
import 'package:flutter/material.dart';

void main() => runApp(new MyApp());

class MyApp extends StatelessWidget {
  // This widget is the root of your application.
  @override
  Widget build(BuildContext context) {
    return new MaterialApp(
      title: 'Scroller issue',
      home: new MyHomePage(),
    );
  }
}

class MyHomePage extends StatelessWidget {
  final _controller = new ScrollController();

  bool _didGetNotification(ScrollNotification notification) {
    if (notification is UserScrollNotification) {
      if (notification.direction.toString() == ""ScrollDirection.idle"") {
        // We've stopped scrolling... now animate automatically to the 10000-pixel spot
        _controller.animateTo(10000.0, duration: const Duration(seconds: 2), curve: Curves.elasticOut);

        /* Above works great... unless the user tries to interact with the list WHILE it's
        // animating. In this case, you end up with:
        **********************************************************************************
        Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart':
          Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
        ********************************************************************************** */
      }
    }
    return true;
  }

  @override
  Widget build(BuildContext context) {
    return new Scaffold(
      body: new NotificationListener(
        onNotification: _didGetNotification,
        child: new ListView.builder(
          padding: new EdgeInsets.all(8.0),
          controller: _controller,
          itemExtent: 60.0,
          itemBuilder: (BuildContext context, int index) {
            return new Text('Item No. $index');
          },
        ),
      ),
    );
  }
}
It works great as long as the user only interacts with the ListView when it is idle, but if the user attempts to scroll/drag/touch the list during the animateTo() process, the following exception is raised within the Flutter Scrollable package:
'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.

After this, the ListView becomes unresponsive.
I'm sure there's a better way to achieve what I'm trying to do, but the platform itself doesn't seem to recover from this. Figured I'd make a note of it.
Thanks!
Logs
Run your application with flutter run and attach all the log output.
    CADisplay.name = LCD;
    CADisplay.deviceName = PurpleMain;
    CADisplay.seed = 1;
    tags = 0;
    currentMode = <FBSDisplayMode: 0x604000097430; 375x667@2x (750x1334/2) 60Hz sRGB SDR>;
    safeOverscanRatio = {0.89999997615814209, 0.89999997615814209};
    nativeCenter = {375, 667};
    pixelSize = {750, 1334};
    bounds = {{0, 0}, {375, 667}};
    CADisplay = <CADisplay:LCD PurpleMain>;
}
Syncing files to device iPhone 6s...                  1.9s

  To hot reload your app on the fly, press ""r"". To restart the app entirely, press ""R"".
An Observatory debugger and profiler on iPhone 6s is available at: http://127.0.0.1:8100/
For a more detailed help message, press ""h"". To quit, press ""q"".
 EXCEPTION CAUGHT BY GESTURE 
The following assertion was thrown while handling a gesture:
'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 467 pos 12: '_hold == null':
is not true.

Either the assertion indicates an error in the framework itself, or we should provide substantially
more information in this error message to help you determine and fix the underlying cause.
In either case, please report this assertion by filing a bug on GitHub:
https://github.com/flutter/flutter/issues/new

When the exception was thrown, this was the stack:
#2      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:467:12)
#3      DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:54)
#4      GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)
#5      DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)
#6      GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)
#7      GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)
(elided 4 frames from class _AssertionError and package dart:async)

Handler: onStart
Recognizer:
VerticalDragGestureRecognizer#6eafd

Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 455 pos 12: '_drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 464 pos 12: '_drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.


Run flutter analyze and attach any output of that command also.
Analyzing /Users/mfahy/Apps/list_view_attempts...
No issues found!
Ran in 6.5s

Flutter Doctor
[  +21 ms] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref --symbolic @{u}
[  +36 ms] Exit code 0 from: git rev-parse --abbrev-ref --symbolic @{u}
[        ] origin/alpha
[        ] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref HEAD
[   +7 ms] Exit code 0 from: git rev-parse --abbrev-ref HEAD
[        ] alpha
[        ] [/Users/mfahy/flutter/] git ls-remote --get-url origin
[   +9 ms] Exit code 0 from: git ls-remote --get-url origin
[        ] https://github.com/flutter/flutter.git
[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%H
[  +28 ms] Exit code 0 from: git log -n 1 --pretty=format:%H
[        ] 2e449f06f0a3be076e336ad6b30b0e9ec99dbdfe
[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%ar
[  +10 ms] Exit code 0 from: git log -n 1 --pretty=format:%ar
[        ] 5 days ago
[        ] [/Users/mfahy/flutter/] git describe --match v*.*.* --first-parent --long --tags
[  +44 ms] Exit code 0 from: git describe --match v*.*.* --first-parent --long --tags
[        ] v0.0.21-0-g2e449f06f
[ +473 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[+1263 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[        ] 3.0
[ +452 ms] [] Flutter (on Mac OS X 10.13.3 17D47, locale en-US, channel alpha)
[   +1 ms]      Flutter version 0.0.21 at /Users/mfahy/flutter
[        ]      Framework revision 2e449f06f0 (5 days ago), 2018-01-29 14:26:51 -0800
[        ]      Engine revision 6921873c71
[        ]      Tools Dart version 2.0.0-dev.16.0
[        ]      Engine Dart version 2.0.0-edge.da1f52592ef73fe3afa485385cb995b9aec0181a
[ +130 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[ +218 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[        ] 3.0
[  +98 ms] java -version
[  +87 ms] [] Android toolchain - develop for Android devices (Android SDK 27.0.3)
[        ]      Android SDK at /Users/mfahy/Library/Android/sdk
[        ]      Android NDK location not configured (optional; useful for native profiling support)
[        ]      Platform android-27, build-tools 27.0.3
[        ]      Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
[        ]      Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)
[+1189 ms] DevToolsSecurity -status
[  +49 ms] Developer mode is currently enabled.
[        ] python -c import six
[ +126 ms] idevice_id -h
[  +10 ms] idevice_id -h
[   +9 ms] idevice_id -l
[  +28 ms] 5a62f0e8de345b9794b9a61c9bfcd02026c68a86
[   +1 ms] idevicename
[  +44 ms] ios-deploy --version
[  +54 ms] ios-deploy --version
[  +16 ms] 1.9.2
[   +1 ms] ios-deploy --version
[  +33 ms] ios-deploy --version
[  +21 ms] 1.9.2
[   +2 ms] pod --version
[ +905 ms] pod --version
[ +605 ms] 1.3.1
[   +2 ms] pod --version
[ +533 ms] 1.3.1
[   +1 ms] [-] iOS toolchain - develop for iOS devices (Xcode 9.2)
[        ]      Xcode at /Applications/Xcode.app/Contents/Developer
[        ]      Xcode 9.2, Build version 9C40b
[        ]      Verify that all connected devices have been paired with this computer in Xcode.
                 If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.
                 To update, run:
                   brew uninstall --ignore-dependencies libimobiledevice
                   brew install --HEAD libimobiledevice
                   brew install ideviceinstaller
[        ]      ios-deploy 1.9.2
[        ]      CocoaPods version 1.3.1
[   +2 ms] [] Android Studio (version 3.0)
[        ]      Android Studio at /Applications/Android Studio.app/Contents
[        ]      Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)
[   +6 ms] /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString
[ +249 ms] Exit code 0 from: /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString
[        ] 2017.3.4
[  +79 ms] [] IntelliJ IDEA Ultimate Edition (version 2017.3.4)
[        ]      Flutter plugin version 21.2.3
[        ]      Dart plugin version 173.4548.30
[   +4 ms] /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l
[  +17 ms] Exit code 0 from: /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l
[        ] List of devices attached
[   +8 ms] idevice_id -h
[  +82 ms] which ideviceinstaller
[   +5 ms] Exit code 0 from: which ideviceinstaller
[        ] /usr/local/bin/ideviceinstaller
[        ] which iproxy
[   +4 ms] Exit code 0 from: which iproxy
[        ] /usr/local/bin/iproxy
[   +4 ms] /usr/bin/xcrun simctl list --json devices
[ +218 ms] [] Connected devices
[        ]      ViPhone 6S  5a62f0e8de345b9794b9a61c9bfcd02026c68a86  ios  iOS 11.3
[        ]      iPhone 6s   6F36E8FD-E570-453E-B943-8243E4FAEDD6      ios  iOS 11.2 (simulator)
[  +21 ms] ""flutter doctor"" took 6,972ms.
[  +42 ms] ensureAnalyticsSent: 38ms
[   +2 ms] exiting with code 0

Hope this helps! Thanks!",Dragging a list that is currently handling an animateTo animation throws exception,"Dragging a list that is currently handling an animateTo animation throws exceptionSteps to Reproduce
The end goal is to create a scrolling list that automatically ""snaps"" to a given location based on the actual scroll position at which the user stops scrolling. This may not be the best solution (I'm new to Flutter), but here's the process by which I get the crash:
Wrap a ListView within a NotificationListener. Upon receiving a UserScrollNotification with ScrollDirection.idle, call the ScrollController's animateTo() method to scroll to another location (in the example here, I arbitrarily scroll to 10000.0 pixels).
Sample code
import 'package:flutter/material.dart';

void main() => runApp(new MyApp());

class MyApp extends StatelessWidget {
  // This widget is the root of your application.
  @override
  Widget build(BuildContext context) {
    return new MaterialApp(
      title: 'Scroller issue',
      home: new MyHomePage(),
    );
  }
}

class MyHomePage extends StatelessWidget {
  final _controller = new ScrollController();

  bool _didGetNotification(ScrollNotification notification) {
    if (notification is UserScrollNotification) {
      if (notification.direction.toString() == ""ScrollDirection.idle"") {
        // We've stopped scrolling... now animate automatically to the 10000-pixel spot
        _controller.animateTo(10000.0, duration: const Duration(seconds: 2), curve: Curves.elasticOut);

        /* Above works great... unless the user tries to interact with the list WHILE it's
        // animating. In this case, you end up with:
        **********************************************************************************
        Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart':
          Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
        ********************************************************************************** */
      }
    }
    return true;
  }

  @override
  Widget build(BuildContext context) {
    return new Scaffold(
      body: new NotificationListener(
        onNotification: _didGetNotification,
        child: new ListView.builder(
          padding: new EdgeInsets.all(8.0),
          controller: _controller,
          itemExtent: 60.0,
          itemBuilder: (BuildContext context, int index) {
            return new Text('Item No. $index');
          },
        ),
      ),
    );
  }
}
It works great as long as the user only interacts with the ListView when it is idle, but if the user attempts to scroll/drag/touch the list during the animateTo() process, the following exception is raised within the Flutter Scrollable package:
'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.

After this, the ListView becomes unresponsive.
I'm sure there's a better way to achieve what I'm trying to do, but the platform itself doesn't seem to recover from this. Figured I'd make a note of it.
Thanks!
Logs
Run your application with flutter run and attach all the log output.
    CADisplay.name = LCD;
    CADisplay.deviceName = PurpleMain;
    CADisplay.seed = 1;
    tags = 0;
    currentMode = <FBSDisplayMode: 0x604000097430; 375x667@2x (750x1334/2) 60Hz sRGB SDR>;
    safeOverscanRatio = {0.89999997615814209, 0.89999997615814209};
    nativeCenter = {375, 667};
    pixelSize = {750, 1334};
    bounds = {{0, 0}, {375, 667}};
    CADisplay = <CADisplay:LCD PurpleMain>;
}
Syncing files to device iPhone 6s...                  1.9s

  To hot reload your app on the fly, press ""r"". To restart the app entirely, press ""R"".
An Observatory debugger and profiler on iPhone 6s is available at: http://127.0.0.1:8100/
For a more detailed help message, press ""h"". To quit, press ""q"".
 EXCEPTION CAUGHT BY GESTURE 
The following assertion was thrown while handling a gesture:
'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 467 pos 12: '_hold == null':
is not true.

Either the assertion indicates an error in the framework itself, or we should provide substantially
more information in this error message to help you determine and fix the underlying cause.
In either case, please report this assertion by filing a bug on GitHub:
https://github.com/flutter/flutter/issues/new

When the exception was thrown, this was the stack:
#2      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:467:12)
#3      DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:54)
#4      GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)
#5      DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)
#6      GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)
#7      GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)
(elided 4 frames from class _AssertionError and package dart:async)

Handler: onStart
Recognizer:
VerticalDragGestureRecognizer#6eafd

Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 455 pos 12: '_drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 464 pos 12: '_drag == null': is not true.
Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.


Run flutter analyze and attach any output of that command also.
Analyzing /Users/mfahy/Apps/list_view_attempts...
No issues found!
Ran in 6.5s

Flutter Doctor
[  +21 ms] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref --symbolic @{u}
[  +36 ms] Exit code 0 from: git rev-parse --abbrev-ref --symbolic @{u}
[        ] origin/alpha
[        ] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref HEAD
[   +7 ms] Exit code 0 from: git rev-parse --abbrev-ref HEAD
[        ] alpha
[        ] [/Users/mfahy/flutter/] git ls-remote --get-url origin
[   +9 ms] Exit code 0 from: git ls-remote --get-url origin
[        ] https://github.com/flutter/flutter.git
[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%H
[  +28 ms] Exit code 0 from: git log -n 1 --pretty=format:%H
[        ] 2e449f06f0a3be076e336ad6b30b0e9ec99dbdfe
[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%ar
[  +10 ms] Exit code 0 from: git log -n 1 --pretty=format:%ar
[        ] 5 days ago
[        ] [/Users/mfahy/flutter/] git describe --match v*.*.* --first-parent --long --tags
[  +44 ms] Exit code 0 from: git describe --match v*.*.* --first-parent --long --tags
[        ] v0.0.21-0-g2e449f06f
[ +473 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[+1263 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[        ] 3.0
[ +452 ms] [] Flutter (on Mac OS X 10.13.3 17D47, locale en-US, channel alpha)
[   +1 ms]      Flutter version 0.0.21 at /Users/mfahy/flutter
[        ]      Framework revision 2e449f06f0 (5 days ago), 2018-01-29 14:26:51 -0800
[        ]      Engine revision 6921873c71
[        ]      Tools Dart version 2.0.0-dev.16.0
[        ]      Engine Dart version 2.0.0-edge.da1f52592ef73fe3afa485385cb995b9aec0181a
[ +130 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[ +218 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString
[        ] 3.0
[  +98 ms] java -version
[  +87 ms] [] Android toolchain - develop for Android devices (Android SDK 27.0.3)
[        ]      Android SDK at /Users/mfahy/Library/Android/sdk
[        ]      Android NDK location not configured (optional; useful for native profiling support)
[        ]      Platform android-27, build-tools 27.0.3
[        ]      Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
[        ]      Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)
[+1189 ms] DevToolsSecurity -status
[  +49 ms] Developer mode is currently enabled.
[        ] python -c import six
[ +126 ms] idevice_id -h
[  +10 ms] idevice_id -h
[   +9 ms] idevice_id -l
[  +28 ms] 5a62f0e8de345b9794b9a61c9bfcd02026c68a86
[   +1 ms] idevicename
[  +44 ms] ios-deploy --version
[  +54 ms] ios-deploy --version
[  +16 ms] 1.9.2
[   +1 ms] ios-deploy --version
[  +33 ms] ios-deploy --version
[  +21 ms] 1.9.2
[   +2 ms] pod --version
[ +905 ms] pod --version
[ +605 ms] 1.3.1
[   +2 ms] pod --version
[ +533 ms] 1.3.1
[   +1 ms] [-] iOS toolchain - develop for iOS devices (Xcode 9.2)
[        ]      Xcode at /Applications/Xcode.app/Contents/Developer
[        ]      Xcode 9.2, Build version 9C40b
[        ]      Verify that all connected devices have been paired with this computer in Xcode.
                 If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.
                 To update, run:
                   brew uninstall --ignore-dependencies libimobiledevice
                   brew install --HEAD libimobiledevice
                   brew install ideviceinstaller
[        ]      ios-deploy 1.9.2
[        ]      CocoaPods version 1.3.1
[   +2 ms] [] Android Studio (version 3.0)
[        ]      Android Studio at /Applications/Android Studio.app/Contents
[        ]      Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)
[   +6 ms] /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString
[ +249 ms] Exit code 0 from: /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString
[        ] 2017.3.4
[  +79 ms] [] IntelliJ IDEA Ultimate Edition (version 2017.3.4)
[        ]      Flutter plugin version 21.2.3
[        ]      Dart plugin version 173.4548.30
[   +4 ms] /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l
[  +17 ms] Exit code 0 from: /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l
[        ] List of devices attached
[   +8 ms] idevice_id -h
[  +82 ms] which ideviceinstaller
[   +5 ms] Exit code 0 from: which ideviceinstaller
[        ] /usr/local/bin/ideviceinstaller
[        ] which iproxy
[   +4 ms] Exit code 0 from: which iproxy
[        ] /usr/local/bin/iproxy
[   +4 ms] /usr/bin/xcrun simctl list --json devices
[ +218 ms] [] Connected devices
[        ]      ViPhone 6S  5a62f0e8de345b9794b9a61c9bfcd02026c68a86  ios  iOS 11.3
[        ]      iPhone 6s   6F36E8FD-E570-453E-B943-8243E4FAEDD6      ios  iOS 11.2 (simulator)
[  +21 ms] ""flutter doctor"" took 6,972ms.
[  +42 ms] ensureAnalyticsSent: 38ms
[   +2 ms] exiting with code 0

Hope this helps! Thanks!",3
,2681,146,1,6651,"We seem to have a documentation gap, between ""Getting Started"" and ""Tour of the Widget Framework"". In a recent UX study, we noticed that the high-level concepts and the unique bits of Flutter aren't clearly called out, which made it more challenging for a new user to wrap their heads around what Flutter is and how it thinks.
Initial thoughts on this doc:

Audience is brand new users for Flutter
Intended for the ""ok, what's this flutter thing? what's different?""
Intended to be the next doc you read, after getting started, and before you start diving in
Fairly short, high level

Initial thoughts on table of contents:

What is Flutter?

what problem(s) does it solve
who is it for
what's special?


Functional-reactive framework

widget lifecycle
setState
Only rebuilding what needs to be rebuilt (efficient)
Customizable/extensible


Material design

full set of widgets out of the box


Dev cycle

Hot reload
Fast restart


Tooling

CLI
IntelliJ


Plugins/interop

coming soon!



Once a new user scans through this doc, they should know

What's special about flutter?
What does functional-reactive mean, for Flutter?
setState is a thing, and why not to be scared of it
How to dev cycle
Where to find additional info on widgets, layout, plugins, etc

Thanks!
(this came up in a recent UX study)","Request for a ""Thinking in Flutter"" doc","Request for a ""Thinking in Flutter"" docWe seem to have a documentation gap, between ""Getting Started"" and ""Tour of the Widget Framework"". In a recent UX study, we noticed that the high-level concepts and the unique bits of Flutter aren't clearly called out, which made it more challenging for a new user to wrap their heads around what Flutter is and how it thinks.
Initial thoughts on this doc:

Audience is brand new users for Flutter
Intended for the ""ok, what's this flutter thing? what's different?""
Intended to be the next doc you read, after getting started, and before you start diving in
Fairly short, high level

Initial thoughts on table of contents:

What is Flutter?

what problem(s) does it solve
who is it for
what's special?


Functional-reactive framework

widget lifecycle
setState
Only rebuilding what needs to be rebuilt (efficient)
Customizable/extensible


Material design

full set of widgets out of the box


Dev cycle

Hot reload
Fast restart


Tooling

CLI
IntelliJ


Plugins/interop

coming soon!



Once a new user scans through this doc, they should know

What's special about flutter?
What does functional-reactive mean, for Flutter?
setState is a thing, and why not to be scared of it
How to dev cycle
Where to find additional info on widgets, layout, plugins, etc

Thanks!
(this came up in a recent UX study)",3
,2682,148,2,3070,"Please check out this video for an example: https://dl.dropboxusercontent.com/u/316685/video.mp4
In the Buttons page, in the gallery app, swipe left slowly. Notice how the tab underline under FLOATING doesn't animate during swipe.
Then, complete the swipe, and RAISED will be underlined. Then, swipe left and right, and the underline is animated.",Tab underline not moving on first swipe,"Tab underline not moving on first swipePlease check out this video for an example: https://dl.dropboxusercontent.com/u/316685/video.mp4
In the Buttons page, in the gallery app, swipe left slowly. Notice how the tab underline under FLOATING doesn't animate during swipe.
Then, complete the swipe, and RAISED will be underlined. Then, swipe left and right, and the underline is animated.",3
,2683,148,3,30690,"Steps to Reproduce

clone flutter/plugins
cd packages/firebase_ml_vision/example
flutter run (on a real device)
scan some datamatrix

Here is an album with 4 almost exactly same photo but only one work :
https://photos.google.com/share/AF1QipOUTCX6xK77XqMfz4yBki2d3eygi_0GQqNwfiHgDkwl1x_eAodtMVSucjXLgBOIsA?key=Z2lUcnA5bzM5ZWtYOFAwaUNRNzNDOXNGLUlnZUFn
You can use it for testing
What is expected
The scanning should work and return the data in the datamatrix
What happen
The plugin doesn't find the datamatrix in the image most of the time.
Remark
On the emulator the plugins work as expected
Flutter doctor
Flutter (Channel stable, v1.2.1, on Linux, locale en_US.UTF-8)
     Flutter version 1.2.1 at /home/kevin/Work/github.com/flutter/flutter
     Framework revision 8661d8aecd (7 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /home/kevin/Android/Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842/jre/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.3)
     Android Studio at /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] VS Code (version 1.33.0)
     VS Code at /usr/share/code
     Flutter extension version 2.25.0

[] Connected device (2 available)
     TA 1012                    NB1GAS3771407100  android-arm64  Android 9 (API 28)
     Android SDK built for x86  emulator-5554     android-x86    Android 9 (API 28) (emulator)

 No issues found!",firebase_ml_vision scanning datamatrix is really not accurate.,"firebase_ml_vision scanning datamatrix is really not accurate.Steps to Reproduce

clone flutter/plugins
cd packages/firebase_ml_vision/example
flutter run (on a real device)
scan some datamatrix

Here is an album with 4 almost exactly same photo but only one work :
https://photos.google.com/share/AF1QipOUTCX6xK77XqMfz4yBki2d3eygi_0GQqNwfiHgDkwl1x_eAodtMVSucjXLgBOIsA?key=Z2lUcnA5bzM5ZWtYOFAwaUNRNzNDOXNGLUlnZUFn
You can use it for testing
What is expected
The scanning should work and return the data in the datamatrix
What happen
The plugin doesn't find the datamatrix in the image most of the time.
Remark
On the emulator the plugins work as expected
Flutter doctor
Flutter (Channel stable, v1.2.1, on Linux, locale en_US.UTF-8)
     Flutter version 1.2.1 at /home/kevin/Work/github.com/flutter/flutter
     Framework revision 8661d8aecd (7 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /home/kevin/Android/Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842/jre/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.3)
     Android Studio at /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] VS Code (version 1.33.0)
     VS Code at /usr/share/code
     Flutter extension version 2.25.0

[] Connected device (2 available)
     TA 1012                    NB1GAS3771407100  android-arm64  Android 9 (API 28)
     Android SDK built for x86  emulator-5554     android-x86    Android 9 (API 28) (emulator)

 No issues found!",3
,2684,148,4,2586,"I created a brand-new project with flutter create, and then flutter run to my iPod. The app is installed onto the device, but when I run it, it's a pure white screen, and then approx 10s later it quite.
~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 run
Starting lib/main.dart on iPod touch...
[....] Waiting for iOS device to be connected
[....] Using N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7).
------ Install phase ------
[  0%] Found N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7) connected through USB, beginning install
[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/ to device
[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/com.apple.ZipMetadata.plist to device
[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/ to device
[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/CodeResources to device
[  7%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x.png to device
[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x~ipad.png to device
[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@3x.png to device
[  9%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29~ipad.png to device
[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x.png to device
[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x~ipad.png to device
[ 11%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@3x.png to device
[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40~ipad.png to device
[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@2x.png to device
[ 13%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@3x.png to device
[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76@2x~ipad.png to device
[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76~ipad.png to device
[ 15%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon83.5x83.5@2x~ipad.png to device
[ 16%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/embedded.mobileprovision to device
[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/ to device
[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/ to device
[ 18%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/ to device
[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/CodeResources to device
[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/app.flx to device
[ 20%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/FlutterApplication to device
[ 28%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/Info.plist to device
[ 29%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/icudtl.dat to device
[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Info.plist to device
[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/ to device
[ 35%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/01J-lp-oVM-view-Ze5-6b-2t3.nib to device
[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/Info.plist to device
[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/UIViewController-01J-lp-oVM.nib to device
[ 37%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/PkgInfo to device
[ 38%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Runner to device
[ 49%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/ServiceDefinitions.json to device
[ 52%] CreatingStagingDirectory
[ 57%] ExtractingPackage
[ 60%] InspectingPackage
[ 60%] TakingInstallLock
[ 65%] PreflightingApplication
[ 65%] InstallingEmbeddedProfile
[ 70%] VerifyingApplication
[ 75%] CreatingContainer
[ 80%] InstallingApplication
[ 85%] PostflightingApplication
[ 90%] SandboxingApplication
[ 95%] GeneratingApplicationMap
[100%] Installed package ios/.generated/build/Release-iphoneos/Runner.app

Output of the logs:
~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 logs
Showing iPod touch logs:
Mar 10 14:05:05 iPod-touch installd[47] <Notice>: 0x16e12f000 -[MIClientConnection installPath:withOptions:completion:]: Install of ""/var/mobile/Media/PublicStaging/Runner.app"" type Developer (LSInstallType = (null)) requested by mobile_installation_proxy (pid 285)
Mar 10 14:06:12 iPod-touch SpringBoard[54] <Warning>: Forcing crash report of <FBApplicationProcess: 0x14ff58dd0; Runner; pid: 1143> (reason: 1, description: com.yourcompany.muteField6440 failed to scene-create after 19.86s (launch took 0.14s of total time limit 20.00s))
Mar 10 14:06:12 iPod-touch ReportCrash[1144] <Warning>: saved type '109_Runner' report (4 of max 25) as /var/mobile/Library/Logs/CrashReporter/Runner_2016-03-10-140612_iPod-touch.ips

~/flutter/testme3 $ flutter --version
Flutter from git@github.com:flutter/flutter.git (on master)
Framework: 3b5fba40227cd0ee0949bbe195edd372f84b0e7e (79 minutes ago)
Engine:    d1515e676438d79046508ca009044df246564ced

[] The iOS toolchain is fully installed.
[] The Android toolchain is fully installed.
[] The Atom development environment is fully installed.","App is white, then quits, on iPod","App is white, then quits, on iPodI created a brand-new project with flutter create, and then flutter run to my iPod. The app is installed onto the device, but when I run it, it's a pure white screen, and then approx 10s later it quite.
~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 run
Starting lib/main.dart on iPod touch...
[....] Waiting for iOS device to be connected
[....] Using N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7).
------ Install phase ------
[  0%] Found N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7) connected through USB, beginning install
[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/ to device
[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/com.apple.ZipMetadata.plist to device
[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/ to device
[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/CodeResources to device
[  7%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x.png to device
[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x~ipad.png to device
[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@3x.png to device
[  9%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29~ipad.png to device
[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x.png to device
[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x~ipad.png to device
[ 11%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@3x.png to device
[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40~ipad.png to device
[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@2x.png to device
[ 13%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@3x.png to device
[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76@2x~ipad.png to device
[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76~ipad.png to device
[ 15%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon83.5x83.5@2x~ipad.png to device
[ 16%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/embedded.mobileprovision to device
[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/ to device
[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/ to device
[ 18%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/ to device
[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/CodeResources to device
[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/app.flx to device
[ 20%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/FlutterApplication to device
[ 28%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/Info.plist to device
[ 29%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/icudtl.dat to device
[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Info.plist to device
[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/ to device
[ 35%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/01J-lp-oVM-view-Ze5-6b-2t3.nib to device
[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/Info.plist to device
[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/UIViewController-01J-lp-oVM.nib to device
[ 37%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/PkgInfo to device
[ 38%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Runner to device
[ 49%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/ServiceDefinitions.json to device
[ 52%] CreatingStagingDirectory
[ 57%] ExtractingPackage
[ 60%] InspectingPackage
[ 60%] TakingInstallLock
[ 65%] PreflightingApplication
[ 65%] InstallingEmbeddedProfile
[ 70%] VerifyingApplication
[ 75%] CreatingContainer
[ 80%] InstallingApplication
[ 85%] PostflightingApplication
[ 90%] SandboxingApplication
[ 95%] GeneratingApplicationMap
[100%] Installed package ios/.generated/build/Release-iphoneos/Runner.app

Output of the logs:
~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 logs
Showing iPod touch logs:
Mar 10 14:05:05 iPod-touch installd[47] <Notice>: 0x16e12f000 -[MIClientConnection installPath:withOptions:completion:]: Install of ""/var/mobile/Media/PublicStaging/Runner.app"" type Developer (LSInstallType = (null)) requested by mobile_installation_proxy (pid 285)
Mar 10 14:06:12 iPod-touch SpringBoard[54] <Warning>: Forcing crash report of <FBApplicationProcess: 0x14ff58dd0; Runner; pid: 1143> (reason: 1, description: com.yourcompany.muteField6440 failed to scene-create after 19.86s (launch took 0.14s of total time limit 20.00s))
Mar 10 14:06:12 iPod-touch ReportCrash[1144] <Warning>: saved type '109_Runner' report (4 of max 25) as /var/mobile/Library/Logs/CrashReporter/Runner_2016-03-10-140612_iPod-touch.ips

~/flutter/testme3 $ flutter --version
Flutter from git@github.com:flutter/flutter.git (on master)
Framework: 3b5fba40227cd0ee0949bbe195edd372f84b0e7e (79 minutes ago)
Engine:    d1515e676438d79046508ca009044df246564ced

[] The iOS toolchain is fully installed.
[] The Android toolchain is fully installed.
[] The Atom development environment is fully installed.",3
,2685,144,5,28512,"I am a student who has only recently started to learn Flutter.
I have a GridView which builds a dynamic number of grids. If the devices screen is too small sometimes the number of grids overflow and this comes through to the console:
I/flutter ( 6315):  EXCEPTION CAUGHT BY RENDERING LIBRARY 
I/flutter ( 6315): The following message was thrown during layout:
I/flutter ( 6315): A RenderFlex overflowed by 120 pixels on the bottom.
I/flutter ( 6315): 
I/flutter ( 6315): The overflowing RenderFlex has an orientation of Axis.vertical.
I/flutter ( 6315): The edge of the RenderFlex that is overflowing has been marked in the rendering with a yellow and
I/flutter ( 6315): black striped pattern. This is usually caused by the contents being too big for the RenderFlex.
I/flutter ( 6315): Consider applying a flex factor (e.g. using an Expanded widget) to force the children of the
I/flutter ( 6315): RenderFlex to fit within the available space instead of being sized to their natural size.
I/flutter ( 6315): This is considered an error condition because it indicates that there is content that cannot be
I/flutter ( 6315): seen. If the content is legitimately bigger than the available space, consider clipping it with a
I/flutter ( 6315): ClipRect widget before putting it in the flex, or using a scrollable container rather than a Flex,
I/flutter ( 6315): like a ListView.
I/flutter ( 6315): The specific RenderFlex in question is:
I/flutter ( 6315):   RenderFlex#601d5 relayoutBoundary=up8 OVERFLOWING
I/flutter ( 6315):   creator: Column  MediaQuery  Padding  SafeArea  Align  DefaultTextStyle 
I/flutter ( 6315):   AnimatedDefaultTextStyle  _InkFeatures-[GlobalKey#13a7a ink renderer] 
I/flutter ( 6315):   NotificationListener<LayoutChangedNotification>  PhysicalModel  AnimatedPhysicalModel  Material
I/flutter ( 6315):    
I/flutter ( 6315):   parentData: offset=Offset(0.0, 24.0) (can use size)
I/flutter ( 6315):   constraints: BoxConstraints(0.0<=w<=320.0, 0.0<=h<=509.3)
I/flutter ( 6315):   size: Size(320.0, 509.3)
I/flutter ( 6315):   direction: vertical
I/flutter ( 6315):   mainAxisAlignment: spaceBetween
I/flutter ( 6315):   mainAxisSize: max
I/flutter ( 6315):   crossAxisAlignment: center
I/flutter ( 6315):   verticalDirection: down
I/flutter ( 6315): 
I/flutter ( 6315): 

return new GridView.builder(
      scrollDirection: Axis.vertical,
      shrinkWrap: true,
      gridDelegate: new SliverGridDelegateWithMaxCrossAxisExtent(
          maxCrossAxisExtent: 150.0, //grid is 500.0 pixels wide, and [maxCrossAxisExtent] is 150.0, this delegate will create a grid with 4 columns that are 125.0 pixels wide
          mainAxisSpacing: 10.0,
          crossAxisSpacing: 10,
          childAspectRatio: gridChildAspecRat,
      ), //n/3 * 1.5
      padding: const EdgeInsets.all(5.0),
      itemCount: buttonsList.length,
      itemBuilder: (context, i) => new SizedBox(
            width: 100.0, //100
            height: 100.0, //100
            child: new RaisedButton(
              padding: const EdgeInsets.all(8.0),
              onPressed: buttonsList[i].enabled?()=>playTicTacToe(buttonsList[i]):null,
              child: new Text(
                buttonsList[i].type,
                style: new TextStyle(color: Colors.white, fontSize: 20.0),
              ),
              color: buttonsList[i].bkgrnd,
              disabledColor: buttonsList[i].bkgrnd,
            ),
          ),
    );
Is there any way I could add a try / catch block somewhere which could remove cards from the buttonsList  until there aren't enough cards on the screen to  overflow?
try {
     /* GridView Builder */
    }catch(e){
      debugPrint('heightTooManyCards');
        _nCards = _nCards - 1;
        buttonsList = doInit();
    }
I couldn't find anywhere to catch the error message outputted in the terminal during runtime.
Any ideas on how I could fix this?
Thank you all very much!",Grid View Builder Catch RenderFlex Overflow,"Grid View Builder Catch RenderFlex OverflowI am a student who has only recently started to learn Flutter.
I have a GridView which builds a dynamic number of grids. If the devices screen is too small sometimes the number of grids overflow and this comes through to the console:
I/flutter ( 6315):  EXCEPTION CAUGHT BY RENDERING LIBRARY 
I/flutter ( 6315): The following message was thrown during layout:
I/flutter ( 6315): A RenderFlex overflowed by 120 pixels on the bottom.
I/flutter ( 6315): 
I/flutter ( 6315): The overflowing RenderFlex has an orientation of Axis.vertical.
I/flutter ( 6315): The edge of the RenderFlex that is overflowing has been marked in the rendering with a yellow and
I/flutter ( 6315): black striped pattern. This is usually caused by the contents being too big for the RenderFlex.
I/flutter ( 6315): Consider applying a flex factor (e.g. using an Expanded widget) to force the children of the
I/flutter ( 6315): RenderFlex to fit within the available space instead of being sized to their natural size.
I/flutter ( 6315): This is considered an error condition because it indicates that there is content that cannot be
I/flutter ( 6315): seen. If the content is legitimately bigger than the available space, consider clipping it with a
I/flutter ( 6315): ClipRect widget before putting it in the flex, or using a scrollable container rather than a Flex,
I/flutter ( 6315): like a ListView.
I/flutter ( 6315): The specific RenderFlex in question is:
I/flutter ( 6315):   RenderFlex#601d5 relayoutBoundary=up8 OVERFLOWING
I/flutter ( 6315):   creator: Column  MediaQuery  Padding  SafeArea  Align  DefaultTextStyle 
I/flutter ( 6315):   AnimatedDefaultTextStyle  _InkFeatures-[GlobalKey#13a7a ink renderer] 
I/flutter ( 6315):   NotificationListener<LayoutChangedNotification>  PhysicalModel  AnimatedPhysicalModel  Material
I/flutter ( 6315):    
I/flutter ( 6315):   parentData: offset=Offset(0.0, 24.0) (can use size)
I/flutter ( 6315):   constraints: BoxConstraints(0.0<=w<=320.0, 0.0<=h<=509.3)
I/flutter ( 6315):   size: Size(320.0, 509.3)
I/flutter ( 6315):   direction: vertical
I/flutter ( 6315):   mainAxisAlignment: spaceBetween
I/flutter ( 6315):   mainAxisSize: max
I/flutter ( 6315):   crossAxisAlignment: center
I/flutter ( 6315):   verticalDirection: down
I/flutter ( 6315): 
I/flutter ( 6315): 

return new GridView.builder(
      scrollDirection: Axis.vertical,
      shrinkWrap: true,
      gridDelegate: new SliverGridDelegateWithMaxCrossAxisExtent(
          maxCrossAxisExtent: 150.0, //grid is 500.0 pixels wide, and [maxCrossAxisExtent] is 150.0, this delegate will create a grid with 4 columns that are 125.0 pixels wide
          mainAxisSpacing: 10.0,
          crossAxisSpacing: 10,
          childAspectRatio: gridChildAspecRat,
      ), //n/3 * 1.5
      padding: const EdgeInsets.all(5.0),
      itemCount: buttonsList.length,
      itemBuilder: (context, i) => new SizedBox(
            width: 100.0, //100
            height: 100.0, //100
            child: new RaisedButton(
              padding: const EdgeInsets.all(8.0),
              onPressed: buttonsList[i].enabled?()=>playTicTacToe(buttonsList[i]):null,
              child: new Text(
                buttonsList[i].type,
                style: new TextStyle(color: Colors.white, fontSize: 20.0),
              ),
              color: buttonsList[i].bkgrnd,
              disabledColor: buttonsList[i].bkgrnd,
            ),
          ),
    );
Is there any way I could add a try / catch block somewhere which could remove cards from the buttonsList  until there aren't enough cards on the screen to  overflow?
try {
     /* GridView Builder */
    }catch(e){
      debugPrint('heightTooManyCards');
        _nCards = _nCards - 1;
        buttonsList = doInit();
    }
I couldn't find anywhere to catch the error message outputted in the terminal during runtime.
Any ideas on how I could fix this?
Thank you all very much!",3
,2686,148,6,18792,"here is flow thats i want to manage back
1 - on click home page's button start store list page.
2 - on click store list page's button start filter page.
3 - on click store filter page's button store list page.
4 - on press back go to home page.",is flutter have concept like android addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK),"is flutter have concept like android addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK)here is flow thats i want to manage back
1 - on click home page's button start store list page.
2 - on click store list page's button start filter page.
3 - on click store filter page's button store list page.
4 - on press back go to home page.",3
,2687,144,7,32181,"The integration test passes on iOS and fails on Android.
I/flutter (21641): 00:02 +0: FirebaseStorage putFile, getDownloadURL, writeToFile [E]
I/flutter (21641):   Expected: 'text/plain'
I/flutter (21641):     Actual: 'application/octet-stream'
I/flutter (21641):      Which: is different.
I/flutter (21641):             Expected: text/plain ...
I/flutter (21641):               Actual: applicatio ...
I/flutter (21641):                       ^
I/flutter (21641):              Differ at offset 0
I/flutter (21641):   
I/flutter (21641):   package:test_api/src/frontend/expect.dart 152:30                                                       fail
I/flutter (21641):   package:test_api/src/frontend/expect.dart 146:3                                                        _expect
I/flutter (21641):   package:test_api/src/frontend/expect.dart 59:3                                                         expect
I/flutter (21641):   package:flutter_test/src/widget_tester.dart 196:3                                                      expect
I/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart 47:7  main.<fn>.<fn>
I/flutter (21641):   ===== asynchronous gap ===========================
I/flutter (21641):   dart:async/future_impl.dart 22:43                                                                      _Completer.completeError
I/flutter (21641):   dart:async-patch/async_patch.dart 40:18                                                                _AsyncAwaitCompleter.completeError
I/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart       main.<fn>.<fn>
I/flutter (21641):   ===== async
I/flutter (21641): 00:02 +0 -1: (tearDownAll)
I/flutter (21641): 00:02 +1 -1: Some tests failed.",firebase_storage Content-Type autodetection not working on Android,"firebase_storage Content-Type autodetection not working on AndroidThe integration test passes on iOS and fails on Android.
I/flutter (21641): 00:02 +0: FirebaseStorage putFile, getDownloadURL, writeToFile [E]
I/flutter (21641):   Expected: 'text/plain'
I/flutter (21641):     Actual: 'application/octet-stream'
I/flutter (21641):      Which: is different.
I/flutter (21641):             Expected: text/plain ...
I/flutter (21641):               Actual: applicatio ...
I/flutter (21641):                       ^
I/flutter (21641):              Differ at offset 0
I/flutter (21641):   
I/flutter (21641):   package:test_api/src/frontend/expect.dart 152:30                                                       fail
I/flutter (21641):   package:test_api/src/frontend/expect.dart 146:3                                                        _expect
I/flutter (21641):   package:test_api/src/frontend/expect.dart 59:3                                                         expect
I/flutter (21641):   package:flutter_test/src/widget_tester.dart 196:3                                                      expect
I/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart 47:7  main.<fn>.<fn>
I/flutter (21641):   ===== asynchronous gap ===========================
I/flutter (21641):   dart:async/future_impl.dart 22:43                                                                      _Completer.completeError
I/flutter (21641):   dart:async-patch/async_patch.dart 40:18                                                                _AsyncAwaitCompleter.completeError
I/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart       main.<fn>.<fn>
I/flutter (21641):   ===== async
I/flutter (21641): 00:02 +0 -1: (tearDownAll)
I/flutter (21641): 00:02 +1 -1: Some tests failed.",3
,2688,148,8,7766,"My flutter run --release for my iPhone is stuck at:
~/Code/hello_cupertino[master*] $ flutter run --release
Launching lib/main.dart on flutter 6s in release mode...
Traceback (most recent call last):
  File ""/tmp/fruitstrap_d83d5bc53967baa0ee18626ba87b6254b2ab5418.py"", line 35, in run_command
    lldb.target.Launch(lldb.SBLaunchInfo(shlex.split(args[1] and args[1] or '--enable-dart-profiling')), error)
IndexError: list index out of range

Any ideas?
~/Code/hello_cupertino[master*] $ flutter --version
Flutter  channel master  git@github.com:flutter/flutter.git
Framework  revision a0f0c42fe3 (64 minutes ago)  2017-01-31 14:48:48
Engine  revision 2d54edf0f9
Tools  Dart 1.22.0-dev.9.1

and
~/Code/hello_cupertino[master*] $ flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/sethladd/Code/flutter
     Framework revision a0f0c42fe3 (64 minutes ago), 2017-01-31 14:48:48
     Engine revision 2d54edf0f9
     Tools Dart version 1.22.0-dev.9.1

[] Android toolchain - develop for Android devices (Android SDK 25.0.0)
     Android SDK at /Users/sethladd/Library/Android/sdk
     Platform android-25, build-tools 25.0.0
     Java(TM) SE Runtime Environment (build 1.8.0_91-b14)

[] iOS toolchain - develop for iOS devices (Xcode 8.2.1)
     XCode at /Applications/Xcode.app/Contents/Developer
     Xcode 8.2.1, Build version 8C1002

[] IntelliJ IDEA Community Edition (version 2016.3.1)
     Dart plugin version 163.9166.22
     Flutter plugin version 0.1.4

[] Connected devices
     flutter 6s  d83d5bc53967baa0ee18626ba87b6254b2ab5418  ios  iOS 10.1.1 (14B100)","`flutter run --release` stuck when building for iOS: ""library not found for -lPods-Runner""","`flutter run --release` stuck when building for iOS: ""library not found for -lPods-Runner""My flutter run --release for my iPhone is stuck at:
~/Code/hello_cupertino[master*] $ flutter run --release
Launching lib/main.dart on flutter 6s in release mode...
Traceback (most recent call last):
  File ""/tmp/fruitstrap_d83d5bc53967baa0ee18626ba87b6254b2ab5418.py"", line 35, in run_command
    lldb.target.Launch(lldb.SBLaunchInfo(shlex.split(args[1] and args[1] or '--enable-dart-profiling')), error)
IndexError: list index out of range

Any ideas?
~/Code/hello_cupertino[master*] $ flutter --version
Flutter  channel master  git@github.com:flutter/flutter.git
Framework  revision a0f0c42fe3 (64 minutes ago)  2017-01-31 14:48:48
Engine  revision 2d54edf0f9
Tools  Dart 1.22.0-dev.9.1

and
~/Code/hello_cupertino[master*] $ flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/sethladd/Code/flutter
     Framework revision a0f0c42fe3 (64 minutes ago), 2017-01-31 14:48:48
     Engine revision 2d54edf0f9
     Tools Dart version 1.22.0-dev.9.1

[] Android toolchain - develop for Android devices (Android SDK 25.0.0)
     Android SDK at /Users/sethladd/Library/Android/sdk
     Platform android-25, build-tools 25.0.0
     Java(TM) SE Runtime Environment (build 1.8.0_91-b14)

[] iOS toolchain - develop for iOS devices (Xcode 8.2.1)
     XCode at /Applications/Xcode.app/Contents/Developer
     Xcode 8.2.1, Build version 8C1002

[] IntelliJ IDEA Community Edition (version 2016.3.1)
     Dart plugin version 163.9166.22
     Flutter plugin version 0.1.4

[] Connected devices
     flutter 6s  d83d5bc53967baa0ee18626ba87b6254b2ab5418  ios  iOS 10.1.1 (14B100)",3
,2689,144,9,8888,"I thought we had decided to rename BlockBody, but I can't find a bug on it anymore.",Rename BlockBody and fix references to RenderBlockBase,"Rename BlockBody and fix references to RenderBlockBaseI thought we had decided to rename BlockBody, but I can't find a bug on it anymore.",3
,2690,148,10,8373,"log:
Flutter crash report; please file at https://github.com/flutter/flutter/issues.
command
flutter doctor
exception
FormatException: Could not parse ""EAP AI-162.3715353"".
package:flutter_tools/src/base/version.dart 48                      Version.Version.parse
package:flutter_tools/src/android/android_studio.dart 64            AndroidStudio.AndroidStudio.fromMacOSBundle
package:flutter_tools/src/android/android_studio.dart 161           AndroidStudio._allMacOS.<fn>
dart:core                                                           Iterable.toList
package:flutter_tools/src/android/android_studio.dart 163           AndroidStudio._allMacOS
package:flutter_tools/src/android/android_studio.dart 124           AndroidStudio.allInstalled
package:flutter_tools/src/android/android_studio_validator.dart 23  AndroidStudioValidator.allValidators
package:flutter_tools/src/doctor.dart 63                            Doctor.validators
package:flutter_tools/src/doctor.dart 121                           Doctor.diagnose
package:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand
package:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand
package:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run
package:args/command_runner.dart 194                                CommandRunner.runCommand
package:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand
package:args/command_runner.dart 109                                CommandRunner.run.<fn>
dart:async                                                          Future.Future.sync
package:args/command_runner.dart 109                                CommandRunner.run
package:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run
package:flutter_tools/executable.dart 128                           main.<fn>.<fn>
package:stack_trace                                                 Chain.capture
package:flutter_tools/executable.dart 127                           main.<fn>
package:flutter_tools/src/base/context.dart 76                      AppContext._run
package:flutter_tools/src/base/context.dart 66                      AppContext.runInZone.<fn>
dart:async                                                          runZoned
package:flutter_tools/src/base/context.dart 65                      AppContext.runInZone
package:flutter_tools/executable.dart 98                            main
../packages/flutter_tools/bin/flutter_tools.dart 8                  main
===== asynchronous gap ===========================
dart:async                                                          _Completer.completeError
package:flutter_tools/src/doctor.dart 149                           Doctor.diagnose
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/src/doctor.dart                               Doctor.diagnose
package:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/src/commands/doctor.dart                      DoctorCommand.runCommand
package:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand
===== asynchronous gap ===========================
dart:async                                                          _asyncThenWrapperHelper
package:flutter_tools/src/runner/flutter_command.dart               FlutterCommand.verifyThenRunCommand
package:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run
package:args/command_runner.dart 194                                CommandRunner.runCommand
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:args/command_runner.dart                                    CommandRunner.runCommand
package:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand
===== asynchronous gap ===========================
dart:async                                                          _asyncThenWrapperHelper
package:flutter_tools/src/runner/flutter_command_runner.dart        FlutterCommandRunner.runCommand
package:args/command_runner.dart 109                                CommandRunner.run.<fn>
dart:async                                                          Future.Future.sync
package:args/command_runner.dart 109                                CommandRunner.run
package:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run
package:flutter_tools/executable.dart 128                           main.<fn>.<fn>
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/executable.dart                               main.<fn>.<fn>
package:stack_trace                                                 Chain.capture
package:flutter_tools/executable.dart 127                           main.<fn>

flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/zhai/Documents/Flutter/flutter
     Framework revision 2a05cfdf07 (2 hours ago), 2017-02-23 16:27:30
     Engine revision ab09530927
     Tools Dart version 1.23.0-dev.0.0

[] Android toolchain - develop for Android devices (Android SDK 25.0.2)
     Android SDK at /Users/zhai/Documents/WorkSpace/adt-bundle-mac-x86_64-20130917/sdk
     Platform android-25, build-tools 25.0.2
     Java Development Kit (JDK) found: javac 1.8.0_92

[] iOS toolchain - develop for iOS devices (Xcode 8.2.1)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 8.2.1, Build version 8C1002
     ios-deploy 1.9.0","FormatException: Could not parse ""EAP AI-162.3715353""?","FormatException: Could not parse ""EAP AI-162.3715353""?log:
Flutter crash report; please file at https://github.com/flutter/flutter/issues.
command
flutter doctor
exception
FormatException: Could not parse ""EAP AI-162.3715353"".
package:flutter_tools/src/base/version.dart 48                      Version.Version.parse
package:flutter_tools/src/android/android_studio.dart 64            AndroidStudio.AndroidStudio.fromMacOSBundle
package:flutter_tools/src/android/android_studio.dart 161           AndroidStudio._allMacOS.<fn>
dart:core                                                           Iterable.toList
package:flutter_tools/src/android/android_studio.dart 163           AndroidStudio._allMacOS
package:flutter_tools/src/android/android_studio.dart 124           AndroidStudio.allInstalled
package:flutter_tools/src/android/android_studio_validator.dart 23  AndroidStudioValidator.allValidators
package:flutter_tools/src/doctor.dart 63                            Doctor.validators
package:flutter_tools/src/doctor.dart 121                           Doctor.diagnose
package:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand
package:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand
package:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run
package:args/command_runner.dart 194                                CommandRunner.runCommand
package:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand
package:args/command_runner.dart 109                                CommandRunner.run.<fn>
dart:async                                                          Future.Future.sync
package:args/command_runner.dart 109                                CommandRunner.run
package:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run
package:flutter_tools/executable.dart 128                           main.<fn>.<fn>
package:stack_trace                                                 Chain.capture
package:flutter_tools/executable.dart 127                           main.<fn>
package:flutter_tools/src/base/context.dart 76                      AppContext._run
package:flutter_tools/src/base/context.dart 66                      AppContext.runInZone.<fn>
dart:async                                                          runZoned
package:flutter_tools/src/base/context.dart 65                      AppContext.runInZone
package:flutter_tools/executable.dart 98                            main
../packages/flutter_tools/bin/flutter_tools.dart 8                  main
===== asynchronous gap ===========================
dart:async                                                          _Completer.completeError
package:flutter_tools/src/doctor.dart 149                           Doctor.diagnose
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/src/doctor.dart                               Doctor.diagnose
package:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/src/commands/doctor.dart                      DoctorCommand.runCommand
package:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand
===== asynchronous gap ===========================
dart:async                                                          _asyncThenWrapperHelper
package:flutter_tools/src/runner/flutter_command.dart               FlutterCommand.verifyThenRunCommand
package:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run
package:args/command_runner.dart 194                                CommandRunner.runCommand
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:args/command_runner.dart                                    CommandRunner.runCommand
package:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand
===== asynchronous gap ===========================
dart:async                                                          _asyncThenWrapperHelper
package:flutter_tools/src/runner/flutter_command_runner.dart        FlutterCommandRunner.runCommand
package:args/command_runner.dart 109                                CommandRunner.run.<fn>
dart:async                                                          Future.Future.sync
package:args/command_runner.dart 109                                CommandRunner.run
package:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run
package:flutter_tools/executable.dart 128                           main.<fn>.<fn>
===== asynchronous gap ===========================
dart:async                                                          Future.Future.microtask
package:flutter_tools/executable.dart                               main.<fn>.<fn>
package:stack_trace                                                 Chain.capture
package:flutter_tools/executable.dart 127                           main.<fn>

flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/zhai/Documents/Flutter/flutter
     Framework revision 2a05cfdf07 (2 hours ago), 2017-02-23 16:27:30
     Engine revision ab09530927
     Tools Dart version 1.23.0-dev.0.0

[] Android toolchain - develop for Android devices (Android SDK 25.0.2)
     Android SDK at /Users/zhai/Documents/WorkSpace/adt-bundle-mac-x86_64-20130917/sdk
     Platform android-25, build-tools 25.0.2
     Java Development Kit (JDK) found: javac 1.8.0_92

[] iOS toolchain - develop for iOS devices (Xcode 8.2.1)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 8.2.1, Build version 8C1002
     ios-deploy 1.9.0",3
,2691,148,11,29031,"I have problem with the ""options: GoogleMapOptions"" part, where the options is not define. Am i missing something? Im new with flutter.
This is my main.dart
**import 'package:flutter/material.dart';
import 'package:google_maps_flutter/google_maps_flutter.dart';
import 'dart:async';
void main() => runApp(MyApp());
class MyApp extends StatelessWidget {
// This widget is the root of your application.
@override
Widget build(BuildContext context) {
return MaterialApp(
title: 'Google Map',
theme: ThemeData(
primarySwatch: Colors.blue,
),
home: MyHomePage(title: 'Google Map'),
);
}
}
class MyHomePage extends StatefulWidget {
MyHomePage({Key key, this.title}) : super(key: key);
final String title;
@override
_MyHomePageState createState() => _MyHomePageState();
}
class _MyHomePageState extends State {
@override
Widget build(BuildContext context) {
return Scaffold(
  appBar: AppBar(
    title: Text(widget.title),
  ),
  body: Center(
    child: Column(
      children: <Widget>[
      GoogleMap(
        onMapCreated: (GoogleMapController controller) {},
        options: GoogleMapOptions(
          cameraPosition: CameraPosition(
            target: LatLng(37.4219999, -122.0862462),
          ),
        ), initialCameraPosition: null,
      ),
      ],
    ),
  ),
  floatingActionButton: FloatingActionButton(
  ), // This trailing comma makes auto-formatting nicer for build methods.
);

}
}**
where the pubspec i had include
google_maps_flutter:
git:
url: git://github.com/flutter/plugins
path: packages/google_maps_flutter",The parameter 'options' isnt defined and the method GoogleMapOptions isnt define for class,"The parameter 'options' isnt defined and the method GoogleMapOptions isnt define for classI have problem with the ""options: GoogleMapOptions"" part, where the options is not define. Am i missing something? Im new with flutter.
This is my main.dart
**import 'package:flutter/material.dart';
import 'package:google_maps_flutter/google_maps_flutter.dart';
import 'dart:async';
void main() => runApp(MyApp());
class MyApp extends StatelessWidget {
// This widget is the root of your application.
@override
Widget build(BuildContext context) {
return MaterialApp(
title: 'Google Map',
theme: ThemeData(
primarySwatch: Colors.blue,
),
home: MyHomePage(title: 'Google Map'),
);
}
}
class MyHomePage extends StatefulWidget {
MyHomePage({Key key, this.title}) : super(key: key);
final String title;
@override
_MyHomePageState createState() => _MyHomePageState();
}
class _MyHomePageState extends State {
@override
Widget build(BuildContext context) {
return Scaffold(
  appBar: AppBar(
    title: Text(widget.title),
  ),
  body: Center(
    child: Column(
      children: <Widget>[
      GoogleMap(
        onMapCreated: (GoogleMapController controller) {},
        options: GoogleMapOptions(
          cameraPosition: CameraPosition(
            target: LatLng(37.4219999, -122.0862462),
          ),
        ), initialCameraPosition: null,
      ),
      ],
    ),
  ),
  floatingActionButton: FloatingActionButton(
  ), // This trailing comma makes auto-formatting nicer for build methods.
);

}
}**
where the pubspec i had include
google_maps_flutter:
git:
url: git://github.com/flutter/plugins
path: packages/google_maps_flutter",3
,2692,148,12,18763,"have a bottom navigation bar, and the 2 tab is have appbar + 2 tabs.
get error when clicking bottomNavigationBar 3 to 1 without clicking at 2
https://github.com/jhionan/flutterError
Error:
`I/flutter (11406):  EXCEPTION CAUGHT BY WIDGETS LIBRARY 
I/flutter (11406): The following assertion was thrown while finalizing the widget tree:
I/flutter (11406): 'package:flutter/src/widgets/scroll_position.dart': Failed assertion: line 657 pos 12: 'pixels !=
I/flutter (11406): null': is not true.
I/flutter (11406): 
I/flutter (11406): Either the assertion indicates an error in the framework itself, or we should provide substantially
I/flutter (11406): more information in this error message to help you determine and fix the underlying cause.
I/flutter (11406): In either case, please report this assertion by filing a bug on GitHub:
I/flutter (11406):   https://github.com/flutter/flutter/issues/new
I/flutter (11406): 
I/flutter (11406): When the exception was thrown, this was the stack:
I/flutter (11406): #2      ScrollPosition.dispose (package:flutter/src/widgets/scroll_position.dart)
I/flutter (11406): #3      ScrollPositionWithSingleContext.dispose (package:flutter/src/widgets/scroll_position_with_single_context.dart:255:11)
I/flutter (11406): #4      ScrollableState.dispose (package:flutter/src/widgets/scrollable.dart:324:14)
I/flutter (11406): #5      StatefulElement.unmount (package:flutter/src/widgets/framework.dart:3821:12)
I/flutter (11406): #6      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1697:13)
I/flutter (11406): #7      _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #8      ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #9      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #10     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #11     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #12     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #13     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #14     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #15     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #16     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #17     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #18     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #19     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #20     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #21     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #22     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #23     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #24     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #25     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #26     MultiChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4742:16)
I/flutter (11406): #27     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #28     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #29     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #30     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #31     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #32     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #33     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #34     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #35     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #36     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #37     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #38     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #39     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #40     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #41     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #42     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #43     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #44     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #45     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #46     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #47     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #48     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #49     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #50     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #51     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #52     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #53     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #54     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #55     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #56     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #57     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #58     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #59     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #60     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #61     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #62     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #63     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #64     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #65     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #66     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #67     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #68     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #69     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #70     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #71     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #72     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #73     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #74     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #75     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #76     ListIterable.forEach (dart:_internal/iterable.dart:39:13)
I/flutter (11406): #77     _InactiveElements._unmountAll (package:flutter/src/widgets/framework.dart:1706:25)
I/flutter (11406): #78     BuildOwner.finalizeTree.<anonymous closure> (package:flutter/src/widgets/framework.dart:2328:27)
I/flutter (11406): #79     BuildOwner.lockState (package:flutter/src/widgets/framework.dart:2160:15)
I/flutter (11406): #80     BuildOwner.finalizeTree (package:flutter/src/widgets/framework.dart:2327:7)
I/flutter (11406): #81     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:628:18)
I/flutter (11406): #82     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:208:5)
I/flutter (11406): #83     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:990:15)
I/flutter (11406): #84     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:930:9)
I/flutter (11406): #85     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:842:5)
I/flutter (11406): #86     _invoke (dart:ui/hooks.dart:120:13)
I/flutter (11406): #87     _drawFrame (dart:ui/hooks.dart:109:3)
I/flutter (11406): (elided 2 frames from class _AssertionError)
I/flutter (11406): `",Tabview error,"Tabview errorhave a bottom navigation bar, and the 2 tab is have appbar + 2 tabs.
get error when clicking bottomNavigationBar 3 to 1 without clicking at 2
https://github.com/jhionan/flutterError
Error:
`I/flutter (11406):  EXCEPTION CAUGHT BY WIDGETS LIBRARY 
I/flutter (11406): The following assertion was thrown while finalizing the widget tree:
I/flutter (11406): 'package:flutter/src/widgets/scroll_position.dart': Failed assertion: line 657 pos 12: 'pixels !=
I/flutter (11406): null': is not true.
I/flutter (11406): 
I/flutter (11406): Either the assertion indicates an error in the framework itself, or we should provide substantially
I/flutter (11406): more information in this error message to help you determine and fix the underlying cause.
I/flutter (11406): In either case, please report this assertion by filing a bug on GitHub:
I/flutter (11406):   https://github.com/flutter/flutter/issues/new
I/flutter (11406): 
I/flutter (11406): When the exception was thrown, this was the stack:
I/flutter (11406): #2      ScrollPosition.dispose (package:flutter/src/widgets/scroll_position.dart)
I/flutter (11406): #3      ScrollPositionWithSingleContext.dispose (package:flutter/src/widgets/scroll_position_with_single_context.dart:255:11)
I/flutter (11406): #4      ScrollableState.dispose (package:flutter/src/widgets/scrollable.dart:324:14)
I/flutter (11406): #5      StatefulElement.unmount (package:flutter/src/widgets/framework.dart:3821:12)
I/flutter (11406): #6      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1697:13)
I/flutter (11406): #7      _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #8      ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #9      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #10     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #11     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #12     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #13     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #14     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #15     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #16     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #17     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #18     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #19     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #20     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #21     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #22     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #23     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #24     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #25     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #26     MultiChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4742:16)
I/flutter (11406): #27     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #28     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #29     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #30     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #31     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #32     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #33     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #34     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #35     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #36     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #37     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #38     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #39     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #40     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #41     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #42     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #43     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #44     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #45     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #46     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #47     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #48     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #49     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #50     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #51     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #52     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #53     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #54     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #55     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #56     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #57     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #58     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #59     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #60     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #61     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #62     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #63     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #64     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #65     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)
I/flutter (11406): #66     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #67     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #68     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #69     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #70     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #71     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #72     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #73     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)
I/flutter (11406): #74     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)
I/flutter (11406): #75     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)
I/flutter (11406): #76     ListIterable.forEach (dart:_internal/iterable.dart:39:13)
I/flutter (11406): #77     _InactiveElements._unmountAll (package:flutter/src/widgets/framework.dart:1706:25)
I/flutter (11406): #78     BuildOwner.finalizeTree.<anonymous closure> (package:flutter/src/widgets/framework.dart:2328:27)
I/flutter (11406): #79     BuildOwner.lockState (package:flutter/src/widgets/framework.dart:2160:15)
I/flutter (11406): #80     BuildOwner.finalizeTree (package:flutter/src/widgets/framework.dart:2327:7)
I/flutter (11406): #81     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:628:18)
I/flutter (11406): #82     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:208:5)
I/flutter (11406): #83     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:990:15)
I/flutter (11406): #84     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:930:9)
I/flutter (11406): #85     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:842:5)
I/flutter (11406): #86     _invoke (dart:ui/hooks.dart:120:13)
I/flutter (11406): #87     _drawFrame (dart:ui/hooks.dart:109:3)
I/flutter (11406): (elided 2 frames from class _AssertionError)
I/flutter (11406): `",3
,2693,144,13,7203,"If there's more than one route in the current stack and when popAndPushNamed is called(or calling pop and then push), the following exception will be thrown:
I/flutter ( 6710): NoSuchMethodError: The getter 'subtreeContext' was called on null.
I/flutter ( 6710): Receiver: null
I/flutter ( 6710): Tried calling: subtreeContext
I/flutter ( 6710): When the exception was thrown, this was the stack:
I/flutter ( 6710): #0      Object._noSuchMethod (dart:core-patch/object_patch.dart:44)
I/flutter ( 6710): #1      Object.noSuchMethod (dart:core-patch/object_patch.dart:47)
I/flutter ( 6710): #2      HeroController._updateQuest (package:flutter/src/widgets/heroes.dart:579)
I/flutter ( 6710): #3      BindingBase&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:653)
I/flutter ( 6710): #4      BindingBase&SchedulerBinding.handleBeginFrame (package:flutter/src/scheduler/binding.dart:596)
I/flutter ( 6710): #5      _beginFrame (file:///b/build/slave/Linux_Engine/build/src/flutter/lib/ui/hooks.dart:83)",Calling popAndPushNamed throws exception,"Calling popAndPushNamed throws exceptionIf there's more than one route in the current stack and when popAndPushNamed is called(or calling pop and then push), the following exception will be thrown:
I/flutter ( 6710): NoSuchMethodError: The getter 'subtreeContext' was called on null.
I/flutter ( 6710): Receiver: null
I/flutter ( 6710): Tried calling: subtreeContext
I/flutter ( 6710): When the exception was thrown, this was the stack:
I/flutter ( 6710): #0      Object._noSuchMethod (dart:core-patch/object_patch.dart:44)
I/flutter ( 6710): #1      Object.noSuchMethod (dart:core-patch/object_patch.dart:47)
I/flutter ( 6710): #2      HeroController._updateQuest (package:flutter/src/widgets/heroes.dart:579)
I/flutter ( 6710): #3      BindingBase&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:653)
I/flutter ( 6710): #4      BindingBase&SchedulerBinding.handleBeginFrame (package:flutter/src/scheduler/binding.dart:596)
I/flutter ( 6710): #5      _beginFrame (file:///b/build/slave/Linux_Engine/build/src/flutter/lib/ui/hooks.dart:83)",3
,2694,144,14,10656,"Example test failures visible at https://travis-ci.org/flutter/plugins/jobs/242412801
RUNNING battery tests...
Running ""flutter packages get"" in battery...          4.5s
00:01 +0 -1: loading /home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart [E]
  Failed to load ""/home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart"": Failed assertion: boolean expression must not be null
  package:test            test
  battery_test.dart 24:3  main
  ===== asynchronous gap ===========================
  package:test            serializeSuite
  listener.dart 17:27     main
  
00:01 +0 -1: Some tests failed.",Make `flutter test` verify that target package depends on `flutter_test`,"Make `flutter test` verify that target package depends on `flutter_test`Example test failures visible at https://travis-ci.org/flutter/plugins/jobs/242412801
RUNNING battery tests...
Running ""flutter packages get"" in battery...          4.5s
00:01 +0 -1: loading /home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart [E]
  Failed to load ""/home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart"": Failed assertion: boolean expression must not be null
  package:test            test
  battery_test.dart 24:3  main
  ===== asynchronous gap ===========================
  package:test            serializeSuite
  listener.dart 17:27     main
  
00:01 +0 -1: Some tests failed.",3
,2695,143,15,11204,"On the Gallery App homepage in Android, select an item in the list of demos and start swiping right multiple times really fast. Continue swiping right when the end of the list is reached. Android will scroll and attempt to focus the next item in the list. However, the Semantics Tree has not been updated yet, focusing the next item fails (because from Android's a11y perspective there is no element) and the boing sound is played. After a couple of swipes, the semantic tree has been updated and everything works as expected.",a11y: Semantics Tree doesn't update fast enough for scrolling,"a11y: Semantics Tree doesn't update fast enough for scrollingOn the Gallery App homepage in Android, select an item in the list of demos and start swiping right multiple times really fast. Continue swiping right when the end of the list is reached. Android will scroll and attempt to focus the next item in the list. However, the Semantics Tree has not been updated yet, focusing the next item fails (because from Android's a11y perspective there is no element) and the boing sound is played. After a couple of swipes, the semantic tree has been updated and everything works as expected.",3
,2696,143,16,32946,"We should identify the right balance point between loading the embedding / vm / isolate / framework / userland element tree / resources in the tree based on each parts load time vs its memory use.
This involves #32945 to audit cold init time and also auditing memory consumption.
Our recommended embedding strategy + default behaviors from APIs should be reasonably balanced between fast loading and memory usage.
Document alterations users can make given those APIs such as prewarming/keeping the vm only, prewarming/keeping the isolate. Flutter side APIs to flush parts of the tree when window is not visible.",Offer options that balances embeddable Flutter load time with resource consumption,"Offer options that balances embeddable Flutter load time with resource consumptionWe should identify the right balance point between loading the embedding / vm / isolate / framework / userland element tree / resources in the tree based on each parts load time vs its memory use.
This involves #32945 to audit cold init time and also auditing memory consumption.
Our recommended embedding strategy + default behaviors from APIs should be reasonably balanced between fast loading and memory usage.
Document alterations users can make given those APIs such as prewarming/keeping the vm only, prewarming/keeping the isolate. Flutter side APIs to flush parts of the tree when window is not visible.",3
,2697,144,17,26063,my device is Redmi note 5,app run crash after plugin image picker installed,app run crash after plugin image picker installedmy device is Redmi note 5,3
,2698,144,18,25167,"I'm pretty sorry for the lack of information, and I'm sure this is something I'm doing wrong, but I don't know where else I can ask. I have a pretty small application that shows a list of values from a static file. I uploaded it to iTunes Connect and downloaded it using TestFlight. Everything works just fine in my iPhone X.
I have modified the application to add a Firebase connection with a Firestore DDBB. If I run the application from Android Studio directly in my phone, it works, and it shows the data from the database. If I archive the app and upload it to iTunes Connect, and install it through TestFlight, when opening the application, it just appears as a white screen. The application works but shows nothing, but an empty white screen.
Do you know what is happening or how I can get information that can help to solve my problem?
Regards",Empty application after publishing it in iTunes Connect (Firebase related),"Empty application after publishing it in iTunes Connect (Firebase related)I'm pretty sorry for the lack of information, and I'm sure this is something I'm doing wrong, but I don't know where else I can ask. I have a pretty small application that shows a list of values from a static file. I uploaded it to iTunes Connect and downloaded it using TestFlight. Everything works just fine in my iPhone X.
I have modified the application to add a Firebase connection with a Firestore DDBB. If I run the application from Android Studio directly in my phone, it works, and it shows the data from the database. If I archive the app and upload it to iTunes Connect, and install it through TestFlight, when opening the application, it just appears as a white screen. The application works but shows nothing, but an empty white screen.
Do you know what is happening or how I can get information that can help to solve my problem?
Regards",3
,2699,146,19,14076,"There are multiple Cupertino concepts of a modal dismissable action sheet at the bottom of the screen too such as
https://developer.apple.com/ios/human-interface-guidelines/views/action-sheets/ and https://developer.apple.com/ios/human-interface-guidelines/controls/pickers/. The current showModalBottomSheet helper mechanism is in bottom_sheet.dart and has a lot of Material concepts baked in. Extract into common utility.",Extract modal bottom action sheet concept from Material to Widgets,"Extract modal bottom action sheet concept from Material to WidgetsThere are multiple Cupertino concepts of a modal dismissable action sheet at the bottom of the screen too such as
https://developer.apple.com/ios/human-interface-guidelines/views/action-sheets/ and https://developer.apple.com/ios/human-interface-guidelines/controls/pickers/. The current showModalBottomSheet helper mechanism is in bottom_sheet.dart and has a lot of Material concepts baked in. Extract into common utility.",3
,2700,146,20,24969,"Steps to Reproduce

Create a custom Status Bar color with

SystemChrome.setSystemUIOverlayStyle(
  SystemUiOverlayStyle.dark.copyWith(
statusBarColor: Colors.red,
));

Then create an AlertDialog that shows up on an event

new AlertDialog(
                    title: new Text(""Something went wrong!""),
                    content:
                        new Text(""Be sure you are connected to the Device""),
                    actions: [
                      new FlatButton(
                          child: const Text(""Ok""),
                          onPressed: () {
                            Navigator.pop(context);
                            _alertShown = 0;
                          }),
                    ])
When the AlertDialog is fired up the custom status bar color stays bright and its opacity is not affected",Should the modal barrier of an Alert also change the color of the status bar?,"Should the modal barrier of an Alert also change the color of the status bar?Steps to Reproduce

Create a custom Status Bar color with

SystemChrome.setSystemUIOverlayStyle(
  SystemUiOverlayStyle.dark.copyWith(
statusBarColor: Colors.red,
));

Then create an AlertDialog that shows up on an event

new AlertDialog(
                    title: new Text(""Something went wrong!""),
                    content:
                        new Text(""Be sure you are connected to the Device""),
                    actions: [
                      new FlatButton(
                          child: const Text(""Ok""),
                          onPressed: () {
                            Navigator.pop(context);
                            _alertShown = 0;
                          }),
                    ])
When the AlertDialog is fired up the custom status bar color stays bright and its opacity is not affected",3
,2701,145,21,32445,"Steps to Reproduce

Create an app with a widget that contains a WebView.
In that widget, add a WillPopScope widget.
In the WillPopScope.onWillPop event handler, call showDialog() to show a dialog.

Flutter doctor output
[] Flutter (Channel beta, v0.11.9, on Mac OS X 10.14.4 18E227, locale en-US)
 Flutter version 0.11.9 at /Users/cvolzke/development/flutter
 Framework revision d48e6e4 (6 months ago), 2018-11-20 22:05:23 -0500
 Engine revision 5c81474
 Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)
[!] Android toolchain - develop for Android devices (Android SDK 28.0.3)
 Android SDK at /Users/cvolzke/Library/Android/sdk
 Android NDK location not configured (optional; useful for native profiling support)
 Platform android-stable, build-tools 28.0.3
 Java binary at: /Library/Java/JavaVirtualMachines/jdk-10-latest/Contents/Home/bin/java
 Java version Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)
 Android license status unknown.
[!] iOS toolchain - develop for iOS devices (Xcode 10.0)
 Xcode at /Applications/Xcode.app/Contents/Developer
 Xcode 10.0, Build version 10A255
 Verify that all connected devices have been paired with this computer in Xcode.
If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.
To update with Brew, run:
brew update
brew uninstall --ignore-dependencies libimobiledevice
brew uninstall --ignore-dependencies usbmuxd
brew install --HEAD usbmuxd
brew unlink usbmuxd
brew link usbmuxd
brew install --HEAD libimobiledevice
brew install ideviceinstaller
 ios-deploy 1.9.4
 CocoaPods version 1.5.3
[!] Android Studio (not installed)
 Android Studio not found; download from https://developer.android.com/studio/index.html
(or visit https://flutter.io/setup/#android-setup for detailed instructions).
[!] IntelliJ IDEA Ultimate Edition (version 2018.1.4)
 IntelliJ at /Applications/IntelliJ IDEA.app
 Flutter plugin not installed; this adds Flutter specific functionality.
 Dart plugin not installed; this adds Dart specific functionality.
 For information about installing plugins, see
https://flutter.io/intellij-setup/#installing-the-plugins
[!] Connected device
! No devices available",webview_flutter: showDialog displays the dialog behind any webviews,"webview_flutter: showDialog displays the dialog behind any webviewsSteps to Reproduce

Create an app with a widget that contains a WebView.
In that widget, add a WillPopScope widget.
In the WillPopScope.onWillPop event handler, call showDialog() to show a dialog.

Flutter doctor output
[] Flutter (Channel beta, v0.11.9, on Mac OS X 10.14.4 18E227, locale en-US)
 Flutter version 0.11.9 at /Users/cvolzke/development/flutter
 Framework revision d48e6e4 (6 months ago), 2018-11-20 22:05:23 -0500
 Engine revision 5c81474
 Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)
[!] Android toolchain - develop for Android devices (Android SDK 28.0.3)
 Android SDK at /Users/cvolzke/Library/Android/sdk
 Android NDK location not configured (optional; useful for native profiling support)
 Platform android-stable, build-tools 28.0.3
 Java binary at: /Library/Java/JavaVirtualMachines/jdk-10-latest/Contents/Home/bin/java
 Java version Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)
 Android license status unknown.
[!] iOS toolchain - develop for iOS devices (Xcode 10.0)
 Xcode at /Applications/Xcode.app/Contents/Developer
 Xcode 10.0, Build version 10A255
 Verify that all connected devices have been paired with this computer in Xcode.
If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.
To update with Brew, run:
brew update
brew uninstall --ignore-dependencies libimobiledevice
brew uninstall --ignore-dependencies usbmuxd
brew install --HEAD usbmuxd
brew unlink usbmuxd
brew link usbmuxd
brew install --HEAD libimobiledevice
brew install ideviceinstaller
 ios-deploy 1.9.4
 CocoaPods version 1.5.3
[!] Android Studio (not installed)
 Android Studio not found; download from https://developer.android.com/studio/index.html
(or visit https://flutter.io/setup/#android-setup for detailed instructions).
[!] IntelliJ IDEA Ultimate Edition (version 2018.1.4)
 IntelliJ at /Applications/IntelliJ IDEA.app
 Flutter plugin not installed; this adds Flutter specific functionality.
 Dart plugin not installed; this adds Dart specific functionality.
 For information about installing plugins, see
https://flutter.io/intellij-setup/#installing-the-plugins
[!] Connected device
! No devices available",3
,2702,144,22,30434,"void removeChildRenderObject(RenderObject child) {
    assert(child is RenderBox);
    assert(renderObject.childToSlot.keys.contains(child));
    _updateRenderObject(null, renderObject.childToSlot[child]);
    assert(!renderObject.childToSlot.keys.contains(child));
    assert(!renderObject.slotToChild.keys.contains(slot));
  }

The last line  assert(!renderObject.slotToChild.keys.contains(slot)) seems incorrect, since slot is for _RenderDecorationElement. It should assert !renderObject.slotToChild.keys.contains(renderObject.childToSlot[child]));",assertion in material/input_decorator.dart seems not correct,"assertion in material/input_decorator.dart seems not correctvoid removeChildRenderObject(RenderObject child) {
    assert(child is RenderBox);
    assert(renderObject.childToSlot.keys.contains(child));
    _updateRenderObject(null, renderObject.childToSlot[child]);
    assert(!renderObject.childToSlot.keys.contains(child));
    assert(!renderObject.slotToChild.keys.contains(slot));
  }

The last line  assert(!renderObject.slotToChild.keys.contains(slot)) seems incorrect, since slot is for _RenderDecorationElement. It should assert !renderObject.slotToChild.keys.contains(renderObject.childToSlot[child]));",3
,2703,146,23,839,"Ran flutter init and launched on an iOS simulator using https://github.com/flutter/engine/wiki/Flutter-Apps-on-iOS
Got an invisible toolbar background (you can faintly see the white text overlaid on it):

Running on an iOS device works fine.
Other sample apps are also having issues with the simulator, but this is probably the simplest reduced test case.",iOS simulator builds have clear toolbar,"iOS simulator builds have clear toolbarRan flutter init and launched on an iOS simulator using https://github.com/flutter/engine/wiki/Flutter-Apps-on-iOS
Got an invisible toolbar background (you can faintly see the white text overlaid on it):

Running on an iOS device works fine.
Other sample apps are also having issues with the simulator, but this is probably the simplest reduced test case.",3
,2704,142,24,1147,"We should have a mode where we show the keylines from this picture, exactly the way they're shown in this picture:
https://www.google.com/design/spec/layout/metrics-keylines.html#metrics-keylines-ratio-keylines",MaterialApp should have a debugShowKeylines mode,"MaterialApp should have a debugShowKeylines modeWe should have a mode where we show the keylines from this picture, exactly the way they're shown in this picture:
https://www.google.com/design/spec/layout/metrics-keylines.html#metrics-keylines-ratio-keylines",3
,2705,142,25,11760,"This is regarding firebase_database 0.0.14
I noticed that the itemBuilder for FirebaseAnimatedList has the following typedef:
typedef Widget FirebaseAnimatedListItemBuilder(
  BuildContext context,
  DataSnapshot snapshot,
  Animation<double> animation,
);

The itemBuilder of AnimatedList has the following typedef:
typedef Widget AnimatedListItemBuilder(
  BuildContext context,
  int index, 
  Animation<double> animation,
);

The index of the current element being rendered is essentially being hidden by FirebaseAnimatedList. It would be very useful for creating numbered lists / striped lists etc.
Is this omission by design? Can we add index to FirebaseAnimatedListItemBuilder?
I am aware that this is a breaking change so I'm open to feedback.",firebase_database: FirebaseAnimatedList should expose index to itemBuilder,"firebase_database: FirebaseAnimatedList should expose index to itemBuilderThis is regarding firebase_database 0.0.14
I noticed that the itemBuilder for FirebaseAnimatedList has the following typedef:
typedef Widget FirebaseAnimatedListItemBuilder(
  BuildContext context,
  DataSnapshot snapshot,
  Animation<double> animation,
);

The itemBuilder of AnimatedList has the following typedef:
typedef Widget AnimatedListItemBuilder(
  BuildContext context,
  int index, 
  Animation<double> animation,
);

The index of the current element being rendered is essentially being hidden by FirebaseAnimatedList. It would be very useful for creating numbered lists / striped lists etc.
Is this omission by design? Can we add index to FirebaseAnimatedListItemBuilder?
I am aware that this is a breaking change so I'm open to feedback.",3
,2706,143,26,32156,"This is a tracking bug for the collection of sub-issues around improving Flutter's memory consumption, and offering developers more flexibility in choosing where to set the dial on the time performance vs memory consumption spectrum.
Related issues:

 #13493: Flutter should provide more control over image caching
  #26187: Flutter should be smarter about memory limits for images
 #16995: Free resources after a Flutter view is disposed
 #19358: Unmount everything and dispose states when host activity dies
 #23231: Memory leaks on iOS
 #20690: More deterministic measurement of memory consumption.
 #26081: Disable in-memory decoded frame cache by default
 #26443: Consider mipmapping ui.Images generated by the engine
 #15479: OnMemoryPressure is unreliable on Android
 #25155: Raster cache images may be much bigger than the visible/clipped area
 #19558: IO thread GrContext memory needs to be cleaned up
 #44013: Persist the memory profile timeline",Improve Flutter's memory consumption,"Improve Flutter's memory consumptionThis is a tracking bug for the collection of sub-issues around improving Flutter's memory consumption, and offering developers more flexibility in choosing where to set the dial on the time performance vs memory consumption spectrum.
Related issues:

 #13493: Flutter should provide more control over image caching
  #26187: Flutter should be smarter about memory limits for images
 #16995: Free resources after a Flutter view is disposed
 #19358: Unmount everything and dispose states when host activity dies
 #23231: Memory leaks on iOS
 #20690: More deterministic measurement of memory consumption.
 #26081: Disable in-memory decoded frame cache by default
 #26443: Consider mipmapping ui.Images generated by the engine
 #15479: OnMemoryPressure is unreliable on Android
 #25155: Raster cache images may be much bigger than the visible/clipped area
 #19558: IO thread GrContext memory needs to be cleaned up
 #44013: Persist the memory profile timeline",3
,2707,148,27,3112,https://www.google.com/design/spec/components/progress-activity.html#progress-activity-types-of-indicators,"Support progress bar ""query indeterminate"" mode","Support progress bar ""query indeterminate"" modehttps://www.google.com/design/spec/components/progress-activity.html#progress-activity-types-of-indicators",3
,2708,148,28,28647,"Steps to Reproduce


Plugin added to pubspec.yaml. Below is the list of dependencies section:

dependencies:
  flutter:
    sdk: flutter
  flutter_colorpicker: any
  logging: ^0.11.3+2
  date_calendar: ^0.2.0
  shared_preferences: ^0.4.3
  intl: ^0.15.7
  flutter_localizations:
    sdk: flutter
  fluro: ^1.4.0
  rxdart: ^0.20.0
  sqflite: ^1.1.0
  json_serializable: ^2.0.2
  charts_flutter: ^0.5.0
  flutter_local_notifications: 0.5.1+2
  flutter_calendar_carousel: 1.3.13
  sprintf: ^4.0.2
  firebase_admob: 0.7.0

Used version 0.7.0 according to AndroidX compatibility list

Added to AndroidManifest.xml

<meta-data
            android:name=""com.google.android.gms.ads.APP_ID""
            android:value=""[ADMOB_APP_ID]""/>

where:

APP_ID is my app id e.g. com.program.myapp and
ADMOB_APP_ID is the AdMob id (I have registered AdMob account)

Note: My app is NOT yet published on Google play

Added to android level build.gradle

subprojects {
    project.configurations.all {
        resolutionStrategy.eachDependency { details ->
            if (details.requested.group == 'androidx.core' &&
                    !details.requested.name.contains('androidx')) {
                details.useVersion ""1.0.1""
            }
        }
    }
}


Added to app level build.gradle

dependencies {
    ....
    implementation 'com.google.android.gms:play-services-ads:17.1.1'
}


Start building. Below is the output

D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
	at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)
	at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)
	at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)
	at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)
	at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)
	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)
	at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)
	at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)
	at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.
	at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)
	at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)
	... 6 more
Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete
	at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)
	at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)
	at com.android.tools.r8.D8.run(D8.java:90)
	at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)
	... 7 more
Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
	at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)
	at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)
	at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)
	... 10 more


FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.
> com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 10s
Finished with error: Gradle task assembleDevelopmentDebug failed with exit code 1


Logs

flutter run --verbose log
[+2400 ms] D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
[  +76 ms] D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
[        ] com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\c
aches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
[   +5 ms]      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[        ]      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[        ]      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[        ]      at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[   +6 ms]      at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
[   +1 ms]      at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
[  +15 ms]      at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)
[   +1 ms]      at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)
[        ]      at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)
[        ]      at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)
[        ]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[        ]      at java.lang.reflect.Method.invoke(Method.java:498)
[        ]      at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
[        ]      at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)
[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)
[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)
[        ]      at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)
[        ]      at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
[        ]      at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)
[        ]      at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)
[        ]      at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)
[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)
[        ]      at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
[        ]      at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)
[        ]      at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)
[        ]      at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
[        ]      at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
[        ]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[        ]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[        ]      at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
[        ]      at java.lang.Thread.run(Thread.java:745)
[        ] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-fireba
se-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)
[        ]      at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
[        ]      at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
[        ]      at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
[        ]      at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
[   +1 ms]      at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
[  +12 ms] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)
[        ]      ... 6 more
[        ] Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)
[        ]      at com.android.tools.r8.D8.run(D8.java:90)
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)
[        ]      ... 7 more
[        ] Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `c
om.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
[        ]      at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)
[   +1 ms]      at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)
[        ]      ... 10 more
[  +73 ms] FAILURE: Build failed with an exception.
[        ] * What went wrong:
[        ] Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.
[        ] > com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilde
rException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\cla
sses.jar
[        ] * Try:
[        ] Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.
[        ] * Get more help at https://help.gradle.org
[        ] BUILD FAILED in 9s
[        ] *******************************************************************************************
[   +1 ms] The Gradle failure may have been because of AndroidX incompatibilities in this Flutter app.
[  +13 ms] See https://goo.gl/CP92wY for more information on the problem and how to fix it.
[        ] *******************************************************************************************
Gradle task assembleDevelopmentDebug failed with exit code 1

#0      throwToolExit (package:flutter_tools/src/base/common.dart:24:3)
#1      _buildGradleProjectV2 (package:flutter_tools/src/android/gradle.dart:462:5)
<asynchronous suspension>
#2      buildGradleProject (package:flutter_tools/src/android/gradle.dart:331:14)
<asynchronous suspension>
#3      buildApk (package:flutter_tools/src/android/apk.dart:43:10)
<asynchronous suspension>
#4      AndroidDevice.startApp (package:flutter_tools/src/android/android_device.dart:378:13)
<asynchronous suspension>
#5      FlutterDevice.runHot (package:flutter_tools/src/resident_runner.dart:308:54)
<asynchronous suspension>
#6      HotRunner.run (package:flutter_tools/src/run_hot.dart:295:39)
<asynchronous suspension>
#7      RunCommand.runCommand (package:flutter_tools/src/commands/run.dart:405:37)
<asynchronous suspension>
#8      FlutterCommand.verifyThenRunCommand (package:flutter_tools/src/runner/flutter_command.dart:545:18)
#9      _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#10     _rootRunUnary (dart:async/zone.dart:1132:38)
#11     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#12     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#13     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#14     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#15     Future._complete (dart:async/future_impl.dart:473:7)
#16     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#17     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)
#18     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)
#19     RunCommand.usageValues (package:flutter_tools/src/commands/run.dart)
#20     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#21     _rootRunUnary (dart:async/zone.dart:1132:38)
#22     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#23     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#24     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#25     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#26     Future._complete (dart:async/future_impl.dart:473:7)
#27     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#28     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)
#29     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)
#30     AndroidDevice.targetPlatform (package:flutter_tools/src/android/android_device.dart)
#31     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#32     _rootRunUnary (dart:async/zone.dart:1132:38)
#33     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#34     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#35     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#36     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#37     Future._complete (dart:async/future_impl.dart:473:7)
#38     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#39     _AsyncAwaitCompleter.complete.<anonymous closure> (dart:async/runtime/libasync_patch.dart:33:20)
#40     _rootRun (dart:async/zone.dart:1124:13)
#41     _CustomZone.run (dart:async/zone.dart:1021:19)
#42     _CustomZone.bindCallback.<anonymous closure> (dart:async/zone.dart:947:23)
#43     _microtaskLoop (dart:async/schedule_microtask.dart:41:21)
#44     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50:5)
#45     _runPendingImmediateCallback (dart:isolate/runtime/libisolate_patch.dart:115:13)
#46     _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:172:5)





flutter doctor -v log
[] Flutter (Channel stable, v1.2.1, on Microsoft Windows [Version 10.0.17134.590], locale en-US)
     Flutter version 1.2.1 at E:\DevTools\flutter
     Framework revision 8661d8aecd (13 days ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at E:\DevTools\Android\sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     ANDROID_HOME = E:\DevTools\Android\sdk
     Java binary at: E:\DevTools\android-studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.3)
     Android Studio at E:\DevTools\android-studio
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] Connected device (1 available)
     Android SDK built for x86 64  emulator-5554  android-x64  Android 8.0.0 (API 26) (emulator)

 No issues found!

Side notes:

In case step 4 is omitted, I can compile the app, but  then I get The Google Mobile Ads SDK was initialized incorrectly. although I believe all other settings are just fine.
I also tried the most recent plugin version and it has the identical behavior.",[firebase_admob] - app compilation crashed after the plugin has been added,"[firebase_admob] - app compilation crashed after the plugin has been addedSteps to Reproduce


Plugin added to pubspec.yaml. Below is the list of dependencies section:

dependencies:
  flutter:
    sdk: flutter
  flutter_colorpicker: any
  logging: ^0.11.3+2
  date_calendar: ^0.2.0
  shared_preferences: ^0.4.3
  intl: ^0.15.7
  flutter_localizations:
    sdk: flutter
  fluro: ^1.4.0
  rxdart: ^0.20.0
  sqflite: ^1.1.0
  json_serializable: ^2.0.2
  charts_flutter: ^0.5.0
  flutter_local_notifications: 0.5.1+2
  flutter_calendar_carousel: 1.3.13
  sprintf: ^4.0.2
  firebase_admob: 0.7.0

Used version 0.7.0 according to AndroidX compatibility list

Added to AndroidManifest.xml

<meta-data
            android:name=""com.google.android.gms.ads.APP_ID""
            android:value=""[ADMOB_APP_ID]""/>

where:

APP_ID is my app id e.g. com.program.myapp and
ADMOB_APP_ID is the AdMob id (I have registered AdMob account)

Note: My app is NOT yet published on Google play

Added to android level build.gradle

subprojects {
    project.configurations.all {
        resolutionStrategy.eachDependency { details ->
            if (details.requested.group == 'androidx.core' &&
                    !details.requested.name.contains('androidx')) {
                details.useVersion ""1.0.1""
            }
        }
    }
}


Added to app level build.gradle

dependencies {
    ....
    implementation 'com.google.android.gms:play-services-ads:17.1.1'
}


Start building. Below is the output

D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
	at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)
	at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)
	at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)
	at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)
	at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)
	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)
	at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)
	at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)
	at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.
	at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)
	at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)
	at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)
	... 6 more
Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete
	at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)
	at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)
	at com.android.tools.r8.D8.run(D8.java:90)
	at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)
	... 7 more
Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
	at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)
	at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)
	at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)
	... 10 more


FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.
> com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 10s
Finished with error: Gradle task assembleDevelopmentDebug failed with exit code 1


Logs

flutter run --verbose log
[+2400 ms] D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
[  +76 ms] D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.
[        ] com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\c
aches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
[   +5 ms]      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[        ]      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[        ]      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[        ]      at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[   +6 ms]      at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
[   +1 ms]      at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
[  +15 ms]      at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)
[   +1 ms]      at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)
[        ]      at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)
[        ]      at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)
[        ]      at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)
[        ]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[        ]      at java.lang.reflect.Method.invoke(Method.java:498)
[        ]      at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
[        ]      at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)
[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)
[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)
[        ]      at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)
[        ]      at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
[        ]      at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)
[        ]      at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)
[        ]      at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)
[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)
[        ]      at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)
[        ]      at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
[        ]      at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
[        ]      at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)
[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)
[        ]      at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)
[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)
[        ]      at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
[        ]      at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
[        ]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[        ]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[        ]      at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
[        ]      at java.lang.Thread.run(Thread.java:745)
[        ] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-fireba
se-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\classes.jar
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)
[        ]      at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
[        ]      at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
[        ]      at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
[        ]      at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
[   +1 ms]      at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
[  +12 ms] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)
[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)
[        ]      ... 6 more
[        ] Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)
[        ]      at com.android.tools.r8.D8.run(D8.java:90)
[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)
[        ]      ... 7 more
[        ] Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `c
om.google.android.gms.internal.measurement.zzfq$zzb$zzb`.
[        ]      at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)
[   +1 ms]      at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)
[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)
[        ]      ... 10 more
[  +73 ms] FAILURE: Build failed with an exception.
[        ] * What went wrong:
[        ] Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.
[        ] > com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilde
rException: Failed to process C:\Users\angel\.gradle\caches\transforms-1\files-1.1\jetified-firebase-analytics-16.0.4.aar\cfb42b1b1914d3415eb1bdec7778a942\jars\cla
sses.jar
[        ] * Try:
[        ] Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.
[        ] * Get more help at https://help.gradle.org
[        ] BUILD FAILED in 9s
[        ] *******************************************************************************************
[   +1 ms] The Gradle failure may have been because of AndroidX incompatibilities in this Flutter app.
[  +13 ms] See https://goo.gl/CP92wY for more information on the problem and how to fix it.
[        ] *******************************************************************************************
Gradle task assembleDevelopmentDebug failed with exit code 1

#0      throwToolExit (package:flutter_tools/src/base/common.dart:24:3)
#1      _buildGradleProjectV2 (package:flutter_tools/src/android/gradle.dart:462:5)
<asynchronous suspension>
#2      buildGradleProject (package:flutter_tools/src/android/gradle.dart:331:14)
<asynchronous suspension>
#3      buildApk (package:flutter_tools/src/android/apk.dart:43:10)
<asynchronous suspension>
#4      AndroidDevice.startApp (package:flutter_tools/src/android/android_device.dart:378:13)
<asynchronous suspension>
#5      FlutterDevice.runHot (package:flutter_tools/src/resident_runner.dart:308:54)
<asynchronous suspension>
#6      HotRunner.run (package:flutter_tools/src/run_hot.dart:295:39)
<asynchronous suspension>
#7      RunCommand.runCommand (package:flutter_tools/src/commands/run.dart:405:37)
<asynchronous suspension>
#8      FlutterCommand.verifyThenRunCommand (package:flutter_tools/src/runner/flutter_command.dart:545:18)
#9      _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#10     _rootRunUnary (dart:async/zone.dart:1132:38)
#11     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#12     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#13     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#14     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#15     Future._complete (dart:async/future_impl.dart:473:7)
#16     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#17     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)
#18     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)
#19     RunCommand.usageValues (package:flutter_tools/src/commands/run.dart)
#20     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#21     _rootRunUnary (dart:async/zone.dart:1132:38)
#22     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#23     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#24     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#25     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#26     Future._complete (dart:async/future_impl.dart:473:7)
#27     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#28     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)
#29     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)
#30     AndroidDevice.targetPlatform (package:flutter_tools/src/android/android_device.dart)
#31     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)
#32     _rootRunUnary (dart:async/zone.dart:1132:38)
#33     _CustomZone.runUnary (dart:async/zone.dart:1029:19)
#34     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)
#35     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)
#36     Future._propagateToListeners (dart:async/future_impl.dart:668:32)
#37     Future._complete (dart:async/future_impl.dart:473:7)
#38     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)
#39     _AsyncAwaitCompleter.complete.<anonymous closure> (dart:async/runtime/libasync_patch.dart:33:20)
#40     _rootRun (dart:async/zone.dart:1124:13)
#41     _CustomZone.run (dart:async/zone.dart:1021:19)
#42     _CustomZone.bindCallback.<anonymous closure> (dart:async/zone.dart:947:23)
#43     _microtaskLoop (dart:async/schedule_microtask.dart:41:21)
#44     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50:5)
#45     _runPendingImmediateCallback (dart:isolate/runtime/libisolate_patch.dart:115:13)
#46     _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:172:5)





flutter doctor -v log
[] Flutter (Channel stable, v1.2.1, on Microsoft Windows [Version 10.0.17134.590], locale en-US)
     Flutter version 1.2.1 at E:\DevTools\flutter
     Framework revision 8661d8aecd (13 days ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at E:\DevTools\Android\sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     ANDROID_HOME = E:\DevTools\Android\sdk
     Java binary at: E:\DevTools\android-studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.3)
     Android Studio at E:\DevTools\android-studio
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] Connected device (1 available)
     Android SDK built for x86 64  emulator-5554  android-x64  Android 8.0.0 (API 26) (emulator)

 No issues found!

Side notes:

In case step 4 is omitted, I can compile the app, but  then I get The Google Mobile Ads SDK was initialized incorrectly. although I believe all other settings are just fine.
I also tried the most recent plugin version and it has the identical behavior.",3
,2709,148,29,18675,"#Hi! I copy the example from this link:
https://github.com/flutter/plugins/tree/master/packages/firebase_messaging/example
But it works only with app in foreground or in background.
If I send a notification with the app killed from task manager, the notification doesn't show.
This is what I send on body :
{
  ""notification"": {""body"": ""message"",""title"": ""this is a title""}, 
  ""priority"": ""high"",
  ""data"": {""click_action"": ""FLUTTER_NOTIFICATION_CLICK"", ""id"": ""1"", ""status"": ""done""}, 
  ""to"": ""myToken""
}
Someone can help me?",firebase_messaging notification doesn't show if app is killed,"firebase_messaging notification doesn't show if app is killed#Hi! I copy the example from this link:
https://github.com/flutter/plugins/tree/master/packages/firebase_messaging/example
But it works only with app in foreground or in background.
If I send a notification with the app killed from task manager, the notification doesn't show.
This is what I send on body :
{
  ""notification"": {""body"": ""message"",""title"": ""this is a title""}, 
  ""priority"": ""high"",
  ""data"": {""click_action"": ""FLUTTER_NOTIFICATION_CLICK"", ""id"": ""1"", ""status"": ""done""}, 
  ""to"": ""myToken""
}
Someone can help me?",3
,2710,148,30,17978,"So far I've the plugin (standalone) and the app created. Now I need to (somehow) use and consume a static framework (obj-c) in iOS plugin.
I've the file plugin.h and plugin.m and I added the mylib.framework to the root of iOS folder, next to podspec file. Also tried to add the s.dependency 'mylib' to it. Still, it doesn't seem to work at all. Everytime I try to flutter build ios on my main app (using the plugin) I get the  #import <myplugin/myplugin.h> not found
PS: The library works if I add to the iOS project on the Flutter app, but I MUST use it on a plugin.",Add iOS dependency to Flutter Plugin ,"Add iOS dependency to Flutter Plugin So far I've the plugin (standalone) and the app created. Now I need to (somehow) use and consume a static framework (obj-c) in iOS plugin.
I've the file plugin.h and plugin.m and I added the mylib.framework to the root of iOS folder, next to podspec file. Also tried to add the s.dependency 'mylib' to it. Still, it doesn't seem to work at all. Everytime I try to flutter build ios on my main app (using the plugin) I get the  #import <myplugin/myplugin.h> not found
PS: The library works if I add to the iOS project on the Flutter app, but I MUST use it on a plugin.",3
,2711,148,31,26292,"Hi guys, it's normal for the mobile version of the site to show up like that in the widget catalog and the Widget index",The site don't work fine,"The site don't work fineHi guys, it's normal for the mobile version of the site to show up like that in the widget catalog and the Widget index",3
,2712,148,32,32472,"l am writing an application with flutter and it works well at my android device ,but it builds failed  when l click ""open for editing in android studio"" ,hope someone can help me,
flutter doctor:
[] Flutter (Channel stable, v1.5.4, on Mac OS X 10.14.3 18D109, locale zh-Hans-CN)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 10.2.1)
[] Android Studio (version 3.4)
[] IntelliJ IDEA Ultimate Edition (version 2019.1.1)
[] Connected device (1 available)

here are the error logs:
org.gradle.process.internal.ExecException: Process 'command '/Users/lierdong/development/flutter/bin/flutter'' finished with non-zero exit value 1
	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:396)
	at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:37)
	at org.gradle.api.internal.file.DefaultFileOperations.exec(DefaultFileOperations.java:234)
	at org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1113)
	at org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1108)
	at org.gradle.api.Project$exec$7.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:115)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)
	at BaseFlutterTask.buildBundle(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:529)
	at BaseFlutterTask$buildBundle.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:156)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:160)
	at FlutterTask.build(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:667)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:704)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:671)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$2.run(ExecuteActionsTaskExecuter.java:284)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:273)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:258)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:67)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:145)
	at org.gradle.internal.execution.impl.steps.ExecuteStep.execute(ExecuteStep.java:49)
	at org.gradle.internal.execution.impl.steps.CancelExecutionStep.execute(CancelExecutionStep.java:34)
	at org.gradle.internal.execution.impl.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:69)
	at org.gradle.internal.execution.impl.steps.TimeoutStep.execute(TimeoutStep.java:49)
	at org.gradle.internal.execution.impl.steps.CatchExceptionStep.execute(CatchExceptionStep.java:33)
	at org.gradle.internal.execution.impl.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)
	at org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:43)
	at org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:29)
	at org.gradle.internal.execution.impl.steps.CacheStep.executeWithoutCache(CacheStep.java:134)
	at org.gradle.internal.execution.impl.steps.CacheStep.lambda$execute$3(CacheStep.java:83)
	at java.util.Optional.orElseGet(Optional.java:267)
	at org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:82)
	at org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:36)
	at org.gradle.internal.execution.impl.steps.PrepareCachingStep.execute(PrepareCachingStep.java:33)
	at org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:38)
	at org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:23)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)
	at java.util.Optional.map(Optional.java:215)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:52)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:36)
	at org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:34)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:57)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:119)
	at org.gradle.api.internal.tasks.execution.ResolvePreviousStateExecuter.execute(ResolvePreviousStateExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:93)
	at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:45)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:94)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:56)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:55)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:67)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)
	at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)
	at org.gradle.execution.plan.DefaultPlanExecutor.process(DefaultPlanExecutor.java:74)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.executeWithServices(DefaultTaskExecutionGraph.java:178)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.execute(DefaultTaskExecutionGraph.java:154)
	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:41)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)
	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:24)
	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:46)
	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:49)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:33)
	at org.gradle.initialization.DefaultGradleLauncher$ExecuteTasks.run(DefaultGradleLauncher.java:383)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.initialization.DefaultGradleLauncher.runTasks(DefaultGradleLauncher.java:247)
	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:159)
	at org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:134)
	at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:58)
	at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:55)
	at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:82)
	at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:75)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:183)
	at org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)
	at org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:75)
	at org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:55)
	at org.gradle.tooling.internal.provider.runner.BuildModelActionRunner.run(BuildModelActionRunner.java:54)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:58)
	at org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)
	at org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:39)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:49)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:44)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:44)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:49)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)
	at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:78)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:31)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:42)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:28)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:78)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:52)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:59)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:36)
	at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:68)
	at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:38)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:37)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:26)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:60)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:55)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:41)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:48)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:32)
	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)
	at org.gradle.util.Swapper.swap(Swapper.java:38)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:81)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)
	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)",Can not make project when open for editing in android studio,"Can not make project when open for editing in android studiol am writing an application with flutter and it works well at my android device ,but it builds failed  when l click ""open for editing in android studio"" ,hope someone can help me,
flutter doctor:
[] Flutter (Channel stable, v1.5.4, on Mac OS X 10.14.3 18D109, locale zh-Hans-CN)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 10.2.1)
[] Android Studio (version 3.4)
[] IntelliJ IDEA Ultimate Edition (version 2019.1.1)
[] Connected device (1 available)

here are the error logs:
org.gradle.process.internal.ExecException: Process 'command '/Users/lierdong/development/flutter/bin/flutter'' finished with non-zero exit value 1
	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:396)
	at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:37)
	at org.gradle.api.internal.file.DefaultFileOperations.exec(DefaultFileOperations.java:234)
	at org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1113)
	at org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1108)
	at org.gradle.api.Project$exec$7.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:115)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)
	at BaseFlutterTask.buildBundle(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:529)
	at BaseFlutterTask$buildBundle.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:156)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:160)
	at FlutterTask.build(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:667)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)
	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:704)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:671)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$2.run(ExecuteActionsTaskExecuter.java:284)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:273)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:258)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:67)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:145)
	at org.gradle.internal.execution.impl.steps.ExecuteStep.execute(ExecuteStep.java:49)
	at org.gradle.internal.execution.impl.steps.CancelExecutionStep.execute(CancelExecutionStep.java:34)
	at org.gradle.internal.execution.impl.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:69)
	at org.gradle.internal.execution.impl.steps.TimeoutStep.execute(TimeoutStep.java:49)
	at org.gradle.internal.execution.impl.steps.CatchExceptionStep.execute(CatchExceptionStep.java:33)
	at org.gradle.internal.execution.impl.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)
	at org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:43)
	at org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:29)
	at org.gradle.internal.execution.impl.steps.CacheStep.executeWithoutCache(CacheStep.java:134)
	at org.gradle.internal.execution.impl.steps.CacheStep.lambda$execute$3(CacheStep.java:83)
	at java.util.Optional.orElseGet(Optional.java:267)
	at org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:82)
	at org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:36)
	at org.gradle.internal.execution.impl.steps.PrepareCachingStep.execute(PrepareCachingStep.java:33)
	at org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:38)
	at org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:23)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)
	at java.util.Optional.map(Optional.java:215)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:52)
	at org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:36)
	at org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:34)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:91)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:57)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:119)
	at org.gradle.api.internal.tasks.execution.ResolvePreviousStateExecuter.execute(ResolvePreviousStateExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:93)
	at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:45)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:94)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:56)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:55)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:67)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)
	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)
	at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)
	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)
	at org.gradle.execution.plan.DefaultPlanExecutor.process(DefaultPlanExecutor.java:74)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.executeWithServices(DefaultTaskExecutionGraph.java:178)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.execute(DefaultTaskExecutionGraph.java:154)
	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:41)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)
	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:24)
	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:46)
	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:49)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)
	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:33)
	at org.gradle.initialization.DefaultGradleLauncher$ExecuteTasks.run(DefaultGradleLauncher.java:383)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.initialization.DefaultGradleLauncher.runTasks(DefaultGradleLauncher.java:247)
	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:159)
	at org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:134)
	at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:58)
	at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:55)
	at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:82)
	at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:75)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:183)
	at org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)
	at org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:75)
	at org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:55)
	at org.gradle.tooling.internal.provider.runner.BuildModelActionRunner.run(BuildModelActionRunner.java:54)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:58)
	at org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)
	at org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:39)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:49)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:44)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:44)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:49)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)
	at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:78)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:31)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:42)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:28)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:78)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:52)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:59)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:36)
	at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:68)
	at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:38)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:37)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:26)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:60)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:55)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:41)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:48)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:32)
	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)
	at org.gradle.util.Swapper.swap(Swapper.java:38)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:81)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)
	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)",3
,2713,144,33,18746,"When I try to create a flutter project by visual studio code, lib and test file always get a red signal and it stay in creating Running ""flutter packages get"" in myapp...   status",Running flutter create myapp by VSCode always have problems,"Running flutter create myapp by VSCode always have problemsWhen I try to create a flutter project by visual studio code, lib and test file always get a red signal and it stay in creating Running ""flutter packages get"" in myapp...   status",3
,2714,148,34,7204,"The ignoreTransform doesn't make much sense in a composited system because the local transform (which you're attempting to ignore) will depend on the compositing strategy, which means the ouput will depend on the compositing strategy.
Apparently the quality is ignored in Skia's GPU backend.",Remove ignoreTransform and highQuality options from MaskFilter,"Remove ignoreTransform and highQuality options from MaskFilterThe ignoreTransform doesn't make much sense in a composited system because the local transform (which you're attempting to ignore) will depend on the compositing strategy, which means the ouput will depend on the compositing strategy.
Apparently the quality is ignored in Skia's GPU backend.",3
,2715,144,35,28984,"Steps to Reproduce
Using Android Studio. I have two different applications (one is a game, and the other is a set of widgets to better manage the space in tablets).


They were running smoothly in flutter 1.1.2 beta for devices from API 16 up to API 27, and emulators from API 21 up to API 28.


After flutter upgrade to flutter 1.2.1 beta, both applications in my device with API 16 fails at the moment of touching anything inside the application with the same error.


Then the application dies and a dialog that reads Unfortunately, <application_name> has stopped and an exception is thrown through the log file.


Both applications works fine in a Samsung Galaxy Nexus API 17 Jelly Bean MR1 and up to a Moto G5 Plus API 27, so this is a problem only in Jelly Bean API 16.


Logs
Both applications start fine. But touching anything in the screen we get the following log error:
[+445864 ms] E/InputEventReceiver( 3225): Exception dispatching input event.
[   +4 ms] E/MessageQueue-JNI( 3225): Exception in MessageQueue callback: handleReceiveCallback
[  +22 ms] E/MessageQueue-JNI( 3225): java.lang.NoSuchMethodError: android.view.MotionEvent.isFromSource
[        ] E/MessageQueue-JNI( 3225):   at io.flutter.view.FlutterView.onGenericMotionEvent(FlutterView.java:590)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEventInternal(View.java:7238)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7219)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow$DecorView.superDispatchGenericMotionEvent(PhoneWindow.java:1950)
[   +1 ms] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow.superDispatchGenericMotionEvent(PhoneWindow.java:1406)
[        ] E/MessageQueue-JNI( 3225):   at android.app.Activity.dispatchGenericMotionEvent(Activity.java:2446)
[        ] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow$DecorView.dispatchGenericMotionEvent(PhoneWindow.java:1904)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchPointerEvent(View.java:7325)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverPointerEvent(ViewRootImpl.java:3350)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverInputEvent(ViewRootImpl.java:3295)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.doProcessInputEvents(ViewRootImpl.java:4331)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.enqueueInputEvent(ViewRootImpl.java:4310)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl$WindowInputEventReceiver.onInputEvent(ViewRootImpl.java:4402)
[        ] E/MessageQueue-JNI( 3225):   at android.view.InputEventReceiver.dispatchInputEvent(InputEventReceiver.java:171)
[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.nativePollOnce(Native Method)
[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.next(MessageQueue.java:125)
[        ] E/MessageQueue-JNI( 3225):   at android.os.Looper.loop(Looper.java:124)
[        ] E/MessageQueue-JNI( 3225):   at android.app.ActivityThread.main(ActivityThread.java:4842)
[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invokeNative(Native Method)
[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invoke(Method.java:511)
[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)
[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:657)
[        ] E/MessageQueue-JNI( 3225):   at dalvik.system.NativeStart.main(Native Method)
[        ] W/dalvikvm( 3225): threadid=1: thread exiting with uncaught exception (group=0x427e2438)

After touching the OK in the termination dialog the log is:

[+88378 ms] Service protocol connection closed.
[   +1 ms] Lost connection to device.
[   +3 ms] DevFS: Deleting filesystem on the device
(file:///data/data/games.appsu.minessweeper/cache/mines_sweeperSWJIMS/mines_sweeper/)
[   +1 ms] Sending to VM service: _deleteDevFS({fsName: mines_sweeper})
[ +258 ms] Ignored error while cleaning up DevFS: TimeoutException after 0:00:00.250000: Future not completed
[   +6 ms] ""flutter run"" took 593,477ms.
[        ] ""flutter run"" took 593,477ms.

flutter analyze results:
Analyzing mines_sweeper...                                              
No issues found! (ran in 13.5s)

flutter doctor -v:
[] Flutter (Channel beta, v1.2.1, on Linux, locale en_US.UTF-8)
     Flutter version 1.2.1 at /home/sarah/flutter
     Framework revision 8661d8aecd (3 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /home/sarah/Android/Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /opt/android-studio/jre/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.4)
     Android Studio at /opt/android-studio
     Flutter plugin version 33.3.2
     Dart plugin version 183.5901
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] IntelliJ IDEA Community Edition (version 2018.3)
     IntelliJ at /opt/idea-IC-183.4588.61
     Flutter plugin version 31.1.4
     Dart plugin version 183.4588.61

[] Connected device (1 available)
     d07ab5034db7  d07ab5034db7  android-arm  Android 4.1.2 (API 16)

 No issues found!",InputEventReceiver exception dispatching input event with Flutter 1.2.1 and Android JellyBean 4.1.2,"InputEventReceiver exception dispatching input event with Flutter 1.2.1 and Android JellyBean 4.1.2Steps to Reproduce
Using Android Studio. I have two different applications (one is a game, and the other is a set of widgets to better manage the space in tablets).


They were running smoothly in flutter 1.1.2 beta for devices from API 16 up to API 27, and emulators from API 21 up to API 28.


After flutter upgrade to flutter 1.2.1 beta, both applications in my device with API 16 fails at the moment of touching anything inside the application with the same error.


Then the application dies and a dialog that reads Unfortunately, <application_name> has stopped and an exception is thrown through the log file.


Both applications works fine in a Samsung Galaxy Nexus API 17 Jelly Bean MR1 and up to a Moto G5 Plus API 27, so this is a problem only in Jelly Bean API 16.


Logs
Both applications start fine. But touching anything in the screen we get the following log error:
[+445864 ms] E/InputEventReceiver( 3225): Exception dispatching input event.
[   +4 ms] E/MessageQueue-JNI( 3225): Exception in MessageQueue callback: handleReceiveCallback
[  +22 ms] E/MessageQueue-JNI( 3225): java.lang.NoSuchMethodError: android.view.MotionEvent.isFromSource
[        ] E/MessageQueue-JNI( 3225):   at io.flutter.view.FlutterView.onGenericMotionEvent(FlutterView.java:590)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEventInternal(View.java:7238)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7219)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)
[        ] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow$DecorView.superDispatchGenericMotionEvent(PhoneWindow.java:1950)
[   +1 ms] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow.superDispatchGenericMotionEvent(PhoneWindow.java:1406)
[        ] E/MessageQueue-JNI( 3225):   at android.app.Activity.dispatchGenericMotionEvent(Activity.java:2446)
[        ] E/MessageQueue-JNI( 3225):   at
com.android.internal.policy.impl.PhoneWindow$DecorView.dispatchGenericMotionEvent(PhoneWindow.java:1904)
[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchPointerEvent(View.java:7325)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverPointerEvent(ViewRootImpl.java:3350)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverInputEvent(ViewRootImpl.java:3295)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.doProcessInputEvents(ViewRootImpl.java:4331)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.enqueueInputEvent(ViewRootImpl.java:4310)
[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl$WindowInputEventReceiver.onInputEvent(ViewRootImpl.java:4402)
[        ] E/MessageQueue-JNI( 3225):   at android.view.InputEventReceiver.dispatchInputEvent(InputEventReceiver.java:171)
[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.nativePollOnce(Native Method)
[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.next(MessageQueue.java:125)
[        ] E/MessageQueue-JNI( 3225):   at android.os.Looper.loop(Looper.java:124)
[        ] E/MessageQueue-JNI( 3225):   at android.app.ActivityThread.main(ActivityThread.java:4842)
[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invokeNative(Native Method)
[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invoke(Method.java:511)
[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)
[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:657)
[        ] E/MessageQueue-JNI( 3225):   at dalvik.system.NativeStart.main(Native Method)
[        ] W/dalvikvm( 3225): threadid=1: thread exiting with uncaught exception (group=0x427e2438)

After touching the OK in the termination dialog the log is:

[+88378 ms] Service protocol connection closed.
[   +1 ms] Lost connection to device.
[   +3 ms] DevFS: Deleting filesystem on the device
(file:///data/data/games.appsu.minessweeper/cache/mines_sweeperSWJIMS/mines_sweeper/)
[   +1 ms] Sending to VM service: _deleteDevFS({fsName: mines_sweeper})
[ +258 ms] Ignored error while cleaning up DevFS: TimeoutException after 0:00:00.250000: Future not completed
[   +6 ms] ""flutter run"" took 593,477ms.
[        ] ""flutter run"" took 593,477ms.

flutter analyze results:
Analyzing mines_sweeper...                                              
No issues found! (ran in 13.5s)

flutter doctor -v:
[] Flutter (Channel beta, v1.2.1, on Linux, locale en_US.UTF-8)
     Flutter version 1.2.1 at /home/sarah/flutter
     Framework revision 8661d8aecd (3 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /home/sarah/Android/Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /opt/android-studio/jre/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     All Android licenses accepted.

[] Android Studio (version 3.4)
     Android Studio at /opt/android-studio
     Flutter plugin version 33.3.2
     Dart plugin version 183.5901
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] IntelliJ IDEA Community Edition (version 2018.3)
     IntelliJ at /opt/idea-IC-183.4588.61
     Flutter plugin version 31.1.4
     Dart plugin version 183.4588.61

[] Connected device (1 available)
     d07ab5034db7  d07ab5034db7  android-arm  Android 4.1.2 (API 16)

 No issues found!",3
,2716,144,36,24676,"Steps to Reproduce


...
...
...

Logs",missing localization data != null,"missing localization data != nullSteps to Reproduce


...
...
...

Logs",3
,2717,148,37,14497,"[   +2 ms] _flutter.setAssetBundlePath: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}
[  +16 ms] Error -32601 received from application: Method not found
[        ] {request: {method: _flutter.setAssetBundlePath, params: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}}}
[   +2 ms] _reloadSources: {pause: false, rootLibUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/lib/main.dart, packagesUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/.pac
kages, isolateId: isolates/646758416}
[+5918 ms] F/libc    (29804): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10c in tid 29826 (ui_thread), pid 29804 (xamples.gallery)
[ +259 ms] *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
[   +2 ms] Build fingerprint: 'google/sailfish/sailfish:8.1.0/OPP5.170921.005/4373449:userdebug/dev-keys'
[        ] Revision: '0'
[        ] ABI: 'arm'
[        ] pid: 29804, tid: 29826, name: ui_thread  >>> io.flutter.examples.gallery <<<
[        ] signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10c
[        ] Cause: null pointer dereference
[        ]     r0 d1058500  r1 c3825819  r2 00000001  r3 80000000
[        ]     r4 00000001  r5 00000000  r6 c47cc478  r7 d1058500
[        ]     r8 00000000  r9 00000000  sl ffffffff  fp d105868c
[        ]     ip d215bb04  sp d12d1400  lr d1844ef9  pc d1844ef8  cpsr 600f0030
[   +5 ms] backtrace:
[   +4 ms]     #00 pc 00474ef8  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #01 pc 0057752b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #02 pc 00413eab  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #03 pc 00416e19  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #04 pc 004140ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #05 pc 004ed44d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #06 pc 004ed36d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #07 pc 004657fb  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #08 pc 00464bef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #09 pc 006a2ee3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #10 pc 00387b51  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #11 pc 003874af  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #12 pc 0046353b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #13 pc 0045cf15  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #14 pc 0059452f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #15 pc 0058d8ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #16 pc 0058de9f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #17 pc 00459dc3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #18 pc 00475ebf  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[   +5 ms]     #19 pc 00475f4f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #20 pc 0068b1ef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #21 pc 00385db5  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #22 pc 0008d3d9  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #23 pc 0008fc75  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #24 pc 000101bd  /system/lib/libutils.so (android::Looper::pollInner(int)+576)
[        ]     #25 pc 0000fee5  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+32)
[        ]     #26 pc 0000c443  /system/lib/libandroid.so (ALooper_pollOnce+50)
[        ]     #27 pc 0008fc2f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #28 pc 0008d469  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #29 pc 0008e829  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #30 pc 00047e6b  /system/lib/libc.so (__pthread_start(void*)+22)
[        ]     #31 pc 0001b1d9  /system/lib/libc.so (__start_thread+32)
[+1282 ms] Service protocol connection closed.
[   +5 ms] Lost connection to device.
[   +4 ms] DevFS: Deleting filesystem on the device (file:///data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/)
[   +3 ms] _deleteDevFS: {fsName: flutter_gallery}
[ +249 ms] TimeoutException after 0:00:00.250000: Future not completed
[  +39 ms] ""flutter run"" took 93,465ms.
[ +231 ms] ensureAnalyticsSent: 225ms
[   +5 ms] exiting with code 0
C:\src\flutter\flutter\examples\flutter_gallery [master  +4 ~3 -0 !]>

This is on be0c488 running on Pixel.",Request to hot reload fails (on Windows),"Request to hot reload fails (on Windows)[   +2 ms] _flutter.setAssetBundlePath: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}
[  +16 ms] Error -32601 received from application: Method not found
[        ] {request: {method: _flutter.setAssetBundlePath, params: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}}}
[   +2 ms] _reloadSources: {pause: false, rootLibUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/lib/main.dart, packagesUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/.pac
kages, isolateId: isolates/646758416}
[+5918 ms] F/libc    (29804): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10c in tid 29826 (ui_thread), pid 29804 (xamples.gallery)
[ +259 ms] *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
[   +2 ms] Build fingerprint: 'google/sailfish/sailfish:8.1.0/OPP5.170921.005/4373449:userdebug/dev-keys'
[        ] Revision: '0'
[        ] ABI: 'arm'
[        ] pid: 29804, tid: 29826, name: ui_thread  >>> io.flutter.examples.gallery <<<
[        ] signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10c
[        ] Cause: null pointer dereference
[        ]     r0 d1058500  r1 c3825819  r2 00000001  r3 80000000
[        ]     r4 00000001  r5 00000000  r6 c47cc478  r7 d1058500
[        ]     r8 00000000  r9 00000000  sl ffffffff  fp d105868c
[        ]     ip d215bb04  sp d12d1400  lr d1844ef9  pc d1844ef8  cpsr 600f0030
[   +5 ms] backtrace:
[   +4 ms]     #00 pc 00474ef8  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #01 pc 0057752b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #02 pc 00413eab  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #03 pc 00416e19  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #04 pc 004140ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #05 pc 004ed44d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #06 pc 004ed36d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #07 pc 004657fb  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #08 pc 00464bef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #09 pc 006a2ee3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #10 pc 00387b51  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #11 pc 003874af  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #12 pc 0046353b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #13 pc 0045cf15  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #14 pc 0059452f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #15 pc 0058d8ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #16 pc 0058de9f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #17 pc 00459dc3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #18 pc 00475ebf  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[   +5 ms]     #19 pc 00475f4f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #20 pc 0068b1ef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #21 pc 00385db5  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #22 pc 0008d3d9  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #23 pc 0008fc75  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #24 pc 000101bd  /system/lib/libutils.so (android::Looper::pollInner(int)+576)
[        ]     #25 pc 0000fee5  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+32)
[        ]     #26 pc 0000c443  /system/lib/libandroid.so (ALooper_pollOnce+50)
[        ]     #27 pc 0008fc2f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #28 pc 0008d469  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #29 pc 0008e829  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so
[        ]     #30 pc 00047e6b  /system/lib/libc.so (__pthread_start(void*)+22)
[        ]     #31 pc 0001b1d9  /system/lib/libc.so (__start_thread+32)
[+1282 ms] Service protocol connection closed.
[   +5 ms] Lost connection to device.
[   +4 ms] DevFS: Deleting filesystem on the device (file:///data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/)
[   +3 ms] _deleteDevFS: {fsName: flutter_gallery}
[ +249 ms] TimeoutException after 0:00:00.250000: Future not completed
[  +39 ms] ""flutter run"" took 93,465ms.
[ +231 ms] ensureAnalyticsSent: 225ms
[   +5 ms] exiting with code 0
C:\src\flutter\flutter\examples\flutter_gallery [master  +4 ~3 -0 !]>

This is on be0c488 running on Pixel.",3
,2718,146,38,5385,"We should probably update http://flutter.github.io/google_sign_in/ manually. It would be super cool to automate the publishing of that site.
Our users are finding these sites and asking questions about them :)",google_sign_in: auto-publish API docs,"google_sign_in: auto-publish API docsWe should probably update http://flutter.github.io/google_sign_in/ manually. It would be super cool to automate the publishing of that site.
Our users are finding these sites and asking questions about them :)",3
,2719,146,39,19963,"The checkbox is only given the minimum possible width, currently 18. it should be at least 48 by 48.",DataTable checkbox tap target is too small,"DataTable checkbox tap target is too smallThe checkbox is only given the minimum possible width, currently 18. it should be at least 48 by 48.",3
,2720,146,40,10159,"On https://flutter.io/android-release/ we should add some documentation that explains what keytool does and especially say where it leaves the key.jks file. It wasn't in the current directory, nor in the app directory. I had to search and finally found it in my home directory (ick!).",Documentation for keytool,"Documentation for keytoolOn https://flutter.io/android-release/ we should add some documentation that explains what keytool does and especially say where it leaves the key.jks file. It wasn't in the current directory, nor in the app directory. I had to search and finally found it in my home directory (ick!).",3
,2721,145,41,2208,"~/flutter/testme2 $ flutter start --help
Run your Flutter app on an attached device.

Usage: flutter run [arguments]

Which is the real one? If start is deprecated in favor of run, maybe we can print out ""start is replaced by run. Please update your scripts, as we will soon remove start.""","remove ""flutter start""","remove ""flutter start""~/flutter/testme2 $ flutter start --help
Run your Flutter app on an attached device.

Usage: flutter run [arguments]

Which is the real one? If start is deprecated in favor of run, maybe we can print out ""start is replaced by run. Please update your scripts, as we will soon remove start.""",3
,2722,145,42,8388,"Hi! I'm trying Flutter on Windows platform and i have installed everything with no problem. I created the app from command line but when i try to launch it, (from command line or even from IntellijIDEA plugin) i get this error.
UnimplementedError: Host OS not supported.",Host OS not supported on Windows,"Host OS not supported on WindowsHi! I'm trying Flutter on Windows platform and i have installed everything with no problem. I created the app from command line but when i try to launch it, (from command line or even from IntellijIDEA plugin) i get this error.
UnimplementedError: Host OS not supported.",3
,2723,144,43,12137,"_googleSignIn.signIn() does not seem to work correctly?
This may be related to

flutter/plugins#94
#10552

Steps to Reproduce
You can reproduce it doing the flutter-firebase codelab (https://codelabs.developers.google.com/codelabs/flutter-firebase/#5), using Android 8 (SDK 26). There seems to be some kind of deadlock when calling this method.
It also does not seem to work using the following versions:
dependencies:
  flutter:
    sdk: flutter
  image_picker: 0.1.1
  google_sign_in: 0.3.1
  firebase_analytics: 0.1.0
  firebase_auth: 0.2.3
  firebase_database: 0.1.0
  firebase_storage: 0.0.6
  firebase_messaging: 0.0.5
We also found out that you can sign in like this:

call _googleSignIn.signIn()
select the user
we noticed a very short flicker at the top of the screen, but that's it
if you use await _googleSignIn.signIn(), the code below is never executed
there is also no exception thrown
(deadlock in async call?)
close the app, so that it stops running in the background
open the app again
sign in using _googleSignIn.signInSilently(): this works now because the account has been selected already",_googleSignIn.signIn() deadlock?,"_googleSignIn.signIn() deadlock?_googleSignIn.signIn() does not seem to work correctly?
This may be related to

flutter/plugins#94
#10552

Steps to Reproduce
You can reproduce it doing the flutter-firebase codelab (https://codelabs.developers.google.com/codelabs/flutter-firebase/#5), using Android 8 (SDK 26). There seems to be some kind of deadlock when calling this method.
It also does not seem to work using the following versions:
dependencies:
  flutter:
    sdk: flutter
  image_picker: 0.1.1
  google_sign_in: 0.3.1
  firebase_analytics: 0.1.0
  firebase_auth: 0.2.3
  firebase_database: 0.1.0
  firebase_storage: 0.0.6
  firebase_messaging: 0.0.5
We also found out that you can sign in like this:

call _googleSignIn.signIn()
select the user
we noticed a very short flicker at the top of the screen, but that's it
if you use await _googleSignIn.signIn(), the code below is never executed
there is also no exception thrown
(deadlock in async call?)
close the app, so that it stops running in the background
open the app again
sign in using _googleSignIn.signInSilently(): this works now because the account has been selected already",3
,2724,142,44,5232,"https://youtu.be/KbyuPMZ-9_A
@abarth says we should fix this, but to do so would require re-writing the Hero system (which has to happen for other reasons).  Just noting down one example of where being able to fade/animate-color would be nice.
[] Flutter (on Linux, channel master)
     Flutter at /src/flutter
     Framework revision 92d0445a8f (43 minutes ago), engine revision 603b0701a6

[] Android toolchain - develop for Android devices (Android SDK 23.0.1)
     Android SDK at /usr/local/google/home/eseidel/Android/Sdk
     Platform android-23, build-tools 23.0.1
     OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)

[] Atom - a lightweight development environment for Flutter
     flutter plugin version 0.2.2
     dartlang plugin version 0.6.22",AppBar Hero Transition could crossfade ,"AppBar Hero Transition could crossfade https://youtu.be/KbyuPMZ-9_A
@abarth says we should fix this, but to do so would require re-writing the Hero system (which has to happen for other reasons).  Just noting down one example of where being able to fade/animate-color would be nice.
[] Flutter (on Linux, channel master)
     Flutter at /src/flutter
     Framework revision 92d0445a8f (43 minutes ago), engine revision 603b0701a6

[] Android toolchain - develop for Android devices (Android SDK 23.0.1)
     Android SDK at /usr/local/google/home/eseidel/Android/Sdk
     Platform android-23, build-tools 23.0.1
     OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)

[] Atom - a lightweight development environment for Flutter
     flutter plugin version 0.2.2
     dartlang plugin version 0.6.22",3
,2725,142,45,16474,"New Feature - TabController disable Slide Transition
Ability to Disable or change the Horizontal Slide Transition when changing Tabs. I would like to switch pages by Fading instead of sliding left or right.
Thank you.",New Feature Request - TabController disable Slide Transition,"New Feature Request - TabController disable Slide TransitionNew Feature - TabController disable Slide Transition
Ability to Disable or change the Horizontal Slide Transition when changing Tabs. I would like to switch pages by Fading instead of sliding left or right.
Thank you.",3
,2726,148,46,32556,"Hello ! Please, I'm trying to create a new project flutter_web but I have an error in pub get :
Resolving dependencies...
Git error. Command: git fetch
fatal: not a git repository (or any of the parent directories): .git

I reinstalled git but the problem persists , I also launched the pub cache repair but to no avail
Flutter doctor output :
[] Flutter (Channel stable, v1.5.4-hotfix.2, on Microsoft Windows [version 10.0.10240], locale fr-FR)
     Flutter version 1.5.4-hotfix.2 at D:\Dev\FLUTTER\flutter
     Framework revision 7a4c33425d (12 days ago), 2019-04-29 11:05:24 -0700
     Engine revision 52c7a1e849
     Dart version 2.3.0 (build 2.3.0-dev.0.5 a1668566e5)

[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at D:\Dev\SDK1\SDK1
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: C:\Program Files\Android\Android Studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
    X Android license status unknown.
      Try re-installing or updating your Android SDK Manager.
      See https://developer.android.com/studio/#downloads or visit https://flutter.dev/setup/#android-setup for detailed instructions.

[] Android Studio (version 3.3)
     Android Studio at C:\Program Files\Android\Android Studio
     Flutter plugin version 34.0.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[!] Connected device
    ! No devices available",Flutter_web initialization error,"Flutter_web initialization errorHello ! Please, I'm trying to create a new project flutter_web but I have an error in pub get :
Resolving dependencies...
Git error. Command: git fetch
fatal: not a git repository (or any of the parent directories): .git

I reinstalled git but the problem persists , I also launched the pub cache repair but to no avail
Flutter doctor output :
[] Flutter (Channel stable, v1.5.4-hotfix.2, on Microsoft Windows [version 10.0.10240], locale fr-FR)
     Flutter version 1.5.4-hotfix.2 at D:\Dev\FLUTTER\flutter
     Framework revision 7a4c33425d (12 days ago), 2019-04-29 11:05:24 -0700
     Engine revision 52c7a1e849
     Dart version 2.3.0 (build 2.3.0-dev.0.5 a1668566e5)

[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at D:\Dev\SDK1\SDK1
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: C:\Program Files\Android\Android Studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
    X Android license status unknown.
      Try re-installing or updating your Android SDK Manager.
      See https://developer.android.com/studio/#downloads or visit https://flutter.dev/setup/#android-setup for detailed instructions.

[] Android Studio (version 3.3)
     Android Studio at C:\Program Files\Android\Android Studio
     Flutter plugin version 34.0.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[!] Connected device
    ! No devices available",3
,2727,148,47,10731,"Steps to Reproduce
On step six under Place the message list, there is a chopped up link on, ""Naming the argument...""",Building Beautiful UIs Tutorial - Silly Link Display,"Building Beautiful UIs Tutorial - Silly Link DisplaySteps to Reproduce
On step six under Place the message list, there is a chopped up link on, ""Naming the argument...""",3
,2728,144,48,11183,"I wanted to retain keyboard focus in a TextField even after the text is submitted, so that I can type in multiple messages in a row in a chat app. The default behavior is to unfocus when the text is submitted, so I added a FocusScope.of(context).requestFocus() call in my handleSubmit callback.
With this change, after submitting the text, the text field goes into a weird state where it has focus (i.e. I can type text) but the cursor isn't blinking.
Below is the minimal example code that reproduces this issue.
import 'package:flutter/material.dart';

void main() {
  runApp(new MyApp());
}

class MyApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return new MaterialApp(
      home: new MyHomePage(),
    );
  }
}

class MyHomePage extends StatefulWidget {
  MyHomePage({Key key}) : super(key: key);

  @override
  _MyHomePageState createState() => new _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  final TextEditingController _controller = new TextEditingController();
  final FocusNode _focusNode = new FocusNode();

  @override
  Widget build(BuildContext context) {
    return new Scaffold(
      body: new Center(
        child: new Container(
          width: 300.0,
          child: new TextField(
            controller: _controller,
            focusNode: _focusNode,
            onSubmitted: (String text) {
              print(text);
              _controller.clear();
              FocusScope.of(context).requestFocus(_focusNode);
            },
          ),
        ),
      ),
    );
  }
}",Text cursor doesn't blink when the focus is requested programmatically within onSubmitted callback,"Text cursor doesn't blink when the focus is requested programmatically within onSubmitted callbackI wanted to retain keyboard focus in a TextField even after the text is submitted, so that I can type in multiple messages in a row in a chat app. The default behavior is to unfocus when the text is submitted, so I added a FocusScope.of(context).requestFocus() call in my handleSubmit callback.
With this change, after submitting the text, the text field goes into a weird state where it has focus (i.e. I can type text) but the cursor isn't blinking.
Below is the minimal example code that reproduces this issue.
import 'package:flutter/material.dart';

void main() {
  runApp(new MyApp());
}

class MyApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return new MaterialApp(
      home: new MyHomePage(),
    );
  }
}

class MyHomePage extends StatefulWidget {
  MyHomePage({Key key}) : super(key: key);

  @override
  _MyHomePageState createState() => new _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  final TextEditingController _controller = new TextEditingController();
  final FocusNode _focusNode = new FocusNode();

  @override
  Widget build(BuildContext context) {
    return new Scaffold(
      body: new Center(
        child: new Container(
          width: 300.0,
          child: new TextField(
            controller: _controller,
            focusNode: _focusNode,
            onSubmitted: (String text) {
              print(text);
              _controller.clear();
              FocusScope.of(context).requestFocus(_focusNode);
            },
          ),
        ),
      ),
    );
  }
}",3
,2729,148,49,19582,"Switching between the different app bar display options in the ""Contact profile"" study can cause the Flutter Gallery app to crash. I tested this on a OnePlus3T using the production version on the PlayStore and locally using the version in v0.5.1.
@HansMuller I think this is behind the error in #17598
Steps to Reproduce


Open the ""Contact profile"" study


Scroll the bottom of the page


Select the drop down menu in top right of the app bar and choose the ""App bar snaps"" option


Scroll up until the full app bar is displayed (screenshot attached)



Select the ""App bar floats"" option


The app will not immediately crash but it is in a bad state and further actions will cause various exceptions and crashes. Logs attached below.
Logs
Error when scrolling:
[+34049 ms] I/flutter (29797):  EXCEPTION CAUGHT BY FOUNDATION LIBRARY 
[  +19 ms] I/flutter (29797): The following NoSuchMethodError was thrown while dispatching notifications for ValueNotifier<bool>:
[        ] I/flutter (29797): The method 'stop' was called on null.
[        ] I/flutter (29797): Receiver: null
[   +1 ms] I/flutter (29797): Tried calling: stop(canceled: true)
[        ] I/flutter (29797):
[        ] I/flutter (29797): When the exception was thrown, this was the stack:
[  +12 ms] I/flutter (29797): #0      Object.noSuchMethod (dart:core/runtime/libobject_patch.dart:46:5)
[        ] I/flutter (29797): #1      AnimationController.stop (package:flutter/src/animation/animation_controller.dart:499:13)
[        ] I/flutter (29797): #2      RenderSliverFloatingPersistentHeader.maybeStopSnapAnimation (package:flutter/src/rendering/sliver_persistent_header.dart:457:18)
[        ] I/flutter (29797): #3      _FloatingAppBarState._isScrollingListener (package:flutter/src/material/app_bar.dart:547:15)
[        ] I/flutter (29797): #4      ChangeNotifier.notifyListeners (package:flutter/src/foundation/change_notifier.dart:161:21)
[        ] I/flutter (29797): #5      ValueNotifier.value= (package:flutter/src/foundation/change_notifier.dart:217:5)
[        ] I/flutter (29797): #6      ScrollPosition.beginActivity (package:flutter/src/widgets/scroll_position.dart:613:25)
[        ] I/flutter (29797): #7      ScrollPositionWithSingleContext.beginActivity (package:flutter/src/widgets/scroll_position_with_single_context.dart:117:11)
[        ] I/flutter (29797): #8      ScrollPositionWithSingleContext.drag (package:flutter/src/widgets/scroll_position_with_single_context.dart:245:5)
[        ] I/flutter (29797): #9      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:443:22)
[   +1 ms] I/flutter (29797): #10     DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:47)
[        ] I/flutter (29797): #11     GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)
[        ] I/flutter (29797): #12     DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)
[        ] I/flutter (29797): #13     GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)
[        ] I/flutter (29797): #14     GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)
[        ] I/flutter (29797): (elided 2 frames from package dart:async)
[        ] I/flutter (29797):
[        ] I/flutter (29797): The ValueNotifier<bool> sending notification was:
[        ] I/flutter (29797):   ValueNotifier<bool>#0a1d4(true)
[        ] I/flutter (29797): 

Error when trying to switch to the 'App bar snaps' option:
[+40490 ms] I/flutter (29797): Another exception was thrown: AnimationController.dispose() called more than once.
[   +8 ms] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.
[   +9 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 1 line
[        ] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.
[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.
[+2095 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 20 lines
[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.

Error when clicking the back button
[+242379 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 4062 pos 14: '() {
[ +279 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 2240 pos 16: '!_dirtyElements[index]._active || _dirtyElements[index]._debugIsInScope(context)': is not true.

Error when trying to re-open the Contact profile study after the last crash
[+457845 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3681 pos 12: 'child == _child': is not true.
[   +6 ms] I/flutter (29797): Another exception was thrown: Duplicate GlobalKey detected in widget tree.","Flutter Gallery app crashes when switching between app bar options in the ""Contact profile"" study","Flutter Gallery app crashes when switching between app bar options in the ""Contact profile"" studySwitching between the different app bar display options in the ""Contact profile"" study can cause the Flutter Gallery app to crash. I tested this on a OnePlus3T using the production version on the PlayStore and locally using the version in v0.5.1.
@HansMuller I think this is behind the error in #17598
Steps to Reproduce


Open the ""Contact profile"" study


Scroll the bottom of the page


Select the drop down menu in top right of the app bar and choose the ""App bar snaps"" option


Scroll up until the full app bar is displayed (screenshot attached)



Select the ""App bar floats"" option


The app will not immediately crash but it is in a bad state and further actions will cause various exceptions and crashes. Logs attached below.
Logs
Error when scrolling:
[+34049 ms] I/flutter (29797):  EXCEPTION CAUGHT BY FOUNDATION LIBRARY 
[  +19 ms] I/flutter (29797): The following NoSuchMethodError was thrown while dispatching notifications for ValueNotifier<bool>:
[        ] I/flutter (29797): The method 'stop' was called on null.
[        ] I/flutter (29797): Receiver: null
[   +1 ms] I/flutter (29797): Tried calling: stop(canceled: true)
[        ] I/flutter (29797):
[        ] I/flutter (29797): When the exception was thrown, this was the stack:
[  +12 ms] I/flutter (29797): #0      Object.noSuchMethod (dart:core/runtime/libobject_patch.dart:46:5)
[        ] I/flutter (29797): #1      AnimationController.stop (package:flutter/src/animation/animation_controller.dart:499:13)
[        ] I/flutter (29797): #2      RenderSliverFloatingPersistentHeader.maybeStopSnapAnimation (package:flutter/src/rendering/sliver_persistent_header.dart:457:18)
[        ] I/flutter (29797): #3      _FloatingAppBarState._isScrollingListener (package:flutter/src/material/app_bar.dart:547:15)
[        ] I/flutter (29797): #4      ChangeNotifier.notifyListeners (package:flutter/src/foundation/change_notifier.dart:161:21)
[        ] I/flutter (29797): #5      ValueNotifier.value= (package:flutter/src/foundation/change_notifier.dart:217:5)
[        ] I/flutter (29797): #6      ScrollPosition.beginActivity (package:flutter/src/widgets/scroll_position.dart:613:25)
[        ] I/flutter (29797): #7      ScrollPositionWithSingleContext.beginActivity (package:flutter/src/widgets/scroll_position_with_single_context.dart:117:11)
[        ] I/flutter (29797): #8      ScrollPositionWithSingleContext.drag (package:flutter/src/widgets/scroll_position_with_single_context.dart:245:5)
[        ] I/flutter (29797): #9      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:443:22)
[   +1 ms] I/flutter (29797): #10     DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:47)
[        ] I/flutter (29797): #11     GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)
[        ] I/flutter (29797): #12     DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)
[        ] I/flutter (29797): #13     GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)
[        ] I/flutter (29797): #14     GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)
[        ] I/flutter (29797): (elided 2 frames from package dart:async)
[        ] I/flutter (29797):
[        ] I/flutter (29797): The ValueNotifier<bool> sending notification was:
[        ] I/flutter (29797):   ValueNotifier<bool>#0a1d4(true)
[        ] I/flutter (29797): 

Error when trying to switch to the 'App bar snaps' option:
[+40490 ms] I/flutter (29797): Another exception was thrown: AnimationController.dispose() called more than once.
[   +8 ms] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.
[   +9 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 1 line
[        ] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.
[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.
[+2095 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 20 lines
[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.

Error when clicking the back button
[+242379 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 4062 pos 14: '() {
[ +279 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 2240 pos 16: '!_dirtyElements[index]._active || _dirtyElements[index]._debugIsInScope(context)': is not true.

Error when trying to re-open the Contact profile study after the last crash
[+457845 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3681 pos 12: 'child == _child': is not true.
[   +6 ms] I/flutter (29797): Another exception was thrown: Duplicate GlobalKey detected in widget tree.",3
,2730,148,50,20249,"I want to Add Flutter to existing apps , but command ""flutter create t module xxx"" is not work,
exec command ""flutter create t module xxx"" , and output ""Multiple output directories specified.""
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel master, v0.5.8-pre.241, on Mac OS X 10.12.6 16G1510, locale zh-Hans-CN)
[] Android toolchain - develop for Android devices (Android SDK 27.0.3)
[!] iOS toolchain - develop for iOS devices
 Xcode installation is incomplete; a full installation is necessary for iOS development.
Download at: https://developer.apple.com/xcode/download/
Or install Xcode via the App Store.
Once installed, run:
sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer
 libimobiledevice and ideviceinstaller are not installed. To install, run:
brew install --HEAD libimobiledevice
brew install ideviceinstaller
 ios-deploy not installed. To install:
brew install ios-deploy
 CocoaPods not installed.
CocoaPods is used to retrieve the iOS platform side's plugin code that responds to your plugin usage on the Dart side.
Without resolving iOS dependencies with CocoaPods, plugins will not work on iOS.
For more info, see https://flutter.io/platform-plugins
To install:
brew install cocoapods
pod setup
[] Android Studio (version 3.1)
[!] IntelliJ IDEA Community Edition (version 2018.2)
 Flutter plugin not installed; this adds Flutter specific functionality.
 Dart plugin not installed; this adds Dart specific functionality.
[] Connected devices (1 available)
! Doctor found issues in 2 categories.",Command flutter create t module xxx is not work,"Command flutter create t module xxx is not workI want to Add Flutter to existing apps , but command ""flutter create t module xxx"" is not work,
exec command ""flutter create t module xxx"" , and output ""Multiple output directories specified.""
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel master, v0.5.8-pre.241, on Mac OS X 10.12.6 16G1510, locale zh-Hans-CN)
[] Android toolchain - develop for Android devices (Android SDK 27.0.3)
[!] iOS toolchain - develop for iOS devices
 Xcode installation is incomplete; a full installation is necessary for iOS development.
Download at: https://developer.apple.com/xcode/download/
Or install Xcode via the App Store.
Once installed, run:
sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer
 libimobiledevice and ideviceinstaller are not installed. To install, run:
brew install --HEAD libimobiledevice
brew install ideviceinstaller
 ios-deploy not installed. To install:
brew install ios-deploy
 CocoaPods not installed.
CocoaPods is used to retrieve the iOS platform side's plugin code that responds to your plugin usage on the Dart side.
Without resolving iOS dependencies with CocoaPods, plugins will not work on iOS.
For more info, see https://flutter.io/platform-plugins
To install:
brew install cocoapods
pod setup
[] Android Studio (version 3.1)
[!] IntelliJ IDEA Community Edition (version 2018.2)
 Flutter plugin not installed; this adds Flutter specific functionality.
 Dart plugin not installed; this adds Dart specific functionality.
[] Connected devices (1 available)
! Doctor found issues in 2 categories.",3
,2731,148,51,18099,"I have a NestedScrollView with a SliverAppBar with an expandedHeight and a SliverFixedExtentList inside. Basically this example.
Now I want to jumpTo position 5000.0 in the list.
So I add a ScrollController to the NestedScrollView. But the range that I can jumpTo seems to be limited to the height of the header.
I am not able to programatically scroll the SliverFixedExtentList to any position.
I believe this is because the NestedScrollView doesn't allow access to the innerController, but that is just a guess.
Is there a way to achieve this any other way?
Why, because I try to have a TabBar and each tab scrolls the ListView underneath to a specific section - basically like in the UeberEats app's menu page.",NestedScrollView cannot scroll inner list view,"NestedScrollView cannot scroll inner list viewI have a NestedScrollView with a SliverAppBar with an expandedHeight and a SliverFixedExtentList inside. Basically this example.
Now I want to jumpTo position 5000.0 in the list.
So I add a ScrollController to the NestedScrollView. But the range that I can jumpTo seems to be limited to the height of the header.
I am not able to programatically scroll the SliverFixedExtentList to any position.
I believe this is because the NestedScrollView doesn't allow access to the innerController, but that is just a guess.
Is there a way to achieve this any other way?
Why, because I try to have a TabBar and each tab scrolls the ListView underneath to a specific section - basically like in the UeberEats app's menu page.",3
,2732,148,52,2130,"(reported by user)
on https://flutter.io/setup/
broken link
https://flutter.io/setup/https//developer.android.com/sdk/index.html
correct link
https://developer.android.com/sdk/index.html",broken link to android studio,"broken link to android studio(reported by user)
on https://flutter.io/setup/
broken link
https://flutter.io/setup/https//developer.android.com/sdk/index.html
correct link
https://developer.android.com/sdk/index.html",3
,2733,144,53,2326,"The assertion '_placeholderSize == null' is not true. actually means that liftToOverlay was called twice. The text in the assertion should include this helpful hint.
Steps to Reproduce
Use a Mimicable
Call liftToOverlay a second time before the first has finished.
Flutter Version
Flutter from https://github.com/flutter/flutter.git (on master)
Framework: 9ce6bff (2 days ago)
Engine:    006a702
Logs
03-02 10:20:56.726  8213  8230 I flutter : -- EXCEPTION CAUGHT BY GESTURE LIBRARY ---------------------------------
03-02 10:20:56.728  8213  8230 I flutter : The following exception was raised while routing a pointer event:
03-02 10:20:56.729  8213  8230 I flutter : 'packages/flutter/src/widgets/mimic.dart': Failed assertion: line 176: '_placeholderSize == null' is not true.
03-02 10:20:56.729  8213  8230 I flutter : Event:
03-02 10:20:56.732  8213  8230 I flutter : PointerUpEvent(Point(115.5, 102.2))
03-02 10:20:56.732  8213  8230 I flutter : Stack trace:
03-02 10:20:56.735  8213  8230 I flutter : #0      _AssertionError._throwNew (dart:core-patch/errors_patch.dart:27)
03-02 10:20:56.735  8213  8230 I flutter : #1      MimicableState.startMimic (packages/flutter/src/widgets/mimic.dart:176)
03-02 10:20:56.735  8213  8230 I flutter : #2      MimicableState.liftToOverlay (packages/flutter/src/widgets/mimic.dart:198)
03-02 10:20:56.735  8213  8230 I flutter : #3      RouteBarState._onTap (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:146)
03-02 10:20:56.735  8213  8230 I flutter : #4      RouteBarState.build.<anonymous closure>.<anonymous closure> (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:167)
03-02 10:20:56.736  8213  8230 I flutter : #5      _InkResponseState._handleTap (packages/flutter/src/material/ink_well.dart:101)
03-02 10:20:56.736  8213  8230 I flutter : #6      TapGestureRecognizer._checkUp (packages/flutter/src/gestures/tap.dart:83)
03-02 10:20:56.736  8213  8230 I flutter : #7      TapGestureRecognizer.handlePrimaryPointer (packages/flutter/src/gestures/tap.dart:33)
03-02 10:20:56.737  8213  8230 I flutter : #8      PrimaryPointerGestureRecognizer.handleEvent (packages/flutter/src/gestures/recognizer.dart:138)
03-02 10:20:56.737  8213  8230 I flutter : #9      PointerRouter.route (packages/flutter/src/gestures/pointer_router.dart:65)
03-02 10:20:56.737  8213  8230 I flutter : #10     BindingBase&Scheduler&Gesturer.handleEvent (packages/flutter/src/gestures/binding.dart:117)
03-02 10:20:56.737  8213  8230 I flutter : #11     BindingBase&Scheduler&Gesturer.dispatchEvent (packages/flutter/src/gestures/binding.dart:96)
03-02 10:20:56.737  8213  8230 I flutter : #12     BindingBase&Scheduler&Gesturer._handlePointerEvent (packages/flutter/src/gestures/binding.dart:71)
03-02 10:20:56.737  8213  8230 I flutter : #13     BindingBase&Scheduler&Gesturer._handlePointerPacket (packages/flutter/src/gestures/binding.dart:43)
03-02 10:20:56.737  8213  8230 I flutter : #14     _dispatchPointerPacket (file:///b/build/slave/Linux_Engine/build/src/out/android_Release/gen/sky/bindings/hooks.dart:43)
03-02 10:20:56.737  8213  8230 I flutter : ------------------------------------------------------------------------",Non-actionable assert: '_placeholderSize == null' is not true.,"Non-actionable assert: '_placeholderSize == null' is not true.The assertion '_placeholderSize == null' is not true. actually means that liftToOverlay was called twice. The text in the assertion should include this helpful hint.
Steps to Reproduce
Use a Mimicable
Call liftToOverlay a second time before the first has finished.
Flutter Version
Flutter from https://github.com/flutter/flutter.git (on master)
Framework: 9ce6bff (2 days ago)
Engine:    006a702
Logs
03-02 10:20:56.726  8213  8230 I flutter : -- EXCEPTION CAUGHT BY GESTURE LIBRARY ---------------------------------
03-02 10:20:56.728  8213  8230 I flutter : The following exception was raised while routing a pointer event:
03-02 10:20:56.729  8213  8230 I flutter : 'packages/flutter/src/widgets/mimic.dart': Failed assertion: line 176: '_placeholderSize == null' is not true.
03-02 10:20:56.729  8213  8230 I flutter : Event:
03-02 10:20:56.732  8213  8230 I flutter : PointerUpEvent(Point(115.5, 102.2))
03-02 10:20:56.732  8213  8230 I flutter : Stack trace:
03-02 10:20:56.735  8213  8230 I flutter : #0      _AssertionError._throwNew (dart:core-patch/errors_patch.dart:27)
03-02 10:20:56.735  8213  8230 I flutter : #1      MimicableState.startMimic (packages/flutter/src/widgets/mimic.dart:176)
03-02 10:20:56.735  8213  8230 I flutter : #2      MimicableState.liftToOverlay (packages/flutter/src/widgets/mimic.dart:198)
03-02 10:20:56.735  8213  8230 I flutter : #3      RouteBarState._onTap (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:146)
03-02 10:20:56.735  8213  8230 I flutter : #4      RouteBarState.build.<anonymous closure>.<anonymous closure> (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:167)
03-02 10:20:56.736  8213  8230 I flutter : #5      _InkResponseState._handleTap (packages/flutter/src/material/ink_well.dart:101)
03-02 10:20:56.736  8213  8230 I flutter : #6      TapGestureRecognizer._checkUp (packages/flutter/src/gestures/tap.dart:83)
03-02 10:20:56.736  8213  8230 I flutter : #7      TapGestureRecognizer.handlePrimaryPointer (packages/flutter/src/gestures/tap.dart:33)
03-02 10:20:56.737  8213  8230 I flutter : #8      PrimaryPointerGestureRecognizer.handleEvent (packages/flutter/src/gestures/recognizer.dart:138)
03-02 10:20:56.737  8213  8230 I flutter : #9      PointerRouter.route (packages/flutter/src/gestures/pointer_router.dart:65)
03-02 10:20:56.737  8213  8230 I flutter : #10     BindingBase&Scheduler&Gesturer.handleEvent (packages/flutter/src/gestures/binding.dart:117)
03-02 10:20:56.737  8213  8230 I flutter : #11     BindingBase&Scheduler&Gesturer.dispatchEvent (packages/flutter/src/gestures/binding.dart:96)
03-02 10:20:56.737  8213  8230 I flutter : #12     BindingBase&Scheduler&Gesturer._handlePointerEvent (packages/flutter/src/gestures/binding.dart:71)
03-02 10:20:56.737  8213  8230 I flutter : #13     BindingBase&Scheduler&Gesturer._handlePointerPacket (packages/flutter/src/gestures/binding.dart:43)
03-02 10:20:56.737  8213  8230 I flutter : #14     _dispatchPointerPacket (file:///b/build/slave/Linux_Engine/build/src/out/android_Release/gen/sky/bindings/hooks.dart:43)
03-02 10:20:56.737  8213  8230 I flutter : ------------------------------------------------------------------------",3
,2734,148,54,13510,"Steps to Reproduce
await FirebaseDatabase.instance.reference().child('foo/bar').once();
As far as I know this should return immediately in any case, not only when a value is found,
but in Flutter it only returns after the first event which may never come.
This makes it impossible to check if a location exists.
firebase_database 0.1.2
Logs
Flutter Doctor
] Flutter (on Mac OS X 10.13.1 17B1003, locale en-AT, channel alpha)
     Flutter at /Users/zoechi/flutter/flutter
     Framework revision fd7853faad (7 days ago), 2017-12-05 16:12:55 -0800
     Engine revision b57fca02b5
     Tools Dart version 1.25.0-dev.11.0
     Engine Dart version 2.0.0-edge.d4cfecb1065d322d3670df7e9ec9a0cc2d4b90f0

[] Android toolchain - develop for Android devices (Android SDK 27.0.1)
     Android SDK at /usr/local/opt/android-sdk
     Android NDK at /usr/local/opt/android-sdk/ndk-bundle
     Platform android-27, build-tools 27.0.1
     ANDROID_HOME = /usr/local/opt/android-sdk
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] iOS toolchain - develop for iOS devices (Xcode 9.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.2, Build version 9C40b
     ios-deploy 1.9.2
     CocoaPods version 1.3.1

[] Android Studio (version 3.0)
     Android Studio at /Applications/Android Studio.app/Contents
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] IntelliJ IDEA Ultimate Edition (version 2017.3)
     Flutter plugin version 20.0.3
     Dart plugin version 173.3727.108

[] Connected devices
     Pixel XL                   HT69V0203649   android-arm  Android 8.1.0 (API 27)
     Android SDK built for x86  emulator-5554  android-x86  Android 8.0.0 (API 26) (emulator)



For more information about diagnosing and reporting Flutter bugs, please see https://flutter.io/bug-reports/.

See also https://stackoverflow.com/questions/37910008/check-if-value-exists-in-firebase-db",Querying for a non-existent location waits forever,"Querying for a non-existent location waits foreverSteps to Reproduce
await FirebaseDatabase.instance.reference().child('foo/bar').once();
As far as I know this should return immediately in any case, not only when a value is found,
but in Flutter it only returns after the first event which may never come.
This makes it impossible to check if a location exists.
firebase_database 0.1.2
Logs
Flutter Doctor
] Flutter (on Mac OS X 10.13.1 17B1003, locale en-AT, channel alpha)
     Flutter at /Users/zoechi/flutter/flutter
     Framework revision fd7853faad (7 days ago), 2017-12-05 16:12:55 -0800
     Engine revision b57fca02b5
     Tools Dart version 1.25.0-dev.11.0
     Engine Dart version 2.0.0-edge.d4cfecb1065d322d3670df7e9ec9a0cc2d4b90f0

[] Android toolchain - develop for Android devices (Android SDK 27.0.1)
     Android SDK at /usr/local/opt/android-sdk
     Android NDK at /usr/local/opt/android-sdk/ndk-bundle
     Platform android-27, build-tools 27.0.1
     ANDROID_HOME = /usr/local/opt/android-sdk
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] iOS toolchain - develop for iOS devices (Xcode 9.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.2, Build version 9C40b
     ios-deploy 1.9.2
     CocoaPods version 1.3.1

[] Android Studio (version 3.0)
     Android Studio at /Applications/Android Studio.app/Contents
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] IntelliJ IDEA Ultimate Edition (version 2017.3)
     Flutter plugin version 20.0.3
     Dart plugin version 173.3727.108

[] Connected devices
     Pixel XL                   HT69V0203649   android-arm  Android 8.1.0 (API 27)
     Android SDK built for x86  emulator-5554  android-x86  Android 8.0.0 (API 26) (emulator)



For more information about diagnosing and reporting Flutter bugs, please see https://flutter.io/bug-reports/.

See also https://stackoverflow.com/questions/37910008/check-if-value-exists-in-firebase-db",3
,2735,148,55,19956,"From URL: https://flutter.io/tutorials/layout/
In the second figure, the caption to the right that reads ""Column"" should read ""Rows""",Issue from website page Building Layouts in Flutter,"Issue from website page Building Layouts in FlutterFrom URL: https://flutter.io/tutorials/layout/
In the second figure, the caption to the right that reads ""Column"" should read ""Rows""",3
,2736,142,56,17360,"Sometimes when writing a test you change your mind about what file names to use, but you do so after having run it with the old names, and then check in the unused files. We should be able to catch that case since when generating goldens we know every test we run and know every file it generated.
This might require that we put files from tests in a directory specific to the test.
cc @tvolkert","When generating golden files, catch the case of orphan files","When generating golden files, catch the case of orphan filesSometimes when writing a test you change your mind about what file names to use, but you do so after having run it with the old names, and then check in the unused files. We should be able to catch that case since when generating goldens we know every test we run and know every file it generated.
This might require that we put files from tests in a directory specific to the test.
cc @tvolkert",3
,2737,148,57,25875,"When i use the  drawRect, The out byte can display on the image widget,
But  when  i use the  drawImageRect, I found the out byte  always be the same code , and it can't display on the image widget.   But I can draw it on the  CustomPainter
If  I change the image2 to the source image, It will be ok.
How can i do  to  get the cut image
               child: Text(""image""),
               onPressed: (){
               var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());
               var dest = Rect.fromLTWH(0,0,200,200);
               PictureRecorder recorder =  PictureRecorder();
               Canvas canvas2 = Canvas(recorder);
               Paint paint2 = new Paint();
               paint2.color =Colors.red;
               canvas2.drawRect(Rect.fromLTWH(0, 0, 150, 150), paint2);
               canvas2.drawImageRect(image, source, dest, paint2);
               var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());
               image2.toByteData(format:ui.ImageByteFormat.png).then((byte){
                 imageDes=   Uint8List.view(byte.buffer);
               var mByte=byte.buffer.asUint8List();
               print(""aaa ${mByte}"");
               getApplicationDocumentsDirectory().then((dir){
               String path = dir.path +""/test.png"";
               new File(path).writeAsBytesSync(mByte);
               print(path);
               });
               setState(() {
                 imageDes =mByte;
               });
               });
              print(imageDes);
           Navigator.push(context,new MaterialPageRoute(builder: (context) =>  ImageTestPage(title: ""image"",image: byte.buffer.asUint8List())));
        },
      );
           }),` ` 
` 


`class SignaturePainter extends CustomPainter {
 SignaturePainter(this.points,this.isStart,this.width,this.height,this.cWidth,this.cHeight,this.image);
 bool  isStart = false;
 double startX=0;
 double startY=0;
 double cWidth =200;
 double cHeight =200;
 double width;
 double height;
 flutterui.Image image;

 final List<Offset> points; 

 void paint(Canvas canvas, Size size) {
   Paint paint = new Paint()
     ..color = Colors.blue[200]
     ..strokeCap = StrokeCap.round
     ..isAntiAlias = true
     ..strokeWidth = 2.0

     ..strokeJoin = StrokeJoin.bevel;
//    print(""change ${this.points}"");
   if(image!=null){

     double dwidth =0;
     double dheight =0;
     if(image.width.toDouble()/width>image.height.toDouble()/height){
       dwidth = width;
       dheight = image.height.toDouble()*dwidth/image.width.toDouble();
     }
     else{
       dheight = height;
       dwidth = image.width.toDouble() * dheight/image.height.toDouble();
     }
     canvas.drawImageRect(image, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH((width-dwidth)/2,
         (height-dheight)/2, dwidth,dheight), paint);

//      var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());
//      var dest = Rect.fromLTWH(0,0,400,400);
//      PictureRecorder recorder =  PictureRecorder();
//      Canvas canvas2 = Canvas(recorder);
//      Paint paint2 = new Paint();
//      canvas2.drawImageRect(image, source, dest, paint2);
//      var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());
////    canvas.drawImageRect(image2, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH(0,0,400,400), paint);
//      image2.toByteData(format:ui.ImageByteFormat.png).then((byte){
//        var mByte=byte.buffer.asUint8List();
//        print(""aaa ${mByte}"");
//        getApplicationDocumentsDirectory().then((dir){
//          String path = dir.path +""/test.png"";
//          new File(path).writeAsBytesSync(mByte);
//          print(path);
//        });
//      });
   }

   if(this.points.length>1){
     double startX = points[1].dx - points[0].dx+points[2].dx;
     double startY = points[1].dy - points[0].dy+points[2].dy;
     if(startX<0)
       startX = 0;
     else if(startX+cWidth>width){
       startX = width-cWidth;
     }
     if(startY<0)
       startY=0;
     else if(startY + cHeight>height){
       startY = height-cHeight;
     }
//      canvas.drawRect(Rect.fromLTRB(startX,startY,startX+200,startY+200), paint);
     List<Offset> points2 =[
       Offset(startX,startY),
       Offset(startX+cWidth,startY),
       Offset(startX+cWidth,startY+cHeight),
       Offset(startX,startY+cHeight),
       Offset(startX,startY),
     ];
     canvas.drawPoints(PointMode.polygon, points2, paint);
   }
   else{
     List<Offset> points2 =[
       Offset(startX,startY),
       Offset(startX+cWidth,startY),
       Offset(startX+cWidth,startY+cHeight),
       Offset(startX,startY+cHeight),
       Offset(startX,startY),
     ];
     canvas.drawPoints(PointMode.polygon, points2, paint);
   }

 }`",canvas.drawImageRect can't output the image,"canvas.drawImageRect can't output the imageWhen i use the  drawRect, The out byte can display on the image widget,
But  when  i use the  drawImageRect, I found the out byte  always be the same code , and it can't display on the image widget.   But I can draw it on the  CustomPainter
If  I change the image2 to the source image, It will be ok.
How can i do  to  get the cut image
               child: Text(""image""),
               onPressed: (){
               var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());
               var dest = Rect.fromLTWH(0,0,200,200);
               PictureRecorder recorder =  PictureRecorder();
               Canvas canvas2 = Canvas(recorder);
               Paint paint2 = new Paint();
               paint2.color =Colors.red;
               canvas2.drawRect(Rect.fromLTWH(0, 0, 150, 150), paint2);
               canvas2.drawImageRect(image, source, dest, paint2);
               var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());
               image2.toByteData(format:ui.ImageByteFormat.png).then((byte){
                 imageDes=   Uint8List.view(byte.buffer);
               var mByte=byte.buffer.asUint8List();
               print(""aaa ${mByte}"");
               getApplicationDocumentsDirectory().then((dir){
               String path = dir.path +""/test.png"";
               new File(path).writeAsBytesSync(mByte);
               print(path);
               });
               setState(() {
                 imageDes =mByte;
               });
               });
              print(imageDes);
           Navigator.push(context,new MaterialPageRoute(builder: (context) =>  ImageTestPage(title: ""image"",image: byte.buffer.asUint8List())));
        },
      );
           }),` ` 
` 


`class SignaturePainter extends CustomPainter {
 SignaturePainter(this.points,this.isStart,this.width,this.height,this.cWidth,this.cHeight,this.image);
 bool  isStart = false;
 double startX=0;
 double startY=0;
 double cWidth =200;
 double cHeight =200;
 double width;
 double height;
 flutterui.Image image;

 final List<Offset> points; 

 void paint(Canvas canvas, Size size) {
   Paint paint = new Paint()
     ..color = Colors.blue[200]
     ..strokeCap = StrokeCap.round
     ..isAntiAlias = true
     ..strokeWidth = 2.0

     ..strokeJoin = StrokeJoin.bevel;
//    print(""change ${this.points}"");
   if(image!=null){

     double dwidth =0;
     double dheight =0;
     if(image.width.toDouble()/width>image.height.toDouble()/height){
       dwidth = width;
       dheight = image.height.toDouble()*dwidth/image.width.toDouble();
     }
     else{
       dheight = height;
       dwidth = image.width.toDouble() * dheight/image.height.toDouble();
     }
     canvas.drawImageRect(image, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH((width-dwidth)/2,
         (height-dheight)/2, dwidth,dheight), paint);

//      var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());
//      var dest = Rect.fromLTWH(0,0,400,400);
//      PictureRecorder recorder =  PictureRecorder();
//      Canvas canvas2 = Canvas(recorder);
//      Paint paint2 = new Paint();
//      canvas2.drawImageRect(image, source, dest, paint2);
//      var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());
////    canvas.drawImageRect(image2, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH(0,0,400,400), paint);
//      image2.toByteData(format:ui.ImageByteFormat.png).then((byte){
//        var mByte=byte.buffer.asUint8List();
//        print(""aaa ${mByte}"");
//        getApplicationDocumentsDirectory().then((dir){
//          String path = dir.path +""/test.png"";
//          new File(path).writeAsBytesSync(mByte);
//          print(path);
//        });
//      });
   }

   if(this.points.length>1){
     double startX = points[1].dx - points[0].dx+points[2].dx;
     double startY = points[1].dy - points[0].dy+points[2].dy;
     if(startX<0)
       startX = 0;
     else if(startX+cWidth>width){
       startX = width-cWidth;
     }
     if(startY<0)
       startY=0;
     else if(startY + cHeight>height){
       startY = height-cHeight;
     }
//      canvas.drawRect(Rect.fromLTRB(startX,startY,startX+200,startY+200), paint);
     List<Offset> points2 =[
       Offset(startX,startY),
       Offset(startX+cWidth,startY),
       Offset(startX+cWidth,startY+cHeight),
       Offset(startX,startY+cHeight),
       Offset(startX,startY),
     ];
     canvas.drawPoints(PointMode.polygon, points2, paint);
   }
   else{
     List<Offset> points2 =[
       Offset(startX,startY),
       Offset(startX+cWidth,startY),
       Offset(startX+cWidth,startY+cHeight),
       Offset(startX,startY+cHeight),
       Offset(startX,startY),
     ];
     canvas.drawPoints(PointMode.polygon, points2, paint);
   }

 }`",3
,2738,144,58,29992,"I am using vs code and trying to debug the application, but when i hit 'Start Debugging' button in vs code, app simply launches and never hits breakpoint. anyone has a clue asto what could be wrong ?",Unable to debug / hit breakpoints in flutter app in the visual studio code,"Unable to debug / hit breakpoints in flutter app in the visual studio codeI am using vs code and trying to debug the application, but when i hit 'Start Debugging' button in vs code, app simply launches and never hits breakpoint. anyone has a clue asto what could be wrong ?",3
,2739,144,59,22175,"Steps to Reproduce


Navigate to a flutter project
run flutter doctor (or any flutter command)
Experience this output:

Building flutter tool...
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (9 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (8 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (7 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (6 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (5 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (4 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (3 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (2 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (1 tries left)
Command 'pub upgrade' still failed after 10 tries, giving up.

Logs
Can't run anything with flutter to get some logs. It just gives me the above response.","Could not find a file named ""pubspec.yaml"" in ""...pub.dartlang.org/archive-2.0.4"".","Could not find a file named ""pubspec.yaml"" in ""...pub.dartlang.org/archive-2.0.4"".Steps to Reproduce


Navigate to a flutter project
run flutter doctor (or any flutter command)
Experience this output:

Building flutter tool...
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (9 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (8 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (7 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (6 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (5 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (4 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (3 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (2 tries left)
Could not find a file named ""pubspec.yaml"" in ""/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4"".
Error: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (1 tries left)
Command 'pub upgrade' still failed after 10 tries, giving up.

Logs
Can't run anything with flutter to get some logs. It just gives me the above response.",3
,2740,148,60,21545,"Even with the provided example, I'm experiencing an issue with camera plugin's orientation when device is rotated. I confirmed this error across different android versions. When the orientation is portrait, everything is fine, working so well. But when we turn around the device, the camera view is 90 degrees off the correct view in clockwise direction, that it should provide. I will attach few screenshots which would help to describe the issue more precisely.
Portrait View

Landscape View",Orientation(Rotation) Issue of Camera Plugin,"Orientation(Rotation) Issue of Camera PluginEven with the provided example, I'm experiencing an issue with camera plugin's orientation when device is rotated. I confirmed this error across different android versions. When the orientation is portrait, everything is fine, working so well. But when we turn around the device, the camera view is 90 degrees off the correct view in clockwise direction, that it should provide. I will attach few screenshots which would help to describe the issue more precisely.
Portrait View

Landscape View",3
,2741,148,61,21632,"With engine roll 6a8a73c the following benchmark regressed:

futter_gallery_ios32__transition_perf/99th_percentile_frame_rasterizer_time_millis

/cc @bkonyi",Engine roll caused regression,"Engine roll caused regressionWith engine roll 6a8a73c the following benchmark regressed:

futter_gallery_ios32__transition_perf/99th_percentile_frame_rasterizer_time_millis

/cc @bkonyi",3
,2742,142,62,16421,"I could use support reading/writing metadata, especially custom metadata field.",Request: firebase_storage to support metadata,"Request: firebase_storage to support metadataI could use support reading/writing metadata, especially custom metadata field.",3
,2743,148,63,31045,"I found this line of code in flutter/lib/src/painting/matrix_utils.dart, line 166, commit: 27b058a
assert(transform.determinant != 0.0);

I guess it is expected to be:
assert(transform.determinant() != 0.0);

However, if my guess is correct, there is another issue which causes this assertion to fail. Followed are the steps to reproduce

Replace determinant with determinant() as above;
Open project flutter_gallery in Android Studio;
Start running main.dart in Android simulator;
As the application is launched, tap 'Material -> Bottom app bar' to open the bottom app bar demo;
Tap 'None' radio in FAB shape, the Floating Action Button should disappear;
Tap 'Circular' or 'Diamond' radio in  FAB shape.

The console output looks as follows
Launching lib/main.dart on Android SDK built for x86 in debug mode...
Initializing gradle...
Resolving dependencies...
Running Gradle task 'assembleDebug'...
Built build/app/outputs/apk/debug/app-debug.apk.
Installing build/app/outputs/apk/app.apk...
Syncing files to device Android SDK built for x86...
D/EGL_emulation( 7685): eglMakeCurrent: 0xe74852a0: ver 3 0 (tinfo 0xe74832e0)
I/flutter ( 7685):  EXCEPTION CAUGHT BY RENDERING LIBRARY 
I/flutter ( 7685): The following NoSuchMethodError was thrown during paint():
I/flutter ( 7685): The getter 'left' was called on null.
I/flutter ( 7685): Receiver: null
I/flutter ( 7685): Tried calling: left
I/flutter ( 7685): 
I/flutter ( 7685): When the exception was thrown, this was the stack:
I/flutter ( 7685): #0      Object.noSuchMethod (dart:core-patch/object_patch.dart:50:5)
I/flutter ( 7685): #1      Rect.overlaps (dart:ui/geometry.dart:771:24)
I/flutter ( 7685): #2      _DiamondNotchedRectangle.getOuterPath (package:flutter_gallery/demo/material/bottom_app_bar_demo.dart:459:15)
I/flutter ( 7685): #3      _BottomAppBarClipper.getClip (package:flutter/src/material/bottom_app_bar.dart:166:18)
I/flutter ( 7685): #4      _RenderCustomClip._updateClip (package:flutter/src/rendering/proxy_box.dart:1212:25)
I/flutter ( 7685): #5      RenderPhysicalShape.paint (package:flutter/src/rendering/proxy_box.dart:1780:7)
I/flutter ( 7685): #6      RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #7      PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #8      _RenderCustomMultiChildLayoutBox&RenderBox&ContainerRenderObjectMixin&RenderBoxContainerDefaultsMixin.defaultPaint (package:flutter/src/rendering/box.dart:2273:15)
I/flutter ( 7685): #9      RenderCustomMultiChildLayoutBox.paint (package:flutter/src/rendering/custom_layout.dart:361:5)
I/flutter ( 7685): #10     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #11     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #12     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #13     _RenderInkFeatures.paint (package:flutter/src/material/material.dart:510:11)
I/flutter ( 7685): #14     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #15     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #16     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #17     PaintingContext.pushLayer (package:flutter/src/rendering/object.dart:370:12)
I/flutter ( 7685): #18     RenderPhysicalModel.paint (package:flutter/src/rendering/proxy_box.dart:1716:15)
I/flutter ( 7685): #19     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #20     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #21     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #22     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #23     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #24     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #25     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #26     PaintingContext._repaintCompositedChild (package:flutter/src/rendering/object.dart:128:11)
I/flutter ( 7685): #27     PaintingContext.repaintCompositedChild (package:flutter/src/rendering/object.dart:96:5)
I/flutter ( 7685): #28     PipelineOwner.flushPaint (package:flutter/src/rendering/object.dart:859:29)
I/flutter ( 7685): #29     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding.drawFrame (package:flutter/src/rendering/binding.dart:349:19)
I/flutter ( 7685): #30     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:701:13)
I/flutter ( 7685): #31     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:286:5)
I/flutter ( 7685): #32     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:1012:15)
I/flutter ( 7685): #33     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:952:9)
I/flutter ( 7685): #34     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:864:5)
I/flutter ( 7685): #38     _invoke (dart:ui/hooks.dart:219:10)
I/flutter ( 7685): #39     _drawFrame (dart:ui/hooks.dart:178:3)
I/flutter ( 7685): (elided 3 frames from package dart:async)
I/flutter ( 7685): 
I/flutter ( 7685): The following RenderObject was being processed when the exception was fired:
I/flutter ( 7685):   RenderPhysicalShape#9225c relayoutBoundary=up1
I/flutter ( 7685):   creator: PhysicalShape  BottomAppBar  _DemoBottomAppBar  MediaQuery 
I/flutter ( 7685):   LayoutId-[<_ScaffoldSlot.bottomNavigationBar>]  CustomMultiChildLayout  AnimatedBuilder 
I/flutter ( 7685):   DefaultTextStyle  AnimatedDefaultTextStyle  _InkFeatures-[GlobalKey#52319 ink renderer] 
I/flutter ( 7685):   NotificationListener<LayoutChangedNotification>  PhysicalModel  
I/flutter ( 7685):   parentData: offset=Offset(0.0, 635.4); id=_ScaffoldSlot.bottomNavigationBar (can use size)
I/flutter ( 7685):   constraints: BoxConstraints(w=411.4, 0.0<=h<=683.4)
I/flutter ( 7685):   size: Size(411.4, 48.0)
I/flutter ( 7685):   elevation: 8.0
I/flutter ( 7685):   color: Color(0xffffffff)
I/flutter ( 7685):   shadowColor: Color(0xffffffff)
I/flutter ( 7685):   clipper: _BottomAppBarClipper
I/flutter ( 7685): This RenderObject had the following descendants (showing up to depth 5):
I/flutter ( 7685):   RenderCustomPaint#22f12 relayoutBoundary=up2
I/flutter ( 7685):     _RenderInkFeatures#b82a0 relayoutBoundary=up3
I/flutter ( 7685):       RenderPadding#39bef relayoutBoundary=up4
I/flutter ( 7685):         RenderFlex#40c20 relayoutBoundary=up5
I/flutter ( 7685):           RenderSemanticsGestureHandler#24e16 relayoutBoundary=up6
I/flutter ( 7685):           RenderSemanticsGestureHandler#cdda6 relayoutBoundary=up6
I/flutter ( 7685):           RenderSemanticsGestureHandler#26a78 relayoutBoundary=up6
I/flutter ( 7685): 
I/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.
I/flutter ( 7685): Another exception was thrown: NoSuchMethodError: The getter 'left' was called on null.
I/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.
Application finished.",Is this a typo? assert(transform.determinant != 0.0);,"Is this a typo? assert(transform.determinant != 0.0);I found this line of code in flutter/lib/src/painting/matrix_utils.dart, line 166, commit: 27b058a
assert(transform.determinant != 0.0);

I guess it is expected to be:
assert(transform.determinant() != 0.0);

However, if my guess is correct, there is another issue which causes this assertion to fail. Followed are the steps to reproduce

Replace determinant with determinant() as above;
Open project flutter_gallery in Android Studio;
Start running main.dart in Android simulator;
As the application is launched, tap 'Material -> Bottom app bar' to open the bottom app bar demo;
Tap 'None' radio in FAB shape, the Floating Action Button should disappear;
Tap 'Circular' or 'Diamond' radio in  FAB shape.

The console output looks as follows
Launching lib/main.dart on Android SDK built for x86 in debug mode...
Initializing gradle...
Resolving dependencies...
Running Gradle task 'assembleDebug'...
Built build/app/outputs/apk/debug/app-debug.apk.
Installing build/app/outputs/apk/app.apk...
Syncing files to device Android SDK built for x86...
D/EGL_emulation( 7685): eglMakeCurrent: 0xe74852a0: ver 3 0 (tinfo 0xe74832e0)
I/flutter ( 7685):  EXCEPTION CAUGHT BY RENDERING LIBRARY 
I/flutter ( 7685): The following NoSuchMethodError was thrown during paint():
I/flutter ( 7685): The getter 'left' was called on null.
I/flutter ( 7685): Receiver: null
I/flutter ( 7685): Tried calling: left
I/flutter ( 7685): 
I/flutter ( 7685): When the exception was thrown, this was the stack:
I/flutter ( 7685): #0      Object.noSuchMethod (dart:core-patch/object_patch.dart:50:5)
I/flutter ( 7685): #1      Rect.overlaps (dart:ui/geometry.dart:771:24)
I/flutter ( 7685): #2      _DiamondNotchedRectangle.getOuterPath (package:flutter_gallery/demo/material/bottom_app_bar_demo.dart:459:15)
I/flutter ( 7685): #3      _BottomAppBarClipper.getClip (package:flutter/src/material/bottom_app_bar.dart:166:18)
I/flutter ( 7685): #4      _RenderCustomClip._updateClip (package:flutter/src/rendering/proxy_box.dart:1212:25)
I/flutter ( 7685): #5      RenderPhysicalShape.paint (package:flutter/src/rendering/proxy_box.dart:1780:7)
I/flutter ( 7685): #6      RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #7      PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #8      _RenderCustomMultiChildLayoutBox&RenderBox&ContainerRenderObjectMixin&RenderBoxContainerDefaultsMixin.defaultPaint (package:flutter/src/rendering/box.dart:2273:15)
I/flutter ( 7685): #9      RenderCustomMultiChildLayoutBox.paint (package:flutter/src/rendering/custom_layout.dart:361:5)
I/flutter ( 7685): #10     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #11     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #12     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #13     _RenderInkFeatures.paint (package:flutter/src/material/material.dart:510:11)
I/flutter ( 7685): #14     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #15     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #16     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #17     PaintingContext.pushLayer (package:flutter/src/rendering/object.dart:370:12)
I/flutter ( 7685): #18     RenderPhysicalModel.paint (package:flutter/src/rendering/proxy_box.dart:1716:15)
I/flutter ( 7685): #19     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #20     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #21     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #22     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #23     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)
I/flutter ( 7685): #24     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)
I/flutter ( 7685): #25     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)
I/flutter ( 7685): #26     PaintingContext._repaintCompositedChild (package:flutter/src/rendering/object.dart:128:11)
I/flutter ( 7685): #27     PaintingContext.repaintCompositedChild (package:flutter/src/rendering/object.dart:96:5)
I/flutter ( 7685): #28     PipelineOwner.flushPaint (package:flutter/src/rendering/object.dart:859:29)
I/flutter ( 7685): #29     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding.drawFrame (package:flutter/src/rendering/binding.dart:349:19)
I/flutter ( 7685): #30     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:701:13)
I/flutter ( 7685): #31     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:286:5)
I/flutter ( 7685): #32     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:1012:15)
I/flutter ( 7685): #33     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:952:9)
I/flutter ( 7685): #34     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:864:5)
I/flutter ( 7685): #38     _invoke (dart:ui/hooks.dart:219:10)
I/flutter ( 7685): #39     _drawFrame (dart:ui/hooks.dart:178:3)
I/flutter ( 7685): (elided 3 frames from package dart:async)
I/flutter ( 7685): 
I/flutter ( 7685): The following RenderObject was being processed when the exception was fired:
I/flutter ( 7685):   RenderPhysicalShape#9225c relayoutBoundary=up1
I/flutter ( 7685):   creator: PhysicalShape  BottomAppBar  _DemoBottomAppBar  MediaQuery 
I/flutter ( 7685):   LayoutId-[<_ScaffoldSlot.bottomNavigationBar>]  CustomMultiChildLayout  AnimatedBuilder 
I/flutter ( 7685):   DefaultTextStyle  AnimatedDefaultTextStyle  _InkFeatures-[GlobalKey#52319 ink renderer] 
I/flutter ( 7685):   NotificationListener<LayoutChangedNotification>  PhysicalModel  
I/flutter ( 7685):   parentData: offset=Offset(0.0, 635.4); id=_ScaffoldSlot.bottomNavigationBar (can use size)
I/flutter ( 7685):   constraints: BoxConstraints(w=411.4, 0.0<=h<=683.4)
I/flutter ( 7685):   size: Size(411.4, 48.0)
I/flutter ( 7685):   elevation: 8.0
I/flutter ( 7685):   color: Color(0xffffffff)
I/flutter ( 7685):   shadowColor: Color(0xffffffff)
I/flutter ( 7685):   clipper: _BottomAppBarClipper
I/flutter ( 7685): This RenderObject had the following descendants (showing up to depth 5):
I/flutter ( 7685):   RenderCustomPaint#22f12 relayoutBoundary=up2
I/flutter ( 7685):     _RenderInkFeatures#b82a0 relayoutBoundary=up3
I/flutter ( 7685):       RenderPadding#39bef relayoutBoundary=up4
I/flutter ( 7685):         RenderFlex#40c20 relayoutBoundary=up5
I/flutter ( 7685):           RenderSemanticsGestureHandler#24e16 relayoutBoundary=up6
I/flutter ( 7685):           RenderSemanticsGestureHandler#cdda6 relayoutBoundary=up6
I/flutter ( 7685):           RenderSemanticsGestureHandler#26a78 relayoutBoundary=up6
I/flutter ( 7685): 
I/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.
I/flutter ( 7685): Another exception was thrown: NoSuchMethodError: The getter 'left' was called on null.
I/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.
Application finished.",3
,2744,148,64,14373,"More or less the reverse of #8945.
Given a new Flutter project, if some code needs to be done in the platform side:

 How to open Xcode, how is the iOS code structured
 How to open Android Studio, how is the Android code structured
 How to bring in existing local iOS/Android source code
 How to refer to existing iOS/Android packages using gradle / CocoaPods

cc @sethladd, @Sfshaza",Unclear how to add/edit platform view code to an existing Flutter app,"Unclear how to add/edit platform view code to an existing Flutter appMore or less the reverse of #8945.
Given a new Flutter project, if some code needs to be done in the platform side:

 How to open Xcode, how is the iOS code structured
 How to open Android Studio, how is the Android code structured
 How to bring in existing local iOS/Android source code
 How to refer to existing iOS/Android packages using gradle / CocoaPods

cc @sethladd, @Sfshaza",3
,2745,145,65,26829,"When user switches app during rewarding video playback and brings it back to active state, video is paused. Yes, if user clicks on ""close"" button he/she is prompted to resume the video, but it would be nice to have an API to automatically continue playing video",firebase_admob pauses video when app goes to background,"firebase_admob pauses video when app goes to backgroundWhen user switches app during rewarding video playback and brings it back to active state, video is paused. Yes, if user clicks on ""close"" button he/she is prompted to resume the video, but it would be nice to have an API to automatically continue playing video",3
,2746,148,66,2832,"Scrolling the component list up under the app bar flashes the shadow at the border between the app bar and the component list.
Movie:  https://dl.dropboxusercontent.com/u/316685/RECORDING.mp4",Double shadow flash when moving component list underneath app bar,"Double shadow flash when moving component list underneath app barScrolling the component list up under the app bar flashes the shadow at the border between the app bar and the component list.
Movie:  https://dl.dropboxusercontent.com/u/316685/RECORDING.mp4",3
,2747,148,67,21476,"I tried to auto-format my code in vscode with a flutter extension. And this gave me this result

Sometimes it uses 2 spaces, and sometimes it uses 4 spaces. Is this working correctly?",VScode autoformat intending spaces,"VScode autoformat intending spacesI tried to auto-format my code in vscode with a flutter extension. And this gave me this result

Sometimes it uses 2 spaces, and sometimes it uses 4 spaces. Is this working correctly?",3
,2748,144,68,31049,"Steps to Reproduce

Have a method open an AlertDialog
Have a button execute the method
In a test, use await tester.tap(find.byKey(const Key('LoginBtnKey'))); to tap the button
Then verify that the dialog is shown using expect(find.byType(AlertDialog), findsOneWidget);
This gives a framework error.


Logs

 EXCEPTION CAUGHT BY GESTURE 
The following assertion was thrown while handling a gesture:
'package:flutter/src/widgets/localizations.dart': Failed assertion: line 446 pos 12: 'context !=
null': is not true.

Either the assertion indicates an error in the framework itself, or we should provide substantially
more information in this error message to help you determine and fix the underlying cause.
In either case, please report this assertion by filing a bug on GitHub:
  https://github.com/flutter/flutter/issues/new?template=BUG.md

When the exception was thrown, this was the stack:
#2      Localizations.of (package:flutter/src/widgets/localizations.dart:446:12)
#3      debugCheckHasMaterialLocalizations.<anonymous closure> (package:flutter/src/material/debug.dart:88:23)
#4      debugCheckHasMaterialLocalizations (package:flutter/src/material/debug.dart:127:4)
#5      showDialog (package:flutter/src/material/dialog.dart:701:10)


Analyzing weekplanner...                                                
No issues found! (ran in 6.1s; 157 public members lack documentation)


[] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale da-DK)
     Flutter version 1.2.1 at /Users/Tricky/development/flutter
     Framework revision 8661d8aecd (8 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /Users/Tricky/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses

[] iOS toolchain - develop for iOS devices (Xcode 10.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 10.2, Build version 10E125
     ios-deploy 1.9.4
     CocoaPods version 1.6.1

[] Android Studio (version 3.3)
     Android Studio at /Applications/Android Studio.app/Contents
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] IntelliJ IDEA Community Edition (version 2018.3.5)
     IntelliJ at /Applications/IntelliJ IDEA CE.app
     Flutter plugin version 33.3.2
     Dart plugin version 183.5912.23

[!] VS Code (version 1.29.1)
     VS Code at /Applications/Visual Studio Code.app/Contents
     Flutter extension not installed; install from
      https://marketplace.visualstudio.com/items?itemName=Dart-Code.flutter

[!] Connected device
    ! No devices available

! Doctor found issues in 3 categories.",Testing buttons using tester.tap to show a dialog gives Framework Error,"Testing buttons using tester.tap to show a dialog gives Framework ErrorSteps to Reproduce

Have a method open an AlertDialog
Have a button execute the method
In a test, use await tester.tap(find.byKey(const Key('LoginBtnKey'))); to tap the button
Then verify that the dialog is shown using expect(find.byType(AlertDialog), findsOneWidget);
This gives a framework error.


Logs

 EXCEPTION CAUGHT BY GESTURE 
The following assertion was thrown while handling a gesture:
'package:flutter/src/widgets/localizations.dart': Failed assertion: line 446 pos 12: 'context !=
null': is not true.

Either the assertion indicates an error in the framework itself, or we should provide substantially
more information in this error message to help you determine and fix the underlying cause.
In either case, please report this assertion by filing a bug on GitHub:
  https://github.com/flutter/flutter/issues/new?template=BUG.md

When the exception was thrown, this was the stack:
#2      Localizations.of (package:flutter/src/widgets/localizations.dart:446:12)
#3      debugCheckHasMaterialLocalizations.<anonymous closure> (package:flutter/src/material/debug.dart:88:23)
#4      debugCheckHasMaterialLocalizations (package:flutter/src/material/debug.dart:127:4)
#5      showDialog (package:flutter/src/material/dialog.dart:701:10)


Analyzing weekplanner...                                                
No issues found! (ran in 6.1s; 157 public members lack documentation)


[] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale da-DK)
     Flutter version 1.2.1 at /Users/Tricky/development/flutter
     Framework revision 8661d8aecd (8 weeks ago), 2019-02-14 19:19:53 -0800
     Engine revision 3757390fa4
     Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)

[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
     Android SDK at /Users/Tricky/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)
     Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses

[] iOS toolchain - develop for iOS devices (Xcode 10.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 10.2, Build version 10E125
     ios-deploy 1.9.4
     CocoaPods version 1.6.1

[] Android Studio (version 3.3)
     Android Studio at /Applications/Android Studio.app/Contents
     Flutter plugin version 33.3.1
     Dart plugin version 182.5215
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)

[] IntelliJ IDEA Community Edition (version 2018.3.5)
     IntelliJ at /Applications/IntelliJ IDEA CE.app
     Flutter plugin version 33.3.2
     Dart plugin version 183.5912.23

[!] VS Code (version 1.29.1)
     VS Code at /Applications/Visual Studio Code.app/Contents
     Flutter extension not installed; install from
      https://marketplace.visualstudio.com/items?itemName=Dart-Code.flutter

[!] Connected device
    ! No devices available

! Doctor found issues in 3 categories.",3
,2749,148,69,31617,"Steps to Reproduce

So I'm having issues getting the flutter firebase_admob plugin to work on Android (Release). It initializes and loads ads perfectly for the debug build on Android and the debug/release builds on iOS. I've already made the necessary changes to the AndroidManifest.xml file and included my admob app id (See screenshot). I'm really lost at this point. I've looked at all of the available solutions for the issue I'm having and none have worked. Please help!
Logs






info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:30:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:35:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:41:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:47:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:58:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:65:3  sdk_version_async_exported_from_core
info  This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final:
DeleteProgressDialog.progress, DeleteProgressDialog.timeRemaining  lib/ui/DeleteProgressDialog.dart:5:7  must_be_immutable
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:22:1  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:76:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:130:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:138:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:146:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:245:3  sdk_version_async_exported_from_core
info  This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final: UserInfoHeader.user 
lib/ui/UserInfoHeader.dart:5:7  must_be_immutable


[] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale en-US)
 Flutter version 1.2.1 at /Users/xorrior/flutter
 Framework revision 8661d8a (2 months ago), 2019-02-14 19:19:53 -0800
 Engine revision 3757390
 Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)
[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
 Android SDK at /Users/xorrior/Library/Android/sdk
 Android NDK location not configured (optional; useful for native profiling support)
 Platform android-28, build-tools 28.0.3
 Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
 Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
 All Android licenses accepted.
[] iOS toolchain - develop for iOS devices (Xcode 10.2.1)
 Xcode at /Applications/Xcode.app/Contents/Developer
 Xcode 10.2.1, Build version 10E1001
 ios-deploy 1.9.4
 CocoaPods version 1.5.3
[] Android Studio (version 3.4)
 Android Studio at /Applications/Android Studio.app/Contents
 Flutter plugin version 34.0.2
 Dart plugin version 183.5901
 Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
[] IntelliJ IDEA Ultimate Edition (version 2019.1.1)
 IntelliJ at /Applications/IntelliJ IDEA.app
 Flutter plugin version 34.0.4
 Dart plugin version 191.7019
[] VS Code (version 1.33.1)
 VS Code at /Applications/Visual Studio Code.app/Contents
 Flutter extension version 2.25.1
[] Connected device (1 available)
 Pixel 3 XL  93VY18Y3R  android-arm64  Android 9 (API 28)",firebase_admob ^0.8.0+4 Android crashes on startup (Release version only),"firebase_admob ^0.8.0+4 Android crashes on startup (Release version only)Steps to Reproduce

So I'm having issues getting the flutter firebase_admob plugin to work on Android (Release). It initializes and loads ads perfectly for the debug build on Android and the debug/release builds on iOS. I've already made the necessary changes to the AndroidManifest.xml file and included my admob app id (See screenshot). I'm really lost at this point. I've looked at all of the available solutions for the issue I'm having and none have worked. Please help!
Logs






info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:30:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:35:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:41:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:47:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:58:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/TwitterApi.dart:65:3  sdk_version_async_exported_from_core
info  This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final:
DeleteProgressDialog.progress, DeleteProgressDialog.timeRemaining  lib/ui/DeleteProgressDialog.dart:5:7  must_be_immutable
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:22:1  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:76:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:130:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:138:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:146:3  sdk_version_async_exported_from_core
info  The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions 
lib/ui/Login.dart:245:3  sdk_version_async_exported_from_core
info  This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final: UserInfoHeader.user 
lib/ui/UserInfoHeader.dart:5:7  must_be_immutable


[] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale en-US)
 Flutter version 1.2.1 at /Users/xorrior/flutter
 Framework revision 8661d8a (2 months ago), 2019-02-14 19:19:53 -0800
 Engine revision 3757390
 Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)
[] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
 Android SDK at /Users/xorrior/Library/Android/sdk
 Android NDK location not configured (optional; useful for native profiling support)
 Platform android-28, build-tools 28.0.3
 Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
 Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
 All Android licenses accepted.
[] iOS toolchain - develop for iOS devices (Xcode 10.2.1)
 Xcode at /Applications/Xcode.app/Contents/Developer
 Xcode 10.2.1, Build version 10E1001
 ios-deploy 1.9.4
 CocoaPods version 1.5.3
[] Android Studio (version 3.4)
 Android Studio at /Applications/Android Studio.app/Contents
 Flutter plugin version 34.0.2
 Dart plugin version 183.5901
 Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
[] IntelliJ IDEA Ultimate Edition (version 2019.1.1)
 IntelliJ at /Applications/IntelliJ IDEA.app
 Flutter plugin version 34.0.4
 Dart plugin version 191.7019
[] VS Code (version 1.33.1)
 VS Code at /Applications/Visual Studio Code.app/Contents
 Flutter extension version 2.25.1
[] Connected device (1 available)
 Pixel 3 XL  93VY18Y3R  android-arm64  Android 9 (API 28)",3
,2750,148,70,21068,"I have a Streambuilder listening to a behaviourSubject broadcastStream, but whenever I Hot Reload or Hot Restart the code for any small change, The streambuilder stops seing what's on the tip of the stream, forcing me to re-do what I originally did to fill the stream",Hot reloading a page with streambuilder bugs out,"Hot reloading a page with streambuilder bugs outI have a Streambuilder listening to a behaviourSubject broadcastStream, but whenever I Hot Reload or Hot Restart the code for any small change, The streambuilder stops seing what's on the tip of the stream, forcing me to re-do what I originally did to fill the stream",3
,2751,144,71,53,"When I try to flutter start any of the example programs widgets, fitness or rendering I get an error like the following:
Downloading Sky Snapshot from the cloud, one moment please...
Downloading Sky Shell from the cloud, one moment please...
[1108/213053:FATAL:loader.cc(21)] Check failed: base::ReadFileToString(path, &source). /home/dmta/flutter/flutter/examples/rendering/./lib/main.dart
The other examples I have tried worked fine.",`flutter start` doesn't give good error messages when it can't find main.dart,"`flutter start` doesn't give good error messages when it can't find main.dartWhen I try to flutter start any of the example programs widgets, fitness or rendering I get an error like the following:
Downloading Sky Snapshot from the cloud, one moment please...
Downloading Sky Shell from the cloud, one moment please...
[1108/213053:FATAL:loader.cc(21)] Check failed: base::ReadFileToString(path, &source). /home/dmta/flutter/flutter/examples/rendering/./lib/main.dart
The other examples I have tried worked fine.",3
,2752,145,72,2844,"Not exactly an issue but a question.
Im working for a small dev company and we're talking about using flutter in production.
There is no deadline in the next 6 months.
Will it be an arm version by then?
And as a secondary question, will it be stable enough?
Thank you",ARM Support,"ARM SupportNot exactly an issue but a question.
Im working for a small dev company and we're talking about using flutter in production.
There is no deadline in the next 6 months.
Will it be an arm version by then?
And as a secondary question, will it be stable enough?
Thank you",3
,2753,145,73,20912,"Steps to Reproduce

The previous version worked fine. However after  I have followed the new instructions(updated yesterday) for integrating Flutter into existing iOS project. This error came up when I push a FlutterViewController
Logs

[VERBOSE-2:dart_error.cc(16)] Unhandled exception:
MissingPluginException(No implementation found for method getAll on channel plugins.flutter.io/package_info)
#0      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:278:7)
<asynchronous suspension>
#1      PackageInfo.fromPlatform (package:package_info/package_info.dart:35:17)
<asynchronous suspension>
#2      main (file:///Users/jlin/Workspace/iOS-Univadis/Univadis/Flutter/flunivadis/lib/main.dart:8:53)
<asynchronous suspension>
#3      _startIsolate.<anonymous closure> (dart:isolate/runtime/libisolate_patch.dart:279:19)
#4      _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:165:12)


  Flutter git:(poc/integrate_flutter)  flutter doctor -v
[] Flutter (Channel dev, v0.5.7, on Mac OS X 10.13.6 17G65, locale en-FR)
     Flutter version 0.5.7 at /Users/jlin/lib/flutter
     Framework revision 66091f9696 (6 weeks ago), 2018-07-09 12:52:41 -0700
     Engine revision 6fe748490d
     Dart version 2.0.0-dev.63.0.flutter-4c9689c1d2

[] Android toolchain - develop for Android devices (Android SDK 28.0.2)
     Android SDK at /Users/jlin/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.2
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)
     All Android licenses accepted.

[] iOS toolchain - develop for iOS devices (Xcode 9.4.1)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.4.1, Build version 9F2000
     ios-deploy 1.9.2
     CocoaPods version 1.5.3

[] Android Studio (version 3.1)
     Android Studio at /Applications/Android Studio.app/Contents
     Flutter plugin version 27.1.1
     Dart plugin version 173.4700
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)

[] Connected devices (2 available)
     iPhone 8    5BC11302-634E-4D3A-ABE5-C52EA137DEBC  ios  iOS 11.4 (simulator)
     iPad Air 2  2E079443-DD9E-4803-894D-596BB7F7B597  ios  iOS 11.4 (simulator)

 No issues found!",Issue on new instructions for integrating Flutter into existing iOS project. ,"Issue on new instructions for integrating Flutter into existing iOS project. Steps to Reproduce

The previous version worked fine. However after  I have followed the new instructions(updated yesterday) for integrating Flutter into existing iOS project. This error came up when I push a FlutterViewController
Logs

[VERBOSE-2:dart_error.cc(16)] Unhandled exception:
MissingPluginException(No implementation found for method getAll on channel plugins.flutter.io/package_info)
#0      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:278:7)
<asynchronous suspension>
#1      PackageInfo.fromPlatform (package:package_info/package_info.dart:35:17)
<asynchronous suspension>
#2      main (file:///Users/jlin/Workspace/iOS-Univadis/Univadis/Flutter/flunivadis/lib/main.dart:8:53)
<asynchronous suspension>
#3      _startIsolate.<anonymous closure> (dart:isolate/runtime/libisolate_patch.dart:279:19)
#4      _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:165:12)


  Flutter git:(poc/integrate_flutter)  flutter doctor -v
[] Flutter (Channel dev, v0.5.7, on Mac OS X 10.13.6 17G65, locale en-FR)
     Flutter version 0.5.7 at /Users/jlin/lib/flutter
     Framework revision 66091f9696 (6 weeks ago), 2018-07-09 12:52:41 -0700
     Engine revision 6fe748490d
     Dart version 2.0.0-dev.63.0.flutter-4c9689c1d2

[] Android toolchain - develop for Android devices (Android SDK 28.0.2)
     Android SDK at /Users/jlin/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.2
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)
     All Android licenses accepted.

[] iOS toolchain - develop for iOS devices (Xcode 9.4.1)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.4.1, Build version 9F2000
     ios-deploy 1.9.2
     CocoaPods version 1.5.3

[] Android Studio (version 3.1)
     Android Studio at /Applications/Android Studio.app/Contents
     Flutter plugin version 27.1.1
     Dart plugin version 173.4700
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)

[] Connected devices (2 available)
     iPhone 8    5BC11302-634E-4D3A-ABE5-C52EA137DEBC  ios  iOS 11.4 (simulator)
     iPad Air 2  2E079443-DD9E-4803-894D-596BB7F7B597  ios  iOS 11.4 (simulator)

 No issues found!",3
,2754,145,74,27446,"Instead of scanning the project directory on hot reload, setup a Directory watcher to report updated dart files. When hot reloading, grab the current set and pass them to the resident compiler.
Since we no longer need to sync source files to the device, the extra work we're doing to setup the devfs for each potential entry is wasted. We only need to sync the dill file and assets.
Additionally, including build_runner will drastically increase the number of files that will be scanned with the current approach - while there is no issue with a watcher based approach.",Update DevFS to use a Watcher,"Update DevFS to use a WatcherInstead of scanning the project directory on hot reload, setup a Directory watcher to report updated dart files. When hot reloading, grab the current set and pass them to the resident compiler.
Since we no longer need to sync source files to the device, the extra work we're doing to setup the devfs for each potential entry is wasted. We only need to sync the dill file and assets.
Additionally, including build_runner will drastically increase the number of files that will be scanned with the current approach - while there is no issue with a watcher based approach.",3
,2755,146,75,5674,"Steps to Reproduce

Clean install (no iOS simulator)
flutter create test-ios
flutter run

At this point it responds with No connected devices. which is correct but unhelpful. I would like it to hint that I could install the iOS simulator and point to docs explaining how to do so. Maybe flutter doctor should indicate that it is not installed / configured as well.
Flutter Doctor
$ flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/danrubel/work/git/flutter/flutter
     Framework revision 83bf5d10c0 (21 hours ago), 2016-08-30 14:11:54
     Engine revision c4022b61fa
     Tools Dart version 1.19.0-dev.5.0

[] Android toolchain - develop for Android devices (Android SDK 22.0.1)
     Android SDK at /Users/danrubel/Library/Android/sdk
     Platform android-23, build-tools 22.0.1
     Java(TM) SE Runtime Environment (build 1.8.0_91-b14)

[] iOS toolchain - develop for iOS devices (Xcode 7.3.1)
     XCode at /Applications/Xcode.app/Contents/Developer
     Xcode 7.3.1, Build version 7D1014

[] Atom - a lightweight development environment for Flutter
     flutter plugin version 0.2.4
     dartlang plugin version 0.6.35",flutter tool provide hint when/how to install iOS simulator,"flutter tool provide hint when/how to install iOS simulatorSteps to Reproduce

Clean install (no iOS simulator)
flutter create test-ios
flutter run

At this point it responds with No connected devices. which is correct but unhelpful. I would like it to hint that I could install the iOS simulator and point to docs explaining how to do so. Maybe flutter doctor should indicate that it is not installed / configured as well.
Flutter Doctor
$ flutter doctor
[] Flutter (on Mac OS, channel master)
     Flutter at /Users/danrubel/work/git/flutter/flutter
     Framework revision 83bf5d10c0 (21 hours ago), 2016-08-30 14:11:54
     Engine revision c4022b61fa
     Tools Dart version 1.19.0-dev.5.0

[] Android toolchain - develop for Android devices (Android SDK 22.0.1)
     Android SDK at /Users/danrubel/Library/Android/sdk
     Platform android-23, build-tools 22.0.1
     Java(TM) SE Runtime Environment (build 1.8.0_91-b14)

[] iOS toolchain - develop for iOS devices (Xcode 7.3.1)
     XCode at /Applications/Xcode.app/Contents/Developer
     Xcode 7.3.1, Build version 7D1014

[] Atom - a lightweight development environment for Flutter
     flutter plugin version 0.2.4
     dartlang plugin version 0.6.35",3
,2756,146,76,1211,Would help debug bugs like #1210 .,RenderObject exceptions in performLayout should also include information about what the child(ren) are,RenderObject exceptions in performLayout should also include information about what the child(ren) areWould help debug bugs like #1210 .,3
,2757,142,77,11380,"We probably need to create an RTL-aware version of TextAlign that is used by TextPainter, then use that in RenderParagraph, RichText, and Text so that you can align start/end as well as left/right. Also ""justify"" needs to define the justification of the last line.",TextAlign RTL,"TextAlign RTLWe probably need to create an RTL-aware version of TextAlign that is used by TextPainter, then use that in RenderParagraph, RichText, and Text so that you can align start/end as well as left/right. Also ""justify"" needs to define the justification of the last line.",3
,2758,148,78,15274,Look at what the behavior on native is (e.g. is content read out?) and ensure that flutter does the same.,a11y for password text fields,a11y for password text fieldsLook at what the behavior on native is (e.g. is content read out?) and ensure that flutter does the same.,3
,2759,148,79,4944,"Steps to Reproduce

Use Block to render a scrollable page.
The first time top or bottom is reached, observe that the content is reloaded.
Second and subsequent times the overscroll indicator is shown correctly.

Problem does not occur if we manually switch off the overscroll indicator (or switch to the iOS bounce indicator).
Flutter Doctor
[] Flutter (on Linux, channel alpha)
 Framework revision 9c0c022 (3 weeks ago), engine revision bb98655
[] Android toolchain - develop for Android devices (Android SDK 23.0.2)
 Platform android-23, build-tools 23.0.2
 OpenJDK Runtime Environment (build 1.8.0-google-v7-123992248-123972143)",Android overscroll indicator is causing a reload/repaint,"Android overscroll indicator is causing a reload/repaintSteps to Reproduce

Use Block to render a scrollable page.
The first time top or bottom is reached, observe that the content is reloaded.
Second and subsequent times the overscroll indicator is shown correctly.

Problem does not occur if we manually switch off the overscroll indicator (or switch to the iOS bounce indicator).
Flutter Doctor
[] Flutter (on Linux, channel alpha)
 Framework revision 9c0c022 (3 weeks ago), engine revision bb98655
[] Android toolchain - develop for Android devices (Android SDK 23.0.2)
 Platform android-23, build-tools 23.0.2
 OpenJDK Runtime Environment (build 1.8.0-google-v7-123992248-123972143)",3
,2760,148,80,6979,"There's keyboard types for single-line inputs only.
Multiline text inputs need return key.",Support keyboard type on multiline text inputs (Android),"Support keyboard type on multiline text inputs (Android)There's keyboard types for single-line inputs only.
Multiline text inputs need return key.",3
,2761,145,81,15943,"I just tried to upgrade flutter to 0.2.4
The engine upgrade worked but post upgrade flutter doctor reports errors and my app also does not work anymore.
# App execution error::

Launching lib/main.dart on iPhone X in debug mode...
[VERBOSE-2:dart_error.cc(16)] error: Unsupported tag at this point: 0.
[VERBOSE-2:dart_error.cc(16)] Dart_GetClosure expects argument 'library' to be non-null.


# flutter doctor error ::
Running flutter doctor...
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel dev, v0.2.4, on Mac OS X 10.13.3 17D102, locale en-US)

Oops; flutter has exited unexpectedly.
Sending crash report to Google.
Crash report sent (report ID: 2f792ac750627b94)
Unhandled exception:
NoSuchMethodError: The method 'run' was called on null.
Receiver: null
Tried calling: run(Instance(length:2) of '_GrowableList', environment: null, workingDirectory: null)
#0      Object.noSuchMethod (dart:core-patch/dart:core/object_patch.dart:46)
#1      runAsync (package:flutter_tools/src/base/process.dart:227)
<asynchronous suspension>
#2      IOSWorkflow.macDevMode (package:flutter_tools/src/ios/ios_workflow.dart:45)
<asynchronous suspension>
#3      IOSWorkflow.validate (package:flutter_tools/src/ios/ios_workflow.dart:97)
<asynchronous suspension>
#4      Doctor.startValidatorTasks (package:flutter_tools/src/doctor.dart:71)
#5      Doctor.diagnose (package:flutter_tools/src/doctor.dart:128)
<asynchronous suspension>
#6      _doctorText.<anonymous closure> (package:flutter_tools/runner.dart:237)
#7      AppContext._run (package:flutter_tools/src/base/context.dart:76)
<asynchronous suspension>
#8      AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)
#9      _rootRun (dart:async/zone.dart:1126)
#10     _CustomZone.run (dart:async/zone.dart:1023)
#11     runZoned (dart:async/zone.dart:1501)
#12     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)
#13     _doctorText (package:flutter_tools/runner.dart:237)
<asynchronous suspension>
#14     _createLocalCrashReport (package:flutter_tools/runner.dart:212)
<asynchronous suspension>
#15     _handleToolError (package:flutter_tools/runner.dart:167)
<asynchronous suspension>
#16     run.<anonymous closure> (package:flutter_tools/runner.dart:94)
<asynchronous suspension>
#17     AppContext._run (package:flutter_tools/src/base/context.dart:76)
<asynchronous suspension>
#18     AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)
#19     _rootRun (dart:async/zone.dart:1126)
#20     _CustomZone.run (dart:async/zone.dart:1023)
#21     runZoned (dart:async/zone.dart:1501)
#22     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)
#23     run (package:flutter_tools/runner.dart:61)
<asynchronous suspension>
#24     main (package:flutter_tools/executable.dart:48)
<asynchronous suspension>
#25     main (file:///Users/MBahl/flutter/flutter/packages/flutter_tools/bin/flutter_tools.dart:16)
#26     _startIsolate.<anonymous closure> (dart:isolate-patch/dart:isolate/isolate_patch.dart:277)
#27     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:165)",App does not work after flutter upgrade.,"App does not work after flutter upgrade.I just tried to upgrade flutter to 0.2.4
The engine upgrade worked but post upgrade flutter doctor reports errors and my app also does not work anymore.
# App execution error::

Launching lib/main.dart on iPhone X in debug mode...
[VERBOSE-2:dart_error.cc(16)] error: Unsupported tag at this point: 0.
[VERBOSE-2:dart_error.cc(16)] Dart_GetClosure expects argument 'library' to be non-null.


# flutter doctor error ::
Running flutter doctor...
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel dev, v0.2.4, on Mac OS X 10.13.3 17D102, locale en-US)

Oops; flutter has exited unexpectedly.
Sending crash report to Google.
Crash report sent (report ID: 2f792ac750627b94)
Unhandled exception:
NoSuchMethodError: The method 'run' was called on null.
Receiver: null
Tried calling: run(Instance(length:2) of '_GrowableList', environment: null, workingDirectory: null)
#0      Object.noSuchMethod (dart:core-patch/dart:core/object_patch.dart:46)
#1      runAsync (package:flutter_tools/src/base/process.dart:227)
<asynchronous suspension>
#2      IOSWorkflow.macDevMode (package:flutter_tools/src/ios/ios_workflow.dart:45)
<asynchronous suspension>
#3      IOSWorkflow.validate (package:flutter_tools/src/ios/ios_workflow.dart:97)
<asynchronous suspension>
#4      Doctor.startValidatorTasks (package:flutter_tools/src/doctor.dart:71)
#5      Doctor.diagnose (package:flutter_tools/src/doctor.dart:128)
<asynchronous suspension>
#6      _doctorText.<anonymous closure> (package:flutter_tools/runner.dart:237)
#7      AppContext._run (package:flutter_tools/src/base/context.dart:76)
<asynchronous suspension>
#8      AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)
#9      _rootRun (dart:async/zone.dart:1126)
#10     _CustomZone.run (dart:async/zone.dart:1023)
#11     runZoned (dart:async/zone.dart:1501)
#12     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)
#13     _doctorText (package:flutter_tools/runner.dart:237)
<asynchronous suspension>
#14     _createLocalCrashReport (package:flutter_tools/runner.dart:212)
<asynchronous suspension>
#15     _handleToolError (package:flutter_tools/runner.dart:167)
<asynchronous suspension>
#16     run.<anonymous closure> (package:flutter_tools/runner.dart:94)
<asynchronous suspension>
#17     AppContext._run (package:flutter_tools/src/base/context.dart:76)
<asynchronous suspension>
#18     AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)
#19     _rootRun (dart:async/zone.dart:1126)
#20     _CustomZone.run (dart:async/zone.dart:1023)
#21     runZoned (dart:async/zone.dart:1501)
#22     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)
#23     run (package:flutter_tools/runner.dart:61)
<asynchronous suspension>
#24     main (package:flutter_tools/executable.dart:48)
<asynchronous suspension>
#25     main (file:///Users/MBahl/flutter/flutter/packages/flutter_tools/bin/flutter_tools.dart:16)
#26     _startIsolate.<anonymous closure> (dart:isolate-patch/dart:isolate/isolate_patch.dart:277)
#27     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:165)",3
,2762,148,82,1799,Assuming this means the default port.,"`--debug-port 0` on iOS is not ""magical""","`--debug-port 0` on iOS is not ""magical""Assuming this means the default port.",3
,2763,144,83,27124,"There is NullPointerException during executing gradle tasks from the android directory of the project which has dependency to video_player plugin:
java.lang.NullPointerException: (No message provided)Close stacktrace
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)
at com.google.common.base.Splitter.split(Splitter.java:376)
at com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)

Gradle version: 5.1.1
Full gradle scan: https://scans.gradle.com/s/4uiq56gz6bv7u
$ flutter doctor
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel beta, v1.0.0, on Mac OS X 10.14.2 18C54, locale ru-RU)
[] Android toolchain - develop for Android devices (Android SDK 28.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 10.1)
[] Android Studio (version 3.3)
[] IntelliJ IDEA Ultimate Edition (version 2018.3)

Update:
Seems that this happens when two dependencies are together:
  flutter_udid: ^0.0.3
  video_player: ^0.10.0
Steps to reproduce

flutter create test_project
add next dependencies to pubspec.yaml:

flutter_udid: ^0.0.3
video_player: ^0.10.0

cd android
./gradlew tasks",plugins/video_player: java.lang.NullPointerException in gradle,"plugins/video_player: java.lang.NullPointerException in gradleThere is NullPointerException during executing gradle tasks from the android directory of the project which has dependency to video_player plugin:
java.lang.NullPointerException: (No message provided)Close stacktrace
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)
at com.google.common.base.Splitter.split(Splitter.java:376)
at com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)

Gradle version: 5.1.1
Full gradle scan: https://scans.gradle.com/s/4uiq56gz6bv7u
$ flutter doctor
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel beta, v1.0.0, on Mac OS X 10.14.2 18C54, locale ru-RU)
[] Android toolchain - develop for Android devices (Android SDK 28.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 10.1)
[] Android Studio (version 3.3)
[] IntelliJ IDEA Ultimate Edition (version 2018.3)

Update:
Seems that this happens when two dependencies are together:
  flutter_udid: ^0.0.3
  video_player: ^0.10.0
Steps to reproduce

flutter create test_project
add next dependencies to pubspec.yaml:

flutter_udid: ^0.0.3
video_player: ^0.10.0

cd android
./gradlew tasks",3
,2764,148,84,19806,"Hi,
since the plugin is not using the camera, just requesting the intent, the CAMERA permission is not needed. Here the tutorial on how to use the camera intent from the official docs: https://developer.android.com/training/camera/photobasics
Thanks!",[image_picker] Camera permission not needed,"[image_picker] Camera permission not neededHi,
since the plugin is not using the camera, just requesting the intent, the CAMERA permission is not needed. Here the tutorial on how to use the camera intent from the official docs: https://developer.android.com/training/camera/photobasics
Thanks!",3
,2765,148,85,31645,"Experiment with build_runner and modular kernel as a pre-compilation step (swappable with the current frontend_server approach).
Open questions:


How do compilation strategies compare on the current benchmarks for small apps? For large ones? Namely: 1st run, repeated run, repeated run with changes, et cetera. See the current test benchmarks.


Does build_runner give us a better path forward for test integration into other tools?


Does this allow us to better unify web and native tests?


Can we ship the build_rules in the engine with the current build_runner integration strategy?",Teach flutter test to build_runner,"Teach flutter test to build_runnerExperiment with build_runner and modular kernel as a pre-compilation step (swappable with the current frontend_server approach).
Open questions:


How do compilation strategies compare on the current benchmarks for small apps? For large ones? Namely: 1st run, repeated run, repeated run with changes, et cetera. See the current test benchmarks.


Does build_runner give us a better path forward for test integration into other tools?


Does this allow us to better unify web and native tests?


Can we ship the build_rules in the engine with the current build_runner integration strategy?",3
,2766,145,86,17902,"apply plugin: ""com.android.application""

import com.android.build.OutputFile

def enableProguardInReleaseBuilds = false

android {
    compileSdkVersion 27
    buildToolsVersion '27.0.3'

    defaultConfig {
        applicationId ""com.destressambulance""
        minSdkVersion 16
        targetSdkVersion 27
        versionCode 1
        versionName ""1.0""
        ndk {
            abiFilters ""armeabi-v7a"", ""x86""
        }
    }
    splits {
        abi {
            reset()
            enable enableSeparateBuildPerCPUArchitecture
            universalApk false  // If true, also generate a universal APK
            include ""armeabi-v7a"", ""x86""
        }
    }
    buildTypes {
        release {
            minifyEnabled enableProguardInReleaseBuilds
            proguardFiles getDefaultProguardFile(""proguard-android.txt""), ""proguard-rules.pro""
        }
    }
    // applicationVariants are e.g. debug, release
    applicationVariants.all { variant ->
        variant.outputs.each { output ->
            // For each separate APK per architecture, set a unique version code as described here:
            // http://tools.android.com/tech-docs/new-build-system/user-guide/apk-splits
            def versionCodes = [""armeabi-v7a"":1, ""x86"":2]
            def abi = output.getFilter(OutputFile.ABI)
            if (abi != null) {  // null for the universal-debug, universal-release variants
                output.versionCodeOverride =
                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode
            }
        }
    }
}

dependencies {
    implementation project(':react-native-fcm')
    implementation project(':react-native-customized-image-picker')
    implementation project(':react-native-google-places')
    implementation project(':react-native-image-picker')
    implementation fileTree(dir: ""libs"", include: [""*.jar""])
    //noinspection GradleCompatible
    implementation ""com.android.support:appcompat-v7:27.1.1""
    implementation ""com.facebook.react:react-native:0.20.1""  // From node_modules
}

// Run this once to be able to run the application with BUCK
// puts all compile dependencies into folder libs for BUCK to use
task copyDownloadableDepsToLibs(type: Copy) {
    from configurations.compile
    into 'libs'
}",Android dependency 'com.android.support:recyclerview-v7' has different version for the compile (23.0.1) and runtime (27.1.1) classpath. You should manually set the same version via DependencyResolution,"Android dependency 'com.android.support:recyclerview-v7' has different version for the compile (23.0.1) and runtime (27.1.1) classpath. You should manually set the same version via DependencyResolutionapply plugin: ""com.android.application""

import com.android.build.OutputFile

def enableProguardInReleaseBuilds = false

android {
    compileSdkVersion 27
    buildToolsVersion '27.0.3'

    defaultConfig {
        applicationId ""com.destressambulance""
        minSdkVersion 16
        targetSdkVersion 27
        versionCode 1
        versionName ""1.0""
        ndk {
            abiFilters ""armeabi-v7a"", ""x86""
        }
    }
    splits {
        abi {
            reset()
            enable enableSeparateBuildPerCPUArchitecture
            universalApk false  // If true, also generate a universal APK
            include ""armeabi-v7a"", ""x86""
        }
    }
    buildTypes {
        release {
            minifyEnabled enableProguardInReleaseBuilds
            proguardFiles getDefaultProguardFile(""proguard-android.txt""), ""proguard-rules.pro""
        }
    }
    // applicationVariants are e.g. debug, release
    applicationVariants.all { variant ->
        variant.outputs.each { output ->
            // For each separate APK per architecture, set a unique version code as described here:
            // http://tools.android.com/tech-docs/new-build-system/user-guide/apk-splits
            def versionCodes = [""armeabi-v7a"":1, ""x86"":2]
            def abi = output.getFilter(OutputFile.ABI)
            if (abi != null) {  // null for the universal-debug, universal-release variants
                output.versionCodeOverride =
                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode
            }
        }
    }
}

dependencies {
    implementation project(':react-native-fcm')
    implementation project(':react-native-customized-image-picker')
    implementation project(':react-native-google-places')
    implementation project(':react-native-image-picker')
    implementation fileTree(dir: ""libs"", include: [""*.jar""])
    //noinspection GradleCompatible
    implementation ""com.android.support:appcompat-v7:27.1.1""
    implementation ""com.facebook.react:react-native:0.20.1""  // From node_modules
}

// Run this once to be able to run the application with BUCK
// puts all compile dependencies into folder libs for BUCK to use
task copyDownloadableDepsToLibs(type: Copy) {
    from configurations.compile
    into 'libs'
}",3
,2767,148,87,17161,"Would be nice in places like https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/widgets/framework.dart#L3627 and https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/rendering/object.dart#L122 to also point out where the widget was created.
cc @jacob314 I think we chatted about this but I forgot where.",Attach widget creation source in build/paint profile timeline events,"Attach widget creation source in build/paint profile timeline eventsWould be nice in places like https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/widgets/framework.dart#L3627 and https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/rendering/object.dart#L122 to also point out where the widget was created.
cc @jacob314 I think we chatted about this but I forgot where.",3
,2768,145,88,7817,"Tracks a simple battery plugin. We will write this as a first-party plugin to serve as a sample of how to write a plugin.
It will integrate with to BatteryManager on Android, and UIDevice on iOS",Plugin: Battery,"Plugin: BatteryTracks a simple battery plugin. We will write this as a first-party plugin to serve as a sample of how to write a plugin.
It will integrate with to BatteryManager on Android, and UIDevice on iOS",3
,2769,144,89,27931,"I'm suffering horrible error of cloud firestore in whole day.... I don't know what is the problem, I asked for stackoverflow, but there was no answer, how to deal with???
Future<String> test() async {
  CollectionReference col = Firestore.instance.collection('messages');
  QuerySnapshot querySnapshot = await col.where('begin',isEqualTo: false).getDocuments();
}
What is wrong with my code??
Error
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): Failed to handle method call
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): java.lang.IllegalArgumentException: Invalid document reference. Document references must have an even number of segments, but messages has 1
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.google.firebase.firestore.DocumentReference.zza(com.google.firebase:firebase-firestore@@17.1.1:81)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.google.firebase.firestore.FirebaseFirestore.document(com.google.firebase:firebase-firestore@@17.1.1:259)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.getDocumentReference(CloudFirestorePlugin.java:92)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.onMethodCall(CloudFirestorePlugin.java:474)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:200)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.view.FlutterNativeView$PlatformMessageHandlerImpl.handlePlatformMessage(FlutterNativeView.java:188)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:202)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.MessageQueue.nativePollOnce(Native Method)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.MessageQueue.next(MessageQueue.java:325)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.Looper.loop(Looper.java:142)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.app.ActivityThread.main(ActivityThread.java:6938)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at java.lang.reflect.Method.invoke(Native Method)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)",Document references must have an even number of segments,"Document references must have an even number of segmentsI'm suffering horrible error of cloud firestore in whole day.... I don't know what is the problem, I asked for stackoverflow, but there was no answer, how to deal with???
Future<String> test() async {
  CollectionReference col = Firestore.instance.collection('messages');
  QuerySnapshot querySnapshot = await col.where('begin',isEqualTo: false).getDocuments();
}
What is wrong with my code??
Error
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): Failed to handle method call
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): java.lang.IllegalArgumentException: Invalid document reference. Document references must have an even number of segments, but messages has 1
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.google.firebase.firestore.DocumentReference.zza(com.google.firebase:firebase-firestore@@17.1.1:81)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.google.firebase.firestore.FirebaseFirestore.document(com.google.firebase:firebase-firestore@@17.1.1:259)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.getDocumentReference(CloudFirestorePlugin.java:92)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.onMethodCall(CloudFirestorePlugin.java:474)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:200)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.view.FlutterNativeView$PlatformMessageHandlerImpl.handlePlatformMessage(FlutterNativeView.java:188)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:202)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.MessageQueue.nativePollOnce(Native Method)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.MessageQueue.next(MessageQueue.java:325)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.os.Looper.loop(Looper.java:142)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at android.app.ActivityThread.main(ActivityThread.java:6938)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at java.lang.reflect.Method.invoke(Native Method)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)
E/MethodChannel#plugins.flutter.io/cloud_firestore(12549): 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)",3
,2770,145,90,604,"Sorry if this is sparse on details. I mainly want to determine if there is a bug with showDrawer or with the way I attempted to use it.
Yesterday, I attempted to use showDrawer with the 7be58b1 alpha branch of Flutter. The drawer appeared/dismissed correctly, and the widgets I put in the drawer were correct.
Unfortunately, I noticed that no matter how I attempted to change the state of what was drawn in the drawer, it would not re-render. In my case, I tried to put a Switch inside a DrawerItem to toggle debug mode for my app. I did confirm that the state did actually change, which led me to think that Flutter has a bug.
I was following the Stocks demo app quite closely, which uses a Drawer to toggle the user's market sentiment (Optimistic vs Pessimistic). However, I was assuming that the app's behavior would match the Sky Demo app in the Google Play Store. I only just realized that this app is actually 3 months old (last update August 25), so it is very possible that in that time, the Stocks example does not function as it did before.
I am currently unable to try running these examples (though I have found the instructions), so if someone else can run the Stocks app and determine whether the Optimistic/Pessimistic radio buttons update the drawer menu or not, that would be very helpful.
Otherwise, I will look into this more after the break.",setState might not update the widgets in showDrawer,"setState might not update the widgets in showDrawerSorry if this is sparse on details. I mainly want to determine if there is a bug with showDrawer or with the way I attempted to use it.
Yesterday, I attempted to use showDrawer with the 7be58b1 alpha branch of Flutter. The drawer appeared/dismissed correctly, and the widgets I put in the drawer were correct.
Unfortunately, I noticed that no matter how I attempted to change the state of what was drawn in the drawer, it would not re-render. In my case, I tried to put a Switch inside a DrawerItem to toggle debug mode for my app. I did confirm that the state did actually change, which led me to think that Flutter has a bug.
I was following the Stocks demo app quite closely, which uses a Drawer to toggle the user's market sentiment (Optimistic vs Pessimistic). However, I was assuming that the app's behavior would match the Sky Demo app in the Google Play Store. I only just realized that this app is actually 3 months old (last update August 25), so it is very possible that in that time, the Stocks example does not function as it did before.
I am currently unable to try running these examples (though I have found the instructions), so if someone else can run the Stocks app and determine whether the Optimistic/Pessimistic radio buttons update the drawer menu or not, that would be very helpful.
Otherwise, I will look into this more after the break.",3
,2771,146,91,6728,"Tried setting up a flutter app on my personal linux laptop, flutter run failed to start the app on this Android 24 emulator:

Changing the architecture to use Google APIs resolved this issue, but it was unclear from the errors I got that that was an appropriate resolution.  Flutter run provided no indication of the error, and I get the following cryptic output from ADB logcat:
11-06 20:45:43.000  1246  1246 W art     : Unexpected CPU variant for X86 using defaults: x86
Full ADB output:
http://pastebin.com/CEghLXgj
Note: this is unrelated to leafy, I'm just trying out the third party workflow.",Installation fails on Android emulators without Google APIs without a clear indication of the cause,"Installation fails on Android emulators without Google APIs without a clear indication of the causeTried setting up a flutter app on my personal linux laptop, flutter run failed to start the app on this Android 24 emulator:

Changing the architecture to use Google APIs resolved this issue, but it was unclear from the errors I got that that was an appropriate resolution.  Flutter run provided no indication of the error, and I get the following cryptic output from ADB logcat:
11-06 20:45:43.000  1246  1246 W art     : Unexpected CPU variant for X86 using defaults: x86
Full ADB output:
http://pastebin.com/CEghLXgj
Note: this is unrelated to leafy, I'm just trying out the third party workflow.",3
,2772,144,92,16440,"Steps to Reproduce
I accidentally left the ""new"" keyword out of a line of code while working on an app, and when I rebuilt it, the compiler crashed. I then created a brand new project in IntelliJ, and verified the issue still occurred.
Steps to reproduce:


Create a new IntelliJ Flutter project


IntelliJ spits out the ""increment counter"" app.


Remove the new keyword from line 67:
 66: // than having to individually change instances of widgets.
 67: return new Scaffold(
 68:   appBar: new AppBar(



Trigger a hot reload.


The compiler crashes with the message included below.


Interestingly, the compiler does not crash if you leave line 67 alone and instead remove the ""new"" keyword from line 68. Line 67 is creating the instance that's used as the return value of the method, if that matters.
Logs
Performing hot reload...
compiler message: Unhandled exception:
compiler message: Crash when compiling file:///Users/redbrogdon/source/crashtest/lib/main.dart,
compiler message: at character offset 2160:
compiler message: lib/main.dart: Internal problem: Unhandled this in defaultTreeNode.
compiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)
compiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)
compiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)
compiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)
compiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)
compiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)
compiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)
compiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)
compiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)
compiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)
compiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)
compiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)
compiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)
compiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)
compiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)
compiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)
compiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)
compiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)
compiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)
compiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)
compiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)
compiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)
compiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)
compiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)
compiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)
compiler message: <asynchronous suspension>
compiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)
compiler message: <asynchronous suspension>
compiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)
compiler message: <asynchronous suspension>
compiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)
compiler message: <asynchronous suspension>
compiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #33     _rootRun (dart:async/zone.dart:1126)
compiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)
compiler message: #35     runZoned (dart:async/zone.dart:1501)
compiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)
compiler message: <asynchronous suspension>
compiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)
compiler message: <asynchronous suspension>
compiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)
compiler message: <asynchronous suspension>
compiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)
compiler message: <asynchronous suspension>
compiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)
compiler message: <asynchronous suspension>
compiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)
compiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)
compiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)
compiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)
compiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)
compiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)
compiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)
compiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)
compiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)
compiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)
compiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)
compiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)
compiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)
compiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)
compiler message: 
compiler message: 
compiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)
compiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)
compiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)
compiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)
compiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)
compiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)
compiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)
compiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)
compiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)
compiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)
compiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)
compiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)
compiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)
compiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)
compiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)
compiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)
compiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)
compiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)
compiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)
compiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)
compiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)
compiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)
compiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)
compiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)
compiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)
compiler message: <asynchronous suspension>
compiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)
compiler message: <asynchronous suspension>
compiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)
compiler message: <asynchronous suspension>
compiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)
compiler message: <asynchronous suspension>
compiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #33     _rootRun (dart:async/zone.dart:1126)
compiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)
compiler message: #35     runZoned (dart:async/zone.dart:1501)
compiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)
compiler message: <asynchronous suspension>
compiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)
compiler message: <asynchronous suspension>
compiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)
compiler message: <asynchronous suspension>
compiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)
compiler message: <asynchronous suspension>
compiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)
compiler message: <asynchronous suspension>
compiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)
compiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)
compiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)
compiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)
compiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)
compiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)
compiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)
compiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)
compiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)
compiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)
compiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)
compiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)
compiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)
compiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)
Reloaded 1 of 385 libraries in 902ms.

Flutter Doctor
$ flutter doctor
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel beta, v0.2.8, on Mac OS X 10.13.3 17D102, locale en-US)
[] Android toolchain - develop for Android devices (Android SDK 26.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 9.2)
[] Android Studio (version 3.0)
[] IntelliJ IDEA Community Edition (version 2018.1)
[] VS Code (version 1.20.1)
[] Connected devices (1 available)

 No issues found!","Compiler crash when ""new"" keyword removed from default app","Compiler crash when ""new"" keyword removed from default appSteps to Reproduce
I accidentally left the ""new"" keyword out of a line of code while working on an app, and when I rebuilt it, the compiler crashed. I then created a brand new project in IntelliJ, and verified the issue still occurred.
Steps to reproduce:


Create a new IntelliJ Flutter project


IntelliJ spits out the ""increment counter"" app.


Remove the new keyword from line 67:
 66: // than having to individually change instances of widgets.
 67: return new Scaffold(
 68:   appBar: new AppBar(



Trigger a hot reload.


The compiler crashes with the message included below.


Interestingly, the compiler does not crash if you leave line 67 alone and instead remove the ""new"" keyword from line 68. Line 67 is creating the instance that's used as the return value of the method, if that matters.
Logs
Performing hot reload...
compiler message: Unhandled exception:
compiler message: Crash when compiling file:///Users/redbrogdon/source/crashtest/lib/main.dart,
compiler message: at character offset 2160:
compiler message: lib/main.dart: Internal problem: Unhandled this in defaultTreeNode.
compiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)
compiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)
compiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)
compiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)
compiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)
compiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)
compiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)
compiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)
compiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)
compiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)
compiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)
compiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)
compiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)
compiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)
compiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)
compiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)
compiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)
compiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)
compiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)
compiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)
compiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)
compiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)
compiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)
compiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)
compiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)
compiler message: <asynchronous suspension>
compiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)
compiler message: <asynchronous suspension>
compiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)
compiler message: <asynchronous suspension>
compiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)
compiler message: <asynchronous suspension>
compiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #33     _rootRun (dart:async/zone.dart:1126)
compiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)
compiler message: #35     runZoned (dart:async/zone.dart:1501)
compiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)
compiler message: <asynchronous suspension>
compiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)
compiler message: <asynchronous suspension>
compiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)
compiler message: <asynchronous suspension>
compiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)
compiler message: <asynchronous suspension>
compiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)
compiler message: <asynchronous suspension>
compiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)
compiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)
compiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)
compiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)
compiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)
compiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)
compiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)
compiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)
compiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)
compiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)
compiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)
compiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)
compiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)
compiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)
compiler message: 
compiler message: 
compiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)
compiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)
compiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)
compiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)
compiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)
compiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)
compiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)
compiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)
compiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)
compiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)
compiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)
compiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)
compiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)
compiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)
compiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)
compiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)
compiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)
compiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)
compiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)
compiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)
compiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)
compiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)
compiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)
compiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)
compiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)
compiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)
compiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)
compiler message: <asynchronous suspension>
compiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)
compiler message: <asynchronous suspension>
compiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)
compiler message: <asynchronous suspension>
compiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)
compiler message: <asynchronous suspension>
compiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #33     _rootRun (dart:async/zone.dart:1126)
compiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)
compiler message: #35     runZoned (dart:async/zone.dart:1501)
compiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)
compiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)
compiler message: <asynchronous suspension>
compiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)
compiler message: <asynchronous suspension>
compiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)
compiler message: <asynchronous suspension>
compiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)
compiler message: <asynchronous suspension>
compiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)
compiler message: <asynchronous suspension>
compiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)
compiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)
compiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)
compiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)
compiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)
compiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)
compiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)
compiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)
compiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)
compiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)
compiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)
compiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)
compiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)
compiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)
compiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)
compiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)
compiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)
compiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)
compiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)
compiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)
compiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)
compiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)
compiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)
compiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)
Reloaded 1 of 385 libraries in 902ms.

Flutter Doctor
$ flutter doctor
Doctor summary (to see all details, run flutter doctor -v):
[] Flutter (Channel beta, v0.2.8, on Mac OS X 10.13.3 17D102, locale en-US)
[] Android toolchain - develop for Android devices (Android SDK 26.0.3)
[] iOS toolchain - develop for iOS devices (Xcode 9.2)
[] Android Studio (version 3.0)
[] IntelliJ IDEA Community Edition (version 2018.1)
[] VS Code (version 1.20.1)
[] Connected devices (1 available)

 No issues found!",3
,2773,148,93,32866,,Let add-to-app FlutterViewControllers be back swipable in a UINavigationController,Let add-to-app FlutterViewControllers be back swipable in a UINavigationController,3
,2774,148,94,7306,"Links to FlutterViewController.h are broken for example.
A more sustainable fix is probably some sort of automated way to check these as part of publishing. :(",Broken links on website,"Broken links on websiteLinks to FlutterViewController.h are broken for example.
A more sustainable fix is probably some sort of automated way to check these as part of publishing. :(",3
,2775,148,95,16228,"#16035 introduced a bug with Android sdkmanager version 26.1.1 because the --add-modules option isn't supported:
$ SDKMANAGER_OPTS=""--add-modules java.se.ee"" /path/to/android/sdk/tools/bin/sdkmanager --licenses
Unrecognized option: --add-modules
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit

The manifestation was that when the user ran flutter doctor -v, they'd see the following:
[!] Android toolchain - develop for Android devices (Android SDK 27.0.3)
    ...
     Android license status unknown.",Regression in Android license status detection.,"Regression in Android license status detection.#16035 introduced a bug with Android sdkmanager version 26.1.1 because the --add-modules option isn't supported:
$ SDKMANAGER_OPTS=""--add-modules java.se.ee"" /path/to/android/sdk/tools/bin/sdkmanager --licenses
Unrecognized option: --add-modules
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit

The manifestation was that when the user ran flutter doctor -v, they'd see the following:
[!] Android toolchain - develop for Android devices (Android SDK 27.0.3)
    ...
     Android license status unknown.",3
,2776,148,96,31123,"Steps to Reproduce
I have an application with Flutter. It works really fine, but the app crashes on API level 18 and 19.
When I try to run test the apk in Test Lab, it show me error on API level 18 and 19
Logs
 Fatal exception
	at com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)
FATAL EXCEPTION: main
java.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp
	at com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)
	at android.content.ContentProvider.attachInfo(ContentProvider.java:1214)
	at android.content.ContentProvider.attachInfo(ContentProvider.java:1189)
	at com.google.firebase.provider.FirebaseInitProvider.attachInfo(com.google.firebase:firebase-common@@16.0.2:47)
	at android.app.ActivityThread.installProvider(ActivityThread.java:5119)
	at android.app.ActivityThread.installContentProviders(ActivityThread.java:4725)
	at android.app.ActivityThread.handleBindApplication(ActivityThread.java:4665)
	at android.app.ActivityThread.access$1400(ActivityThread.java:159)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1376)
	at android.os.Handler.dispatchMessage(Handler.java:99)
	at android.os.Looper.loop(Looper.java:176)
	at android.app.ActivityThread.main(ActivityThread.java:5419)
	at java.lang.reflect.Method.invokeNative(Native Method)
	at java.lang.reflect.Method.invoke(Method.java:525)
	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1046)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:862)
	at dalvik.system.NativeStart.main(Native Method)

gradle.build
def localProperties = new Properties()
def localPropertiesFile = rootProject.file('local.properties')
if (localPropertiesFile.exists()) {
    localPropertiesFile.withReader('UTF-8') { reader ->
        localProperties.load(reader)
    }
}

def flutterRoot = localProperties.getProperty('flutter.sdk')
if (flutterRoot == null) {
    throw new GradleException(""Flutter SDK not found. Define location with flutter.sdk in the local.properties file."")
}

def flutterVersionCode = localProperties.getProperty('flutter.versionCode')
if (flutterVersionCode == null) {
    flutterVersionCode = '1'
}

def flutterVersionName = localProperties.getProperty('flutter.versionName')
if (flutterVersionName == null) {
    flutterVersionName = '1.0'
}

apply plugin: 'com.android.application'
apply from: ""$flutterRoot/packages/flutter_tools/gradle/flutter.gradle""

repositories {
    mavenCentral()
    maven { url 'https://mapbox.bintray.com/mapbox' }
}

def keystoreProperties = new Properties()
def keystorePropertiesFile = rootProject.file('key.properties')
if (keystorePropertiesFile.exists()) {
    keystoreProperties.load(new FileInputStream(keystorePropertiesFile))
}

android {
    compileSdkVersion 28
    lintOptions {
        disable 'InvalidPackage'
    }
    defaultConfig {
        // TODO: Specify your own unique Application ID (https://developer.android.com/studio/build/application-id.html).
        applicationId ""com.tripmate.myapp""
        minSdkVersion 16
        targetSdkVersion 27
        versionCode flutterVersionCode.toInteger()
        versionName flutterVersionName
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
        multiDexEnabled true
        ndk {
        abiFilters 'armeabi-v7a'
        }

    }
    signingConfigs {
        release {
            keyAlias keystoreProperties['keyAlias']
            keyPassword keystoreProperties['keyPassword']
            storeFile file(keystoreProperties['storeFile'])
            storePassword keystoreProperties['storePassword']
        }
    }
    buildTypes {
        release {
            signingConfig signingConfigs.release
        }
    }
    buildToolsVersion '28.0.3'
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_1_8
        targetCompatibility JavaVersion.VERSION_1_8
    }
}

flutter {
    source '../..'
}

dependencies {
    implementation 'android.arch.core:runtime:1.1.1'
    implementation 'com.android.support:multidex:1.0.0'
    implementation 'com.android.support.constraint:constraint-layout:1.1.3'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
    implementation 'com.google.firebase:firebase-core:16.0.7'
    //Maps Box Maps SDK
    implementation 'com.mapbox.mapboxsdk:mapbox-android-sdk:7.1.2'
    //Permission management API
    implementation 'com.google.android.gms:play-services-maps:16.1.0'
    //Google play services
    implementation 'com.google.android.gms:play-services-location:16.0.0'
    //Bubble Layout
    implementation 'com.daasuu:BubbleLayout:1.2.0'
    //Mapbox navigation
    implementation 'com.mapbox.mapboxsdk:mapbox-android-navigation-ui:0.30.0'
    //Mapbox Annotation Plugin
    implementation 'com.mapbox.mapboxsdk:mapbox-android-plugin-annotation-v7:0.5.0'
    implementation 'com.android.support:design:1.0.0'

}

configurations.all {
    resolutionStrategy {
        force 'com.android.support:support-v4:27.1.0'
    }
}

apply plugin: 'com.google.gms.google-services'
apply plugin: 'io.fabric'


Can anyone help me with this?",java.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp,"java.lang.NoClassDefFoundError: com.google.firebase.FirebaseAppSteps to Reproduce
I have an application with Flutter. It works really fine, but the app crashes on API level 18 and 19.
When I try to run test the apk in Test Lab, it show me error on API level 18 and 19
Logs
 Fatal exception
	at com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)
FATAL EXCEPTION: main
java.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp
	at com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)
	at android.content.ContentProvider.attachInfo(ContentProvider.java:1214)
	at android.content.ContentProvider.attachInfo(ContentProvider.java:1189)
	at com.google.firebase.provider.FirebaseInitProvider.attachInfo(com.google.firebase:firebase-common@@16.0.2:47)
	at android.app.ActivityThread.installProvider(ActivityThread.java:5119)
	at android.app.ActivityThread.installContentProviders(ActivityThread.java:4725)
	at android.app.ActivityThread.handleBindApplication(ActivityThread.java:4665)
	at android.app.ActivityThread.access$1400(ActivityThread.java:159)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1376)
	at android.os.Handler.dispatchMessage(Handler.java:99)
	at android.os.Looper.loop(Looper.java:176)
	at android.app.ActivityThread.main(ActivityThread.java:5419)
	at java.lang.reflect.Method.invokeNative(Native Method)
	at java.lang.reflect.Method.invoke(Method.java:525)
	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1046)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:862)
	at dalvik.system.NativeStart.main(Native Method)

gradle.build
def localProperties = new Properties()
def localPropertiesFile = rootProject.file('local.properties')
if (localPropertiesFile.exists()) {
    localPropertiesFile.withReader('UTF-8') { reader ->
        localProperties.load(reader)
    }
}

def flutterRoot = localProperties.getProperty('flutter.sdk')
if (flutterRoot == null) {
    throw new GradleException(""Flutter SDK not found. Define location with flutter.sdk in the local.properties file."")
}

def flutterVersionCode = localProperties.getProperty('flutter.versionCode')
if (flutterVersionCode == null) {
    flutterVersionCode = '1'
}

def flutterVersionName = localProperties.getProperty('flutter.versionName')
if (flutterVersionName == null) {
    flutterVersionName = '1.0'
}

apply plugin: 'com.android.application'
apply from: ""$flutterRoot/packages/flutter_tools/gradle/flutter.gradle""

repositories {
    mavenCentral()
    maven { url 'https://mapbox.bintray.com/mapbox' }
}

def keystoreProperties = new Properties()
def keystorePropertiesFile = rootProject.file('key.properties')
if (keystorePropertiesFile.exists()) {
    keystoreProperties.load(new FileInputStream(keystorePropertiesFile))
}

android {
    compileSdkVersion 28
    lintOptions {
        disable 'InvalidPackage'
    }
    defaultConfig {
        // TODO: Specify your own unique Application ID (https://developer.android.com/studio/build/application-id.html).
        applicationId ""com.tripmate.myapp""
        minSdkVersion 16
        targetSdkVersion 27
        versionCode flutterVersionCode.toInteger()
        versionName flutterVersionName
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
        multiDexEnabled true
        ndk {
        abiFilters 'armeabi-v7a'
        }

    }
    signingConfigs {
        release {
            keyAlias keystoreProperties['keyAlias']
            keyPassword keystoreProperties['keyPassword']
            storeFile file(keystoreProperties['storeFile'])
            storePassword keystoreProperties['storePassword']
        }
    }
    buildTypes {
        release {
            signingConfig signingConfigs.release
        }
    }
    buildToolsVersion '28.0.3'
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_1_8
        targetCompatibility JavaVersion.VERSION_1_8
    }
}

flutter {
    source '../..'
}

dependencies {
    implementation 'android.arch.core:runtime:1.1.1'
    implementation 'com.android.support:multidex:1.0.0'
    implementation 'com.android.support.constraint:constraint-layout:1.1.3'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
    implementation 'com.google.firebase:firebase-core:16.0.7'
    //Maps Box Maps SDK
    implementation 'com.mapbox.mapboxsdk:mapbox-android-sdk:7.1.2'
    //Permission management API
    implementation 'com.google.android.gms:play-services-maps:16.1.0'
    //Google play services
    implementation 'com.google.android.gms:play-services-location:16.0.0'
    //Bubble Layout
    implementation 'com.daasuu:BubbleLayout:1.2.0'
    //Mapbox navigation
    implementation 'com.mapbox.mapboxsdk:mapbox-android-navigation-ui:0.30.0'
    //Mapbox Annotation Plugin
    implementation 'com.mapbox.mapboxsdk:mapbox-android-plugin-annotation-v7:0.5.0'
    implementation 'com.android.support:design:1.0.0'

}

configurations.all {
    resolutionStrategy {
        force 'com.android.support:support-v4:27.1.0'
    }
}

apply plugin: 'com.google.gms.google-services'
apply plugin: 'io.fabric'


Can anyone help me with this?",3
,2777,145,97,30185,"Tried downgrading the version but didn't work. I am using the latest Xcode and IOS version.
Help @shihaohong @Hixie @jonahwilliams @pepegich",Cloud Firestore 0.9.7 stucks at Installing BoringSSL-GRPC (0.0.2) during pod install,"Cloud Firestore 0.9.7 stucks at Installing BoringSSL-GRPC (0.0.2) during pod installTried downgrading the version but didn't work. I am using the latest Xcode and IOS version.
Help @shihaohong @Hixie @jonahwilliams @pepegich",3
,2778,148,98,13961,"Steps to Reproduce

Run examples/flutter_gallery on iOS (physical device or simulator)
Open ""Text fields"" under ""Material Components""
Enter one or more numbers into the ""Phone Number"" field
Hit backspace
The input field goes into a loop where it repeatedly removes and adds the last character.


Logs
[        ]   To hot reload your app on the fly, press ""r"". To restart the app entirely, press ""R"".
[        ] An Observatory debugger and profiler on iPhone X is available at: http://127.0.0.1:8103/
[        ] For a more detailed help message, press ""h"". To quit, press ""q"".
[+253478 ms] [DEVICE LOG] 2018-01-06 15:52:03.678301+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4633, Description: Loading Preferences From System CFPrefsD For Search List
[+13924 ms] [DEVICE LOG] 2018-01-06 15:52:17.602866+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4634, Description: Loading Preferences From System CFPrefsD For Search List
[   +8 ms] [DEVICE LOG] 2018-01-06 15:52:17.611380+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4635, Description: Loading Preferences From System CFPrefsD For Search List
[ +234 ms] [DEVICE LOG] 2018-01-06 15:52:17.845901+0200  localhost Runner[24134]: (UIKit) Can't find keyplane that supports type 5 for keyboard iPhone-PortraitChoco-PhonePad; using 2024220450015396792_PortraitChoco_iPhone-Complex-Pad_Default
[  +63 ms] [DEVICE LOG] 2018-01-06 15:52:17.900540+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4636, Description: Loading Preferences From System CFPrefsD For Search List
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901476+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4637, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901791+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4638, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901973+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4639, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902255+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463a, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902480+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463b, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902748+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463c, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.903001+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463d, Description: Updating Key-Value Observers Of Preferences
[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.903195+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463e, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.903453+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463f, Description: Updating Key-Value Observers Of Preferences
[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.906445+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc58b0, Description: Loading Preferences From System CFPrefsD For Search List
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907627+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 0 -> 1; styleActivationCount: 0 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907921+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 0 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907992+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activating engine
[        ] [DEVICE LOG] 2018-01-06 15:52:17.908409+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=0, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Inactive -> Activating
[        ] [DEVICE LOG] 2018-01-06 15:52:17.910005+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=3, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Activating -> Running
[+3359 ms] [DEVICE LOG] 2018-01-06 15:52:21.273835+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:21.273875+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:21.274731+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:21.274782+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1
[+2656 ms] [DEVICE LOG] 2018-01-06 15:52:23.928718+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:23.928799+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[   +6 ms] [DEVICE LOG] 2018-01-06 15:52:23.929589+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:23.929682+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1
[+3667 ms] [DEVICE LOG] 2018-01-06 15:52:27.608269+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:27.608319+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:27.609012+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:27.609054+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1

Flutter Doctor
[] Flutter (on Mac OS X 10.13.2 17C88, locale en-SE, channel master)
     Flutter at /Users/fsimon/flutter
     Framework revision a414fb458d (28 hours ago), 2018-01-06 12:02:33 +0100
     Engine revision 12e0e38a8b
     Tools Dart version 1.25.0-dev.11.0
     Engine Dart version 2.0.0-edge.8d7219a5b6a7c2505ff57f23e7cf80da4c724512

[] Android toolchain - develop for Android devices (Android SDK 26.0.3)
     Android SDK at /Users/fsimon/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-26, build-tools 26.0.3
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] iOS toolchain - develop for iOS devices (Xcode 9.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.2, Build version 9C40b
     ios-deploy 1.9.2
     CocoaPods version 1.3.1

[] Android Studio (version 3.0)
     Android Studio at /Applications/Android Studio.app/Contents
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] Connected devices
     iPhone X                   1C6CECD0-0E72-489F-90D7-197A80D82B98  ios          iOS 11.2 (simulator)",Infinite loop in TextField with some inputFormatters (iOS),"Infinite loop in TextField with some inputFormatters (iOS)Steps to Reproduce

Run examples/flutter_gallery on iOS (physical device or simulator)
Open ""Text fields"" under ""Material Components""
Enter one or more numbers into the ""Phone Number"" field
Hit backspace
The input field goes into a loop where it repeatedly removes and adds the last character.


Logs
[        ]   To hot reload your app on the fly, press ""r"". To restart the app entirely, press ""R"".
[        ] An Observatory debugger and profiler on iPhone X is available at: http://127.0.0.1:8103/
[        ] For a more detailed help message, press ""h"". To quit, press ""q"".
[+253478 ms] [DEVICE LOG] 2018-01-06 15:52:03.678301+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4633, Description: Loading Preferences From System CFPrefsD For Search List
[+13924 ms] [DEVICE LOG] 2018-01-06 15:52:17.602866+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4634, Description: Loading Preferences From System CFPrefsD For Search List
[   +8 ms] [DEVICE LOG] 2018-01-06 15:52:17.611380+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4635, Description: Loading Preferences From System CFPrefsD For Search List
[ +234 ms] [DEVICE LOG] 2018-01-06 15:52:17.845901+0200  localhost Runner[24134]: (UIKit) Can't find keyplane that supports type 5 for keyboard iPhone-PortraitChoco-PhonePad; using 2024220450015396792_PortraitChoco_iPhone-Complex-Pad_Default
[  +63 ms] [DEVICE LOG] 2018-01-06 15:52:17.900540+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4636, Description: Loading Preferences From System CFPrefsD For Search List
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901476+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4637, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901791+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4638, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.901973+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4639, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902255+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463a, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902480+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463b, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.902748+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463c, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.903001+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463d, Description: Updating Key-Value Observers Of Preferences
[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.903195+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463e, Description: Updating Key-Value Observers Of Preferences
[        ] [DEVICE LOG] 2018-01-06 15:52:17.903453+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463f, Description: Updating Key-Value Observers Of Preferences
[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.906445+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc58b0, Description: Loading Preferences From System CFPrefsD For Search List
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907627+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 0 -> 1; styleActivationCount: 0 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907921+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 0 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:17.907992+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activating engine
[        ] [DEVICE LOG] 2018-01-06 15:52:17.908409+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=0, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Inactive -> Activating
[        ] [DEVICE LOG] 2018-01-06 15:52:17.910005+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=3, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Activating -> Running
[+3359 ms] [DEVICE LOG] 2018-01-06 15:52:21.273835+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:21.273875+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:21.274731+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:21.274782+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1
[+2656 ms] [DEVICE LOG] 2018-01-06 15:52:23.928718+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:23.928799+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[   +6 ms] [DEVICE LOG] 2018-01-06 15:52:23.929589+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:23.929682+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1
[+3667 ms] [DEVICE LOG] 2018-01-06 15:52:27.608269+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2
[        ] [DEVICE LOG] 2018-01-06 15:52:27.608319+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2
[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:27.609012+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1
[        ] [DEVICE LOG] 2018-01-06 15:52:27.609054+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1

Flutter Doctor
[] Flutter (on Mac OS X 10.13.2 17C88, locale en-SE, channel master)
     Flutter at /Users/fsimon/flutter
     Framework revision a414fb458d (28 hours ago), 2018-01-06 12:02:33 +0100
     Engine revision 12e0e38a8b
     Tools Dart version 1.25.0-dev.11.0
     Engine Dart version 2.0.0-edge.8d7219a5b6a7c2505ff57f23e7cf80da4c724512

[] Android toolchain - develop for Android devices (Android SDK 26.0.3)
     Android SDK at /Users/fsimon/Library/Android/sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-26, build-tools 26.0.3
     Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] iOS toolchain - develop for iOS devices (Xcode 9.2)
     Xcode at /Applications/Xcode.app/Contents/Developer
     Xcode 9.2, Build version 9C40b
     ios-deploy 1.9.2
     CocoaPods version 1.3.1

[] Android Studio (version 3.0)
     Android Studio at /Applications/Android Studio.app/Contents
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)

[] Connected devices
     iPhone X                   1C6CECD0-0E72-489F-90D7-197A80D82B98  ios          iOS 11.2 (simulator)",3
,2779,145,99,25627,"Here is my simple Todo App using Flutter and Node/Express API's and MongoDB
I am specifically having a problem with parsing json array of todo items.
Todo App
First of all when ever I run this project.
I get this error
Could not load source 'dart:core/runtime/libobject_patch.dart': .
Nothing in the debug console.
After Reloading a few times, I land on Login Page,
Then enter credentials and login,
Immediately, the app is frozen and I get the Above error.
If you look at lib/services/todoService.dart  => getAll() method.
Am I doing something wrong?
Secondly,
The json returned from server is as below, and I need to parse it to a List in Dart.
I tried following this
I get the same error as above.
[ { ""completed"": true, ""completedAt"": 1545314655878, ""_id"": ""5c0244d8ac3baf291808b03a"", ""text"": ""Shop some Groceries"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1b9a14fe8f70360cd0a393"", ""text"": ""Eat before 12"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1ba02efe8f70360cd0a394"", ""text"": ""Test todo"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 } ]
My Todo Class looks like this
class Todo {
  final String text;
  final String author;
  final bool completed;
  final DateTime completedAt;
  final String id;

  Todo(
      {this.author = """",
      this.id = """",
      this.completed = false,
      DateTime completedAt,
      this.text = """"})
      : completedAt = completedAt ?? DateTime.now();

  factory Todo.fromJson(Map<String, dynamic> json) {
    return Todo(
      text: json['text'],
      author: json['_author'],
      completed: json['completed'],
      completedAt: DateTime.fromMillisecondsSinceEpoch(json['completedAt']),
      id: json['_id'],
    );
  }
}

I cant find a good way to parse it into a List and return it to a FutureBuilder.
Logs
I use VS Code, No Errors in the debug Console.

[] Flutter (Channel beta, v1.0.0, on Microsoft Windows [Version 10.0.17134.472], locale en-IN)
     Flutter version 1.0.0 at C:\flutter
     Framework revision 5391447fae (3 weeks ago), 2018-11-29 19:41:26 -0800
     Engine revision 7375a0f414
     Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)

[] Android toolchain - develop for Android devices (Android SDK 28.0.3)
     Android SDK at C:\Users\lightyaer\AppData\Local\Android\Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     ANDROID_HOME = C:\Users\lightyaer\AppData\Local\Android\Sdk
     Java binary at: C:\Program Files\Android\Android Studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)
     All Android licenses accepted.

[] Android Studio (version 3.2)
     Android Studio at C:\Program Files\Android\Android Studio
    X Flutter plugin not installed; this adds Flutter specific functionality.
    X Dart plugin not installed; this adds Dart specific functionality.
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)

[] VS Code (version 1.30.1)
     VS Code at C:\Users\lightyaer\AppData\Local\Programs\Microsoft VS Code
     Flutter extension version 2.21.1

[] Connected device (1 available)
     Android SDK built for x86  emulator-5554  android-x86  Android 9 (API 28) (emulator)

 No issues found!",Trouble parsing Json Array ,"Trouble parsing Json Array Here is my simple Todo App using Flutter and Node/Express API's and MongoDB
I am specifically having a problem with parsing json array of todo items.
Todo App
First of all when ever I run this project.
I get this error
Could not load source 'dart:core/runtime/libobject_patch.dart': .
Nothing in the debug console.
After Reloading a few times, I land on Login Page,
Then enter credentials and login,
Immediately, the app is frozen and I get the Above error.
If you look at lib/services/todoService.dart  => getAll() method.
Am I doing something wrong?
Secondly,
The json returned from server is as below, and I need to parse it to a List in Dart.
I tried following this
I get the same error as above.
[ { ""completed"": true, ""completedAt"": 1545314655878, ""_id"": ""5c0244d8ac3baf291808b03a"", ""text"": ""Shop some Groceries"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1b9a14fe8f70360cd0a393"", ""text"": ""Eat before 12"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1ba02efe8f70360cd0a394"", ""text"": ""Test todo"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 } ]
My Todo Class looks like this
class Todo {
  final String text;
  final String author;
  final bool completed;
  final DateTime completedAt;
  final String id;

  Todo(
      {this.author = """",
      this.id = """",
      this.completed = false,
      DateTime completedAt,
      this.text = """"})
      : completedAt = completedAt ?? DateTime.now();

  factory Todo.fromJson(Map<String, dynamic> json) {
    return Todo(
      text: json['text'],
      author: json['_author'],
      completed: json['completed'],
      completedAt: DateTime.fromMillisecondsSinceEpoch(json['completedAt']),
      id: json['_id'],
    );
  }
}

I cant find a good way to parse it into a List and return it to a FutureBuilder.
Logs
I use VS Code, No Errors in the debug Console.

[] Flutter (Channel beta, v1.0.0, on Microsoft Windows [Version 10.0.17134.472], locale en-IN)
     Flutter version 1.0.0 at C:\flutter
     Framework revision 5391447fae (3 weeks ago), 2018-11-29 19:41:26 -0800
     Engine revision 7375a0f414
     Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)

[] Android toolchain - develop for Android devices (Android SDK 28.0.3)
     Android SDK at C:\Users\lightyaer\AppData\Local\Android\Sdk
     Android NDK location not configured (optional; useful for native profiling support)
     Platform android-28, build-tools 28.0.3
     ANDROID_HOME = C:\Users\lightyaer\AppData\Local\Android\Sdk
     Java binary at: C:\Program Files\Android\Android Studio\jre\bin\java
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)
     All Android licenses accepted.

[] Android Studio (version 3.2)
     Android Studio at C:\Program Files\Android\Android Studio
    X Flutter plugin not installed; this adds Flutter specific functionality.
    X Dart plugin not installed; this adds Dart specific functionality.
     Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)

[] VS Code (version 1.30.1)
     VS Code at C:\Users\lightyaer\AppData\Local\Programs\Microsoft VS Code
     Flutter extension version 2.21.1

[] Connected device (1 available)
     Android SDK built for x86  emulator-5554  android-x86  Android 9 (API 28) (emulator)

 No issues found!",3
,2780,146,0,53812,"I really like the new title bar style that can be activated by ""editor.titleBarStyle"": ""custom"". However, as a long time Ubuntu user I'm accustomed to my window close/max/min controls being on the left hand side of the title bar. Is is possbible to have another member in the titleBarStyle enum ""custom-left"" or something named similarly to have the controls render on the far left rather than the far right in the order close, minimize, maximize(restore)?
Below, the native menu for VS Code (I do not mind the square buttons of the ciustom style as they fit well with the VS brand):",Request: Left Close/Max/Min buttons variant for custom title bar,"Request: Left Close/Max/Min buttons variant for custom title barI really like the new title bar style that can be activated by ""editor.titleBarStyle"": ""custom"". However, as a long time Ubuntu user I'm accustomed to my window close/max/min controls being on the left hand side of the title bar. Is is possbible to have another member in the titleBarStyle enum ""custom-left"" or something named similarly to have the controls render on the far left rather than the far right in the order close, minimize, maximize(restore)?
Below, the native menu for VS Code (I do not mind the square buttons of the ciustom style as they fit well with the VS brand):",3
,2781,145,1,37058,"VSCode Version: 1.17.0.0
OS Version: Microsoft Windows [Version 10.0.14393]

Steps to Reproduce:

Update vscode
While updating launch a second vscode instance
Abort the Installation on Dialog
Start vscode again


Reproduces without extensions: Yes",When relaunch vscode after aborted vscode update then Uncaught Exception thrown,"When relaunch vscode after aborted vscode update then Uncaught Exception thrownVSCode Version: 1.17.0.0
OS Version: Microsoft Windows [Version 10.0.14393]

Steps to Reproduce:

Update vscode
While updating launch a second vscode instance
Abort the Installation on Dialog
Start vscode again


Reproduces without extensions: Yes",3
,2782,145,2,8966,"VSCode Version:
Version 1.3.0
Commit e724f26
Date 2016-07-07T16:56:12.476Z
Shell 0.37.6
Renderer 49.0.2623.75
Node 5.10.0
OS Version:
Windows 7 x64 SP1

I have installed Git for Windows with both the git and Unix commands in the PATH.
VSCode doesn't seem to find the Unix commands.",Integratel Terminal doesn't find commands from PATH,"Integratel Terminal doesn't find commands from PATHVSCode Version:
Version 1.3.0
Commit e724f26
Date 2016-07-07T16:56:12.476Z
Shell 0.37.6
Renderer 49.0.2623.75
Node 5.10.0
OS Version:
Windows 7 x64 SP1

I have installed Git for Windows with both the git and Unix commands in the PATH.
VSCode doesn't seem to find the Unix commands.",3
,2783,148,3,40634,"VSCode Version: Code - Insiders 1.20.0-insider (8f015e5, 2017-12-21T08:34:32.639Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




staticserver
gao
0.0.5




Steps to Reproduce:
just mention a color in hex format and hover arrow of the hex code, this should normally open a in-biult color picker. This is not working only in the insider version, color picker works perfectly in Visual Code.
however, whenever the color picker would appear, it would quickly close itself and very rarely it would work properly.
This issue is since 2 previous versions of VSinsider
Reproduces without extensions: Yes",Color picker wont open when hovering over color box or moving between box and value,"Color picker wont open when hovering over color box or moving between box and valueVSCode Version: Code - Insiders 1.20.0-insider (8f015e5, 2017-12-21T08:34:32.639Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




staticserver
gao
0.0.5




Steps to Reproduce:
just mention a color in hex format and hover arrow of the hex code, this should normally open a in-biult color picker. This is not working only in the insider version, color picker works perfectly in Visual Code.
however, whenever the color picker would appear, it would quickly close itself and very rarely it would work properly.
This issue is since 2 previous versions of VSinsider
Reproduces without extensions: Yes",3
,2784,145,4,28693,"Since the update to 1.13 I am seeing fuzzy icons in the UI of VSCode. I am using the light theme. This is unrelated to the theme I select. Switching between light and dark themes doesn't fix the problem.


VSCode Version: Code 1.13.0 (376c52b, 2017-06-08T16:34:53.678Z)
OS Version: Darwin x64 16.6.0

Steps to Reproduce:

Open VS Code
Open a code file",Icons are fuzzy in the UI,"Icons are fuzzy in the UISince the update to 1.13 I am seeing fuzzy icons in the UI of VSCode. I am using the light theme. This is unrelated to the theme I select. Switching between light and dark themes doesn't fix the problem.


VSCode Version: Code 1.13.0 (376c52b, 2017-06-08T16:34:53.678Z)
OS Version: Darwin x64 16.6.0

Steps to Reproduce:

Open VS Code
Open a code file",3
,2785,148,5,27456,"Ref: #25291
Complexity: 2
OS:

 Windows @alexandrudima
 OS X @weinand
 Linux @Tyriar

As part of engineering work we have automated major part of our smoke test. To ensure its stability, it is important to run it on different machines.
Running automated test
Prerequisites

Update stable to the latest version.
Close all VS Code instances that will be used in a test (e.g. latest insiders, stable) not to have any interference with the running test that will spawn them itself.

Once the test is running, please do not interfere with it by doing selection or focusing anywhere within spawned test VS Code instance.
Approximate running time
10 minutes minimum (the more failures it has, the more it runs due to adaptive retry strategy of the waiting time).
Procedure

Run git checkout michelkaporin/smoketest in vscode repository. Currently the code lives on my branch only.
cd test/smoke
npm install
npm test -- --latest ""path/to/binary"" --stable  ""path/to/binary"", where latest argument is the path to VS Code executable to conduct testing, and stable is previous stable version (in order to run 'Data Migration' tests). If the latter argument is ommited, 'Data Migration' tests won't run.

Example commands:
npm test -- --latest ""C:\Program Files (x86)\Microsoft VS Code Insiders\Code - Insiders.exe"" --stable ""C:\Program Files (x86)\Microsoft VS Code\Code.exe""
or
npm test -- --latest ""/Applications/Visual Studio Code - Insiders.app/Contents/MacOS/Electron"" --stable ""/Applications/Visual Studio Code.app/Contents/MacOS/Electron""
Known problems

**#27452 bug results in very many failing tests on OS X, once that is fixed, all should pass.
Spectron spawns multiple cmd windows on Windows OS. This leads to VS Code window to be overlapped by another one. To look at what the test does you need to click on the VS Code title bar to focus on it in every test. (electron-userland/spectron#60)

If you get failing tests, please dump the log with failed error messages here.
Any feedback is appreciated ",Test: Automated Smoke Test,"Test: Automated Smoke TestRef: #25291
Complexity: 2
OS:

 Windows @alexandrudima
 OS X @weinand
 Linux @Tyriar

As part of engineering work we have automated major part of our smoke test. To ensure its stability, it is important to run it on different machines.
Running automated test
Prerequisites

Update stable to the latest version.
Close all VS Code instances that will be used in a test (e.g. latest insiders, stable) not to have any interference with the running test that will spawn them itself.

Once the test is running, please do not interfere with it by doing selection or focusing anywhere within spawned test VS Code instance.
Approximate running time
10 minutes minimum (the more failures it has, the more it runs due to adaptive retry strategy of the waiting time).
Procedure

Run git checkout michelkaporin/smoketest in vscode repository. Currently the code lives on my branch only.
cd test/smoke
npm install
npm test -- --latest ""path/to/binary"" --stable  ""path/to/binary"", where latest argument is the path to VS Code executable to conduct testing, and stable is previous stable version (in order to run 'Data Migration' tests). If the latter argument is ommited, 'Data Migration' tests won't run.

Example commands:
npm test -- --latest ""C:\Program Files (x86)\Microsoft VS Code Insiders\Code - Insiders.exe"" --stable ""C:\Program Files (x86)\Microsoft VS Code\Code.exe""
or
npm test -- --latest ""/Applications/Visual Studio Code - Insiders.app/Contents/MacOS/Electron"" --stable ""/Applications/Visual Studio Code.app/Contents/MacOS/Electron""
Known problems

**#27452 bug results in very many failing tests on OS X, once that is fixed, all should pass.
Spectron spawns multiple cmd windows on Windows OS. This leads to VS Code window to be overlapped by another one. To look at what the test does you need to click on the VS Code title bar to focus on it in every test. (electron-userland/spectron#60)

If you get failing tests, please dump the log with failed error messages here.
Any feedback is appreciated ",3
,2786,148,6,90455,"Issue Type: Bug
When closing files with unsaved changes, a dialog open with the three possible actions: ""Don't save"", ""Cancel"", ""Save"" in that order.
When closing an unsaved workspace, a similar dialog appear with actions ""Save"", ""Cancel"", ""Don't save"" in that order.
That feel inconsistent and could cause confusion. The order should probably be the same in every dialog.
VS Code version: Code 1.42.0 (ae08d54, 2020-02-06T10:51:23.649Z)
OS version: Linux x64 5.0.0-38-generic

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-7260U CPU @ 2.20GHz (4 x 3340)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmetal: disabled_offmultiple_raster_threads: enabled_onoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: disabled_softwareskia_renderer: disabled_offsurface_control: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: enabled_onviz_hit_test_surface_layer: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
1, 1, 1


Memory (System)
11.62GB (0.87GB free)


Process Argv
remarques.md --no-sandbox


Screen Reader
no


VM
0%



Extensions (14)



Extension
Author (truncated)
Version




npm-intellisense
chr
1.3.0


path-intellisense
chr
1.4.2


vscode-eslint
dba
2.0.15


vscode-npm-script
eg2
0.3.11


search-node-modules
jas
1.3.0


vscode-exec-node
mir
0.5.1


vscode-language-pack-fr
MS-
1.42.2


cpptools
ms-
0.26.3


debugger-for-chrome
msj
4.12.6


vscode-npm-scripts
tra
0.2.1


vim
vsc
1.12.4


nodejs-extension-pack
wad
0.1.9


vscode-todo-highlight
way
1.0.4


JavaScriptSnippets
xab
1.7.2",Inconsistency between unsaved file dialog and unsaved workspace dialog,"Inconsistency between unsaved file dialog and unsaved workspace dialogIssue Type: Bug
When closing files with unsaved changes, a dialog open with the three possible actions: ""Don't save"", ""Cancel"", ""Save"" in that order.
When closing an unsaved workspace, a similar dialog appear with actions ""Save"", ""Cancel"", ""Don't save"" in that order.
That feel inconsistent and could cause confusion. The order should probably be the same in every dialog.
VS Code version: Code 1.42.0 (ae08d54, 2020-02-06T10:51:23.649Z)
OS version: Linux x64 5.0.0-38-generic

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-7260U CPU @ 2.20GHz (4 x 3340)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmetal: disabled_offmultiple_raster_threads: enabled_onoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: disabled_softwareskia_renderer: disabled_offsurface_control: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: enabled_onviz_hit_test_surface_layer: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
1, 1, 1


Memory (System)
11.62GB (0.87GB free)


Process Argv
remarques.md --no-sandbox


Screen Reader
no


VM
0%



Extensions (14)



Extension
Author (truncated)
Version




npm-intellisense
chr
1.3.0


path-intellisense
chr
1.4.2


vscode-eslint
dba
2.0.15


vscode-npm-script
eg2
0.3.11


search-node-modules
jas
1.3.0


vscode-exec-node
mir
0.5.1


vscode-language-pack-fr
MS-
1.42.2


cpptools
ms-
0.26.3


debugger-for-chrome
msj
4.12.6


vscode-npm-scripts
tra
0.2.1


vim
vsc
1.12.4


nodejs-extension-pack
wad
0.1.9


vscode-todo-highlight
way
1.0.4


JavaScriptSnippets
xab
1.7.2",3
,2787,148,7,60013,"Issue Type: Bug
i open terminal, but  its look like is not work !
VS Code version: Code 1.27.2 (f46c4c4, 2018-09-12T16:17:45.060Z)
OS version: Windows_NT x64 10.0.17763

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz (8 x 2592)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Memory (System)
15.88GB (8.52GB free)


Process Argv
G:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
67%



Extensions (15)



Extension
Author (truncated)
Version




project-manager
ale
9.0.1


vscode-svgviewer
css
1.4.6


vscode-eslint
dba
1.6.0


xml
Dot
2.3.2


gitlens
eam
8.5.6


prettier-vscode
esb
1.6.1


vue-format
feb
0.1.2


git-project-manager
fel
1.7.1


vscode-attrs-sorter
mrm
2.1.0


cpptools
ms-
0.19.0


vetur
oct
0.13.0


vscode-icons
rob
7.27.0


partial-diff
ryu
1.4.0


vim
vsc
0.16.6


gitblame
wad
2.4.4",terminal ,"terminal Issue Type: Bug
i open terminal, but  its look like is not work !
VS Code version: Code 1.27.2 (f46c4c4, 2018-09-12T16:17:45.060Z)
OS version: Windows_NT x64 10.0.17763

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz (8 x 2592)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Memory (System)
15.88GB (8.52GB free)


Process Argv
G:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
67%



Extensions (15)



Extension
Author (truncated)
Version




project-manager
ale
9.0.1


vscode-svgviewer
css
1.4.6


vscode-eslint
dba
1.6.0


xml
Dot
2.3.2


gitlens
eam
8.5.6


prettier-vscode
esb
1.6.1


vue-format
feb
0.1.2


git-project-manager
fel
1.7.1


vscode-attrs-sorter
mrm
2.1.0


cpptools
ms-
0.19.0


vetur
oct
0.13.0


vscode-icons
rob
7.27.0


partial-diff
ryu
1.4.0


vim
vsc
0.16.6


gitblame
wad
2.4.4",3
,2788,146,8,28823,"VSCode Version: insider
OS Version: Windows 10 64bit

Themes appear in the extensions panel and can be disabled just like normal extensions.
I think disabled themes shouldn't appear when running the preferences: Color Theme command. Right now you may set a theme that's disabled wihch is a bit weird.",You can set themes that are disabled in extensions panel,"You can set themes that are disabled in extensions panelVSCode Version: insider
OS Version: Windows 10 64bit

Themes appear in the extensions panel and can be disabled just like normal extensions.
I think disabled themes shouldn't appear when running the preferences: Color Theme command. Right now you may set a theme that's disabled wihch is a bit weird.",3
,2789,148,9,32275,"VSCode Version: Code 1.15.0 (8b95971, 2017-08-09T20:06:21.685Z)
OS Version: Darwin x64 17.0.0
Extensions:




Extension
Author (truncated)
Version




material-icon-theme
PKi
2.1.0


vscode-docker
Pet
0.0.16


code-settings-sync
Sha
2.8.2


go
Twe
0.0.1


html-css-class-completion
Zig
1.8.0


vscode-color
ans
0.4.5


vscode-eslint
dba
1.2.11


python
don
0.7.0


vscode-babel-coloring
dza
0.0.4


gitlens
eam
4.3.3


vscode-html-css
ecm
0.1.7


tslint
eg2
0.17.0


nim
kos
0.5.26


Go
luk
0.6.63


markdown-shortcuts
mdi
0.8.1


prettify-json
moh
0.0.3


debugger-for-chrome
msj
3.1.7


color-highlight
nau
2.3.0


vetur
oct
0.9.3


proto
pet
0.0.2


Ruby
reb
0.13.0


PostCSS
ric
1.0.1


stylelint
shi
0.28.0


babelrc
wad
1.0.0


vscode-import-cost
wix
2.0.0


vscode-proto3
zxh
0.1.2



(5 theme extensions excluded)

Steps to Reproduce:

Switch to Chinese IME (e.g. Sougou / Baidu)
See the screenshot:


I reproduced this problem in the monaco editor, I think it should be the text input implementation issue using textarea, did not find this problem in ace editor.

Reproduces without extensions: Yes",The incorrect IME position when the first character is entered,"The incorrect IME position when the first character is enteredVSCode Version: Code 1.15.0 (8b95971, 2017-08-09T20:06:21.685Z)
OS Version: Darwin x64 17.0.0
Extensions:




Extension
Author (truncated)
Version




material-icon-theme
PKi
2.1.0


vscode-docker
Pet
0.0.16


code-settings-sync
Sha
2.8.2


go
Twe
0.0.1


html-css-class-completion
Zig
1.8.0


vscode-color
ans
0.4.5


vscode-eslint
dba
1.2.11


python
don
0.7.0


vscode-babel-coloring
dza
0.0.4


gitlens
eam
4.3.3


vscode-html-css
ecm
0.1.7


tslint
eg2
0.17.0


nim
kos
0.5.26


Go
luk
0.6.63


markdown-shortcuts
mdi
0.8.1


prettify-json
moh
0.0.3


debugger-for-chrome
msj
3.1.7


color-highlight
nau
2.3.0


vetur
oct
0.9.3


proto
pet
0.0.2


Ruby
reb
0.13.0


PostCSS
ric
1.0.1


stylelint
shi
0.28.0


babelrc
wad
1.0.0


vscode-import-cost
wix
2.0.0


vscode-proto3
zxh
0.1.2



(5 theme extensions excluded)

Steps to Reproduce:

Switch to Chinese IME (e.g. Sougou / Baidu)
See the screenshot:


I reproduced this problem in the monaco editor, I think it should be the text input implementation issue using textarea, did not find this problem in ace editor.

Reproduces without extensions: Yes",3
,2790,144,10,58686,"VSCode Version:  1.27.2
OS Version: Windows 10 Pro 64bit OS,x64-based processor

Steps to Reproduce:

Start Node Debugger
Error
Debugger attached.
Waiting for the debugger to disconnect...

Does this issue occur when all extensions are disabled?: Yes
I have tried everything as follows  but nothing works->

restarted vs code
restarted my machine
uninstall node,npm,vs code

please suggest some solution for this bug.Basically i'm debugging typescript file.",Debugger attached. Waiting for the debugger to disconnect...,"Debugger attached. Waiting for the debugger to disconnect...VSCode Version:  1.27.2
OS Version: Windows 10 Pro 64bit OS,x64-based processor

Steps to Reproduce:

Start Node Debugger
Error
Debugger attached.
Waiting for the debugger to disconnect...

Does this issue occur when all extensions are disabled?: Yes
I have tried everything as follows  but nothing works->

restarted vs code
restarted my machine
uninstall node,npm,vs code

please suggest some solution for this bug.Basically i'm debugging typescript file.",3
,2791,142,11,51539,"VSCode Version: Version 1.24.0 (1.24.0)
OS Version: Mac High Sierra 10.13.4
ESLint Extension: 1.4.12

Steps to Reproduce:

The default behavior for VSCode is to add spaces to objects:

const x = { a: 123 };


I have a .eslintrc config setup with the rule ""object-curly-spacing"": [""error"", ""never""]. Linting occurs and reports the default spacing behavior as an error as expected.
This behavior can be overridden in the VSCode settings via ""javascript.format.insertSpaceAfterOpeningAndBeforeClosingNonemptyBraces"": false
I was expecting that the default behavior of VSCode would be overridden by the ESLint rules when formatting code.


Does this issue occur when all extensions are disabled?: No. The ESLint extension must be installed for VSCode to utilize the .eslintrc rules.
This may be an issue correctable in the ESLint extension. If so I will move this issue to that project.",Local editor settings not being overridden by ESLint settings,"Local editor settings not being overridden by ESLint settingsVSCode Version: Version 1.24.0 (1.24.0)
OS Version: Mac High Sierra 10.13.4
ESLint Extension: 1.4.12

Steps to Reproduce:

The default behavior for VSCode is to add spaces to objects:

const x = { a: 123 };


I have a .eslintrc config setup with the rule ""object-curly-spacing"": [""error"", ""never""]. Linting occurs and reports the default spacing behavior as an error as expected.
This behavior can be overridden in the VSCode settings via ""javascript.format.insertSpaceAfterOpeningAndBeforeClosingNonemptyBraces"": false
I was expecting that the default behavior of VSCode would be overridden by the ESLint rules when formatting code.


Does this issue occur when all extensions are disabled?: No. The ESLint extension must be installed for VSCode to utilize the .eslintrc rules.
This may be an issue correctable in the ESLint extension. If so I will move this issue to that project.",3
,2792,148,12,1052,"I'm using version df35236
For this code I get an incorrect error.
  /**
   * Removes all the children from `<div class=""children""></div>` element
   *
  */
  removeChildren() {
    if (this.element.querySelector('div.children')) {
      this.element.removeChild(this.element.querySelector('div.children'));
    }
  }
TypeScript compiler does not complain for this code but VSCode throws this error:",Incorrect error in TypeScript code,"Incorrect error in TypeScript codeI'm using version df35236
For this code I get an incorrect error.
  /**
   * Removes all the children from `<div class=""children""></div>` element
   *
  */
  removeChildren() {
    if (this.element.querySelector('div.children')) {
      this.element.removeChild(this.element.querySelector('div.children'));
    }
  }
TypeScript compiler does not complain for this code but VSCode throws this error:",3
,2793,145,13,59547,"It was working before.
now at import the module:
import { Validation } from 'bunnyjs/src/Validation';
[ts]
Could not find a declaration file for module 'bunnyjs/src/Validation'. 'c:/Dev/Notanet/frontend-env/cockpit/node_modules/bunnyjs/src/Validation.js' implicitly has an 'any' type.
Try npm install @types/bunnyjs if it exists or add a new declaration (.d.ts) file containing declare module 'bunnyjs';",After update throwing : Couldn't find declaration for many modules ,"After update throwing : Couldn't find declaration for many modules It was working before.
now at import the module:
import { Validation } from 'bunnyjs/src/Validation';
[ts]
Could not find a declaration file for module 'bunnyjs/src/Validation'. 'c:/Dev/Notanet/frontend-env/cockpit/node_modules/bunnyjs/src/Validation.js' implicitly has an 'any' type.
Try npm install @types/bunnyjs if it exists or add a new declaration (.d.ts) file containing declare module 'bunnyjs';",3
,2794,148,14,59227,Issue Id: 35acb833-bda7-37c3-14da-3ab8f360751bVersions - 1.28.0-insider (9/20/2018 2:13:44 PM)Stack TypeError: Cannot read property 'forEach' of null/vs/base/browser/ui/menu/menu.ts#158:13 (style)/vs/platform/theme/common/styler.ts#53:20 (call)/vs/base/common/event.ts#140:15 (fire)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#350:26 (applyTheme)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#295:17 (onComplete)/vs/base/common/winjs.base.js#1191:0 (_notify)/vs/base/common/winjs.base.js#867:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1057:0 (onComplete)/vs/base/common/winjs.base.js#1587:0 (then)/vs/base/common/winjs.base.js#762:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1073:0    at Object.g [as _notify] (out/vs/workbench/workbench.m, Cannot read property 'forEach' of null,Cannot read property 'forEach' of nullIssue Id: 35acb833-bda7-37c3-14da-3ab8f360751bVersions - 1.28.0-insider (9/20/2018 2:13:44 PM)Stack TypeError: Cannot read property 'forEach' of null/vs/base/browser/ui/menu/menu.ts#158:13 (style)/vs/platform/theme/common/styler.ts#53:20 (call)/vs/base/common/event.ts#140:15 (fire)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#350:26 (applyTheme)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#295:17 (onComplete)/vs/base/common/winjs.base.js#1191:0 (_notify)/vs/base/common/winjs.base.js#867:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1057:0 (onComplete)/vs/base/common/winjs.base.js#1587:0 (then)/vs/base/common/winjs.base.js#762:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1073:0    at Object.g [as _notify] (out/vs/workbench/workbench.m,3
,2795,148,15,32946,/cc @sandy081,Multi root: revisit workspace file format,Multi root: revisit workspace file format/cc @sandy081,3
,2796,142,16,31170,"Feature Request
It would be awesome if when using Go to File... (Command-P) that a preview of the current highlighted file is show and as you up/down arrow key a different file, you see a preview of the current selected file.",Show Preview of file while in Go to File... ,"Show Preview of file while in Go to File... Feature Request
It would be awesome if when using Go to File... (Command-P) that a preview of the current highlighted file is show and as you up/down arrow key a different file, you see a preview of the current selected file.",3
,2797,148,17,25704,"VSCode Version: Code 1.11.2 (6eaebe3, 2017-04-13T07:56:42.517Z)
OS Version: Darwin x64 16.6.0
Extensions:




Extension
Author
Version




python
donjayamanne
0.6.3


cpptools
ms-vscode
0.11.0


vscode-arduino
vsciot-vscode
0.1.2




Steps to Reproduce:

open VS Code
open file from git repo already local on machine
try to commit from git version control panel
fails
sad day",Git version control can't interact with repos that are already local on the machine.,"Git version control can't interact with repos that are already local on the machine.VSCode Version: Code 1.11.2 (6eaebe3, 2017-04-13T07:56:42.517Z)
OS Version: Darwin x64 16.6.0
Extensions:




Extension
Author
Version




python
donjayamanne
0.6.3


cpptools
ms-vscode
0.11.0


vscode-arduino
vsciot-vscode
0.1.2




Steps to Reproduce:

open VS Code
open file from git repo already local on machine
try to commit from git version control panel
fails
sad day",3
,2798,146,18,39462,,can we go back please the new logo is so ugly!,can we go back please the new logo is so ugly!,3
,2799,148,19,27712,"Testing #26203
Have a js file with the following:
function fib(n) {
    if (n === 1 || n === 0) {
        throw new Error('Unexpected');
    }
    return fib(n - 1) + fib(n - 2);
}
fib(3)

Start debugging (F5)
The exception widget shows up at the correct location
The module.js is clickable but it tries to open a file called module.js from the workspace instead of opening the nodejs built-in module
Clicking on module.js from the call stack works correctly, so perhaps the debug adapter needs to be asked about how to resolve links in the error stackframe, and only if it doesn't know how to do it, the default should be to fallback to trying to open files?


Other use-cases would be debugging a remote target where the error stack trace would contain the file paths as installed on the remote machine and these file paths would need to be translated according to the debug configuration to the local filesystem, or otherwise opened in readonly mode by fetching their sources from nodejs
fyi @weinand @isidorn",Cannot click on nodejs built-in modules,"Cannot click on nodejs built-in modulesTesting #26203
Have a js file with the following:
function fib(n) {
    if (n === 1 || n === 0) {
        throw new Error('Unexpected');
    }
    return fib(n - 1) + fib(n - 2);
}
fib(3)

Start debugging (F5)
The exception widget shows up at the correct location
The module.js is clickable but it tries to open a file called module.js from the workspace instead of opening the nodejs built-in module
Clicking on module.js from the call stack works correctly, so perhaps the debug adapter needs to be asked about how to resolve links in the error stackframe, and only if it doesn't know how to do it, the default should be to fallback to trying to open files?


Other use-cases would be debugging a remote target where the error stack trace would contain the file paths as installed on the remote machine and these file paths would need to be translated according to the debug configuration to the local filesystem, or otherwise opened in readonly mode by fetching their sources from nodejs
fyi @weinand @isidorn",3
,2800,146,20,38009,"VSCode Version: Code 1.18.0 (dcee220, 2017-11-08T21:19:36.079Z)
OS Version: Windows_NT ia32 10.0.15063
Extensions:




Extension
Author (truncated)
Version




html-snippets
abu
0.1.0


django-snippets
bib
1.1.0


vscode-styled-jsx
bla
0.1.1


npm-intellisense
chr
1.3.0


path-intellisense
chr
1.4.2


gitignore
cod
0.5.0


vscode-eslint
dba
1.4.3


gitlens
eam
6.0.0


tslint
eg2
1.0.16


vscode-npm-script
eg2
0.3.3


vsc-material-theme
Equ
1.1.1


prettier-vscode
esb
0.24.0


php-debug
fel
1.11.1


php-intellisense
fel
1.5.4


beautify
Hoo
1.1.1


intellij-idea-keybindings
k--
0.2.16


vscode-github
Kni
0.23.0


MagicPython
mag
1.0.12


Kotlin
mat
1.3.0


HTMLHint
mka
0.4.0


vscode-apache
mrm
1.1.1


vscode-attrs-sorter
mrm
2.1.0


vscode-jade-snippets
mrm
1.0.1


vscode-pugbeautify
mrm
1.0.2


vscode-puglint
mrm
2.3.0


vscode-scss
mrm
0.6.2


vscode-stylefmt
mrm
2.5.0


python
ms-
0.8.0


cpptools
ms-
0.14.2


csharp
ms-
1.13.0


PowerShell
ms-
1.5.0


typescript-javascript-grammar
ms-
0.0.24


material-icon-theme
PKi
2.2.4


vscode-template-literal-editor
pli
0.8.4


java
red
0.14.0


vscode-icons
rob
7.17.0


prettier-eslint-vscode
Rob
0.7.1


code-settings-sync
Sha
2.8.5


shader
sle
1.1.2


twig
wha
1.0.2


jinja
who
0.0.8


ReactSnippets
xab
1.4.0




Steps to Reproduce:

Put the terminal panel to the sidebar.
Make that window smaller.
Close button is not visible, and not reachable in any way.

As you see here, the 'X' button should be in the upper right.

Reproduces without extensions: Yes",Terminal sidebar close button not visible,"Terminal sidebar close button not visibleVSCode Version: Code 1.18.0 (dcee220, 2017-11-08T21:19:36.079Z)
OS Version: Windows_NT ia32 10.0.15063
Extensions:




Extension
Author (truncated)
Version




html-snippets
abu
0.1.0


django-snippets
bib
1.1.0


vscode-styled-jsx
bla
0.1.1


npm-intellisense
chr
1.3.0


path-intellisense
chr
1.4.2


gitignore
cod
0.5.0


vscode-eslint
dba
1.4.3


gitlens
eam
6.0.0


tslint
eg2
1.0.16


vscode-npm-script
eg2
0.3.3


vsc-material-theme
Equ
1.1.1


prettier-vscode
esb
0.24.0


php-debug
fel
1.11.1


php-intellisense
fel
1.5.4


beautify
Hoo
1.1.1


intellij-idea-keybindings
k--
0.2.16


vscode-github
Kni
0.23.0


MagicPython
mag
1.0.12


Kotlin
mat
1.3.0


HTMLHint
mka
0.4.0


vscode-apache
mrm
1.1.1


vscode-attrs-sorter
mrm
2.1.0


vscode-jade-snippets
mrm
1.0.1


vscode-pugbeautify
mrm
1.0.2


vscode-puglint
mrm
2.3.0


vscode-scss
mrm
0.6.2


vscode-stylefmt
mrm
2.5.0


python
ms-
0.8.0


cpptools
ms-
0.14.2


csharp
ms-
1.13.0


PowerShell
ms-
1.5.0


typescript-javascript-grammar
ms-
0.0.24


material-icon-theme
PKi
2.2.4


vscode-template-literal-editor
pli
0.8.4


java
red
0.14.0


vscode-icons
rob
7.17.0


prettier-eslint-vscode
Rob
0.7.1


code-settings-sync
Sha
2.8.5


shader
sle
1.1.2


twig
wha
1.0.2


jinja
who
0.0.8


ReactSnippets
xab
1.4.0




Steps to Reproduce:

Put the terminal panel to the sidebar.
Make that window smaller.
Close button is not visible, and not reachable in any way.

As you see here, the 'X' button should be in the upper right.

Reproduces without extensions: Yes",3
,2801,148,21,19662,"There is a discussion going on in 08a1700 where @chrmarti saw an issue on startup for new users that never started Code before. I never saw this issue but the fix seems odd to me: we now suddenly create the directory (userData) via mkdirp that is owned by Electron/Chrome, which seems wrong. It should be created by the framework, not us.
Can we move the getNodeCachedDataDir() into the app.once('ready') callback? That is the place where all code should go that assumes a certain directory structure to be present, not before.",Do not expect app.getPath('userData') to be there before app.once('ready'),"Do not expect app.getPath('userData') to be there before app.once('ready')There is a discussion going on in 08a1700 where @chrmarti saw an issue on startup for new users that never started Code before. I never saw this issue but the fix seems odd to me: we now suddenly create the directory (userData) via mkdirp that is owned by Electron/Chrome, which seems wrong. It should be created by the framework, not us.
Can we move the getNodeCachedDataDir() into the app.once('ready') callback? That is the place where all code should go that assumes a certain directory structure to be present, not before.",3
,2802,148,22,69295,"Issue Type: Bug
codes are not able to run the program
VS Code version: Code 1.30.1 (dea8705, 2018-12-18T18:12:07.165Z)
OS version: Windows_NT x64 6.1.7600

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-2670QM CPU @ 2.20GHz (8 x 2195)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: unavailable_off


Memory (System)
3.95GB (0.90GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (5)



Extension
Author (truncated)
Version




gitlens
eam
9.5.1


tslint
eg2
1.0.43


auto-close-tag
for
0.5.6


LiveServer
rit
5.5.1


autoimport
ste
1.5.3",run code,"run codeIssue Type: Bug
codes are not able to run the program
VS Code version: Code 1.30.1 (dea8705, 2018-12-18T18:12:07.165Z)
OS version: Windows_NT x64 6.1.7600

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-2670QM CPU @ 2.20GHz (8 x 2195)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: unavailable_off


Memory (System)
3.95GB (0.90GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (5)



Extension
Author (truncated)
Version




gitlens
eam
9.5.1


tslint
eg2
1.0.43


auto-close-tag
for
0.5.6


LiveServer
rit
5.5.1


autoimport
ste
1.5.3",3
,2803,148,23,45409,"VSCode Version: 1.20.1
OS Version: Windows 7

Steps to Reproduce:

Press F12 on an import in a node_module file

Behaviour:
Goes to the Typescript definition same as clicking ""Go to Type Definition"".
Expected:
Should go to the implementation.",Go to definition (F12) doesn't work in node_modules with TS,"Go to definition (F12) doesn't work in node_modules with TSVSCode Version: 1.20.1
OS Version: Windows 7

Steps to Reproduce:

Press F12 on an import in a node_module file

Behaviour:
Goes to the Typescript definition same as clicking ""Go to Type Definition"".
Expected:
Should go to the implementation.",3
,2804,146,24,81867,"Issue Type: Bug
Have the current line positioned below the middle of the viewport. Create a horizontal split. The line with the cursor is now hidden in both views. This is super annoying. The line with the cursor should be visible in both views.
https://cl.ly/14374e89477a
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:31:32.854Z)
OS version: Darwin x64 18.7.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz (8 x 2500)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
2, 2, 2


Memory (System)
16.00GB (1.01GB free)


Process Argv



Screen Reader
no


VM
0%",Horizontal split hides the line the cursor is on if it's below the midpoint of the view,"Horizontal split hides the line the cursor is on if it's below the midpoint of the viewIssue Type: Bug
Have the current line positioned below the middle of the viewport. Create a horizontal split. The line with the cursor is now hidden in both views. This is super annoying. The line with the cursor should be visible in both views.
https://cl.ly/14374e89477a
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:31:32.854Z)
OS version: Darwin x64 18.7.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz (8 x 2500)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
2, 2, 2


Memory (System)
16.00GB (1.01GB free)


Process Argv



Screen Reader
no


VM
0%",3
,2805,146,25,75202,"There are some extra pixels under the status bar. Increase the font size to see it more clearly. Also, resizing the window vertically will change the width of the extra pixels.



VSCode Version: 1.35.0
OS Version: Windows_NT x64 10.0.18362

Steps to Reproduce:

Launch VS Code


Does this issue occur when all extensions are disabled?: Yes",Extra pixels under status bar,"Extra pixels under status barThere are some extra pixels under the status bar. Increase the font size to see it more clearly. Also, resizing the window vertically will change the width of the extra pixels.



VSCode Version: 1.35.0
OS Version: Windows_NT x64 10.0.18362

Steps to Reproduce:

Launch VS Code


Does this issue occur when all extensions are disabled?: Yes",3
,2806,145,26,48692,,Git clone should use the new progress API,Git clone should use the new progress API,3
,2807,148,27,72808,"For all the GREAT stuff in VSCode, I can't believe there isn't a ""Remote Changes"" shelf in the SCM view for Git repos (i.e. together with the ""Changes"" and ""Staged Changes"" shelves).  The 3rd party SVN extension has this feature, even the old dinosaur Eclipse has it.
I was even more confused when I didn't find a duplicate in the issues.  Am I missing something?  If I am, could someone please let me know what do Git users do in VSCode (admittedly I am coming from CVS/SVN and am new to Git) to see remote changes? (without using the 3rd party plugin GitLens which is not easy to navigate with all the clutter).  I assume people just don't 'sync' or 'pull' blindly without knowing what changes are coming in?",GIt: Remote Changes Shelf,"GIt: Remote Changes ShelfFor all the GREAT stuff in VSCode, I can't believe there isn't a ""Remote Changes"" shelf in the SCM view for Git repos (i.e. together with the ""Changes"" and ""Staged Changes"" shelves).  The 3rd party SVN extension has this feature, even the old dinosaur Eclipse has it.
I was even more confused when I didn't find a duplicate in the issues.  Am I missing something?  If I am, could someone please let me know what do Git users do in VSCode (admittedly I am coming from CVS/SVN and am new to Git) to see remote changes? (without using the 3rd party plugin GitLens which is not easy to navigate with all the clutter).  I assume people just don't 'sync' or 'pull' blindly without knowing what changes are coming in?",3
,2808,146,28,82123,"I believe right now this is the icon that is being used by VS Code: https://github.com/microsoft/vscode/blob/master/src/vs/workbench/api/browser/media/test.svg
It'd be great if that could be updated to match the new UI.",Update test explorer beaker icon to match new UI ,"Update test explorer beaker icon to match new UI I believe right now this is the icon that is being used by VS Code: https://github.com/microsoft/vscode/blob/master/src/vs/workbench/api/browser/media/test.svg
It'd be great if that could be updated to match the new UI.",3
,2809,142,29,43294,"I'm the kind of person who always presses enter 5 times after running a shell command. I like having some space. But too much space leaves me feeling vulnerable, as if I was stranded in the middle of the ocean. This all or nothing approach with the ""editor.scrollBeyondLastLine"" option isn't working for me.
Can this option be configurable by the line number? If I want to be able to scroll past the bottom by 5 lines, I should be able to set the value to 5. If I want to have the current behavior, I could set the value to true, or ""all"".","Make ""editor.scrollBeyondLastLine"" configurable beyond true or false","Make ""editor.scrollBeyondLastLine"" configurable beyond true or falseI'm the kind of person who always presses enter 5 times after running a shell command. I like having some space. But too much space leaves me feeling vulnerable, as if I was stranded in the middle of the ocean. This all or nothing approach with the ""editor.scrollBeyondLastLine"" option isn't working for me.
Can this option be configurable by the line number? If I want to be able to scroll past the bottom by 5 lines, I should be able to set the value to 5. If I want to have the current behavior, I could set the value to true, or ""all"".",3
,2810,148,30,33444,"""Go to File..."" is essentially a search operation and should be be managed by search.exclude, or optionally an additional config, e.g. goto.exclude. Typical use-case is having Explorer reflect true contents of project (e.g. not hiding generated files), while other operations are for navigating files one will edit.

VSCode Version: @latest
OS Version: OS X

Steps to Reproduce:

Include node_modules in search.exclude, but not in files.exclude in settings
cmd + p
See excluded files appearing in results","""Go to File..."" shows files in search.exclude","""Go to File..."" shows files in search.exclude""Go to File..."" is essentially a search operation and should be be managed by search.exclude, or optionally an additional config, e.g. goto.exclude. Typical use-case is having Explorer reflect true contents of project (e.g. not hiding generated files), while other operations are for navigating files one will edit.

VSCode Version: @latest
OS Version: OS X

Steps to Reproduce:

Include node_modules in search.exclude, but not in files.exclude in settings
cmd + p
See excluded files appearing in results",3
,2811,148,31,29764,"VSCode Version: 1.13.1
OS Version: Win10 Pro

Steps to Reproduce:

Create a file with leading space, e.g. "" my file.txt""
Whilst in the text file folder, pass the file name to VS Code from command line, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" "" my file.txt""; VSCode will try to open file ""my file.txt"" (space removed).
When doing the same with full file path, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" ""c:\ my file.txt""; VSCode will not open any file

Note: When opening from within Code by File->Open, everything works fine
Reproduces without extensions: Yes",Unable to open file with leading space in file name when passed as a parameter from command line,"Unable to open file with leading space in file name when passed as a parameter from command lineVSCode Version: 1.13.1
OS Version: Win10 Pro

Steps to Reproduce:

Create a file with leading space, e.g. "" my file.txt""
Whilst in the text file folder, pass the file name to VS Code from command line, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" "" my file.txt""; VSCode will try to open file ""my file.txt"" (space removed).
When doing the same with full file path, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" ""c:\ my file.txt""; VSCode will not open any file

Note: When opening from within Code by File->Open, everything works fine
Reproduces without extensions: Yes",3
,2812,145,32,25790,"VSCode Version: Code - Insiders 1.12.0-insider (aa05dac, 2017-05-02T00:04:13.700Z)
OS Version: Darwin x64 16.5.0
Extensions:




Extension
Author
Version




ng-template
Angular
0.1.3


code-settings-sync
Shan
2.6.2


sort-lines
Tyriar
1.2.0


Bookmarks
alefragnani
0.14.1


project-manager
alefragnani
0.15.1


path-intellisense
christian-kohler
1.2.0


vscode-eslint
dbaeumer
1.2.8


vscode-html-css
ecmel
0.1.2


tslint
eg2
0.12.0


Angular2
johnpapa
2.2.3


vscode-icon-theme
jtlowe
1.5.0


theme-karyfoundation-themes
karyfoundation
11.1.0


HTMLHint
mkaufman
0.3.3


vscode-autoprefixer
mrmlnc
2.0.0


vscode-stylefmt
mrmlnc
2.3.0


angular2-inline
natewallace
0.0.17


typescript-hero
rbbit
0.12.0


project-snippets
rebornix
0.5.0


stylelint
shinnn
0.24.0


darcula-extended
smlombardi
3.3.2


slime
smlombardi
1.15.0


theme-tesla
smlombardi
6.0.0


change-case
wmaurer
1.0.0




Steps to Reproduce:

in script tags, the opening angle bracket in closing </script> tags and no longer colored like other tag brackets:


for comparison, some non-script tags:

Furthermore, you can see that this font, Fira Code, uses ligatures for the </ yet the script tags no longer are using them.
Both things happened in the current build.",Closing script tags not colored not using ligatures,"Closing script tags not colored not using ligaturesVSCode Version: Code - Insiders 1.12.0-insider (aa05dac, 2017-05-02T00:04:13.700Z)
OS Version: Darwin x64 16.5.0
Extensions:




Extension
Author
Version




ng-template
Angular
0.1.3


code-settings-sync
Shan
2.6.2


sort-lines
Tyriar
1.2.0


Bookmarks
alefragnani
0.14.1


project-manager
alefragnani
0.15.1


path-intellisense
christian-kohler
1.2.0


vscode-eslint
dbaeumer
1.2.8


vscode-html-css
ecmel
0.1.2


tslint
eg2
0.12.0


Angular2
johnpapa
2.2.3


vscode-icon-theme
jtlowe
1.5.0


theme-karyfoundation-themes
karyfoundation
11.1.0


HTMLHint
mkaufman
0.3.3


vscode-autoprefixer
mrmlnc
2.0.0


vscode-stylefmt
mrmlnc
2.3.0


angular2-inline
natewallace
0.0.17


typescript-hero
rbbit
0.12.0


project-snippets
rebornix
0.5.0


stylelint
shinnn
0.24.0


darcula-extended
smlombardi
3.3.2


slime
smlombardi
1.15.0


theme-tesla
smlombardi
6.0.0


change-case
wmaurer
1.0.0




Steps to Reproduce:

in script tags, the opening angle bracket in closing </script> tags and no longer colored like other tag brackets:


for comparison, some non-script tags:

Furthermore, you can see that this font, Fira Code, uses ligatures for the </ yet the script tags no longer are using them.
Both things happened in the current build.",3
,2813,145,33,28203,"The C/C++ extension has been following the protocol here to define our protocol messages (the bulk of our extension is written in C++) https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md
I stumbled on this thread #2628 and saw commits that looked like they changed the enum values of CompletionItemKind and SymbolKind.  So it appears that we are sending incorrect values back to VS Code now.
I just wanted to double-check... are the enum values in VS Code now different than the ones in the protocol?",Update protocol document for CompletionItemKind and SymbolKind?,"Update protocol document for CompletionItemKind and SymbolKind?The C/C++ extension has been following the protocol here to define our protocol messages (the bulk of our extension is written in C++) https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md
I stumbled on this thread #2628 and saw commits that looked like they changed the enum values of CompletionItemKind and SymbolKind.  So it appears that we are sending incorrect values back to VS Code now.
I just wanted to double-check... are the enum values in VS Code now different than the ones in the protocol?",3
,2814,148,34,50004,"Issue Type: Bug
If you register a registerCompletionItemProvider in an exptension for PHP, it is not always called and completion items are not provided in the following cases:

Variable name starts with $

 arr_item = new ClassItem;
 a                         // Completion Item Provider called
$arr_item = new ClassItem;
$a                          // Completion Item Provider not called

Arrow functions

 arr_item = new ClassItem;
 arr_item.t          // Completion Item provider called for the keystroke t 
 arr_item = new ClassItem;
 arr_item->t          // Completion Item provider not called for the keystroke t 
NOTE: The completion item provider is not registered with any trigger characters.
VS Code version: Code 1.23.0 (7c7da59, 2018-05-03T15:23:14.634Z)
OS version: Darwin x64 17.5.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz (8 x 2800)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledvpx_decode: enabledwebgl: enabledwebgl2: enabled


Load (avg)
3, 3, 3


Memory (System)
16.00GB (0.02GB free)


Process Argv
/Applications/Visual Studio Code.app/Contents/MacOS/Electron -psn_0_10988154


Screen Reader
no


VM
0%



Extensions (37)



Extension
Author (truncated)
Version




MochaSnippets
Ala
0.0.1


All-Autocomplete
Ati
0.0.12


vscode-markdownlint
Dav
0.16.0


xml
Dot
1.9.2


EditorConfig
Edi
0.12.1


jquery-snippets
Hri
1.0.0


latex-workshop
Jam
5.4.0


vscode-jest
Ort
2.7.2


code-settings-sync
Sha
2.9.2


quokka-vscode
Wal
1.0.124


html-css-class-completion
Zig
1.17.1


jslint
ajh
1.2.1


copy-relative-path
ale
0.0.2


npm-intellisense
chr
1.3.0


gitignore
cod
0.5.0


riot-tag
cri
0.1.7


vscode-eslint
dba
1.4.9


tslint
eg2
1.0.28


vscode-npm-script
eg2
0.3.4


php-intellisense
fel
2.3.1


auto-close-tag
for
0.5.6


code-runner
for
0.9.3


ginfuru-vscode-jekyll-syntax
gin
0.0.5


docthis
joe
0.6.0


sublime-babel-vscode
jos
0.2.10


prettify-json
moh
0.0.3


extension-manifest-editor
ms-
0.1.5


python
ms-
2018.4.0


cpptools
ms-
0.17.0


sublime-keybindings
ms-
4.0.0


wordcount
ms-
0.1.0


debugger-for-chrome
msj
4.4.3


elm
sbr
0.17.0


code-spell-checker
str
1.6.10


html-preview-vscode
tht
0.1.1


cmake
twx
0.0.17


better-align
wwm
1.1.6",[PHP] provideCompletionItems is not called for items inside of -> methods and also for variables starting with $,"[PHP] provideCompletionItems is not called for items inside of -> methods and also for variables starting with $Issue Type: Bug
If you register a registerCompletionItemProvider in an exptension for PHP, it is not always called and completion items are not provided in the following cases:

Variable name starts with $

 arr_item = new ClassItem;
 a                         // Completion Item Provider called
$arr_item = new ClassItem;
$a                          // Completion Item Provider not called

Arrow functions

 arr_item = new ClassItem;
 arr_item.t          // Completion Item provider called for the keystroke t 
 arr_item = new ClassItem;
 arr_item->t          // Completion Item provider not called for the keystroke t 
NOTE: The completion item provider is not registered with any trigger characters.
VS Code version: Code 1.23.0 (7c7da59, 2018-05-03T15:23:14.634Z)
OS version: Darwin x64 17.5.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz (8 x 2800)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledvpx_decode: enabledwebgl: enabledwebgl2: enabled


Load (avg)
3, 3, 3


Memory (System)
16.00GB (0.02GB free)


Process Argv
/Applications/Visual Studio Code.app/Contents/MacOS/Electron -psn_0_10988154


Screen Reader
no


VM
0%



Extensions (37)



Extension
Author (truncated)
Version




MochaSnippets
Ala
0.0.1


All-Autocomplete
Ati
0.0.12


vscode-markdownlint
Dav
0.16.0


xml
Dot
1.9.2


EditorConfig
Edi
0.12.1


jquery-snippets
Hri
1.0.0


latex-workshop
Jam
5.4.0


vscode-jest
Ort
2.7.2


code-settings-sync
Sha
2.9.2


quokka-vscode
Wal
1.0.124


html-css-class-completion
Zig
1.17.1


jslint
ajh
1.2.1


copy-relative-path
ale
0.0.2


npm-intellisense
chr
1.3.0


gitignore
cod
0.5.0


riot-tag
cri
0.1.7


vscode-eslint
dba
1.4.9


tslint
eg2
1.0.28


vscode-npm-script
eg2
0.3.4


php-intellisense
fel
2.3.1


auto-close-tag
for
0.5.6


code-runner
for
0.9.3


ginfuru-vscode-jekyll-syntax
gin
0.0.5


docthis
joe
0.6.0


sublime-babel-vscode
jos
0.2.10


prettify-json
moh
0.0.3


extension-manifest-editor
ms-
0.1.5


python
ms-
2018.4.0


cpptools
ms-
0.17.0


sublime-keybindings
ms-
4.0.0


wordcount
ms-
0.1.0


debugger-for-chrome
msj
4.4.3


elm
sbr
0.17.0


code-spell-checker
str
1.6.10


html-preview-vscode
tht
0.1.1


cmake
twx
0.0.17


better-align
wwm
1.1.6",3
,2815,145,35,58429,"Issue Type: Bug
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-firstpass.csproj
[info]: OmniSharp.MSBuild.ProjectManager
Update project: NavMeshComponents
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor-firstpass.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\NavMeshComponentsEditor.csproj
VS Code version: Code 1.27.1 (5944e81, 2018-09-06T09:21:18.328Z)
OS version: Windows_NT x64 10.0.17134

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz (4 x 2394)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Memory (System)
15.91GB (7.76GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe C:\work\IDLE-CITY\Project -r -g C:\work\IDLE-CITY\Project\Assets\Scripts\MainController.cs:1


Screen Reader
no


VM
0%



Extensions (8)



Extension
Author (truncated)
Version




bracket-pair-colorizer
Coe
1.0.59


transformer
dak
1.6.0


json-tools
eri
1.0.2


python
ms-
2018.8.0


csharp
ms-
1.16.0


python
tht
0.2.3


unity-tools
Tob
1.0.5


unity-debug
Uni
1.3.0",UNITY PROJECT STOPS LOADED AFTER UPDATE,"UNITY PROJECT STOPS LOADED AFTER UPDATEIssue Type: Bug
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-firstpass.csproj
[info]: OmniSharp.MSBuild.ProjectManager
Update project: NavMeshComponents
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor-firstpass.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\NavMeshComponentsEditor.csproj
VS Code version: Code 1.27.1 (5944e81, 2018-09-06T09:21:18.328Z)
OS version: Windows_NT x64 10.0.17134

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz (4 x 2394)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Memory (System)
15.91GB (7.76GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe C:\work\IDLE-CITY\Project -r -g C:\work\IDLE-CITY\Project\Assets\Scripts\MainController.cs:1


Screen Reader
no


VM
0%



Extensions (8)



Extension
Author (truncated)
Version




bracket-pair-colorizer
Coe
1.0.59


transformer
dak
1.6.0


json-tools
eri
1.0.2


python
ms-
2018.8.0


csharp
ms-
1.16.0


python
tht
0.2.3


unity-tools
Tob
1.0.5


unity-debug
Uni
1.3.0",3
,2816,148,36,67571,"I have noticed a weird behavior, every time I create a new file, it is encoded in ANSI format, instead of the one I expect to have, which is ""files.encoding"": ""utf8""
Anybody has an idea on why vs is creating files on ANSI instead of UTF-8??
I noticed this problem when I was trying to load a js file, it said the first character was invalid, but the code was fine... then I checked into the encoding and it was ANSI... saved the file as UTF-8 using notepad and the problem went away... PLEASE HELP",Encoding Files Problem,"Encoding Files ProblemI have noticed a weird behavior, every time I create a new file, it is encoded in ANSI format, instead of the one I expect to have, which is ""files.encoding"": ""utf8""
Anybody has an idea on why vs is creating files on ANSI instead of UTF-8??
I noticed this problem when I was trying to load a js file, it said the first character was invalid, but the code was fine... then I checked into the encoding and it was ANSI... saved the file as UTF-8 using notepad and the problem went away... PLEASE HELP",3
,2817,148,37,52000,"Environment Details:
VSCode Version : 1.24.0
OS Version : Win10
Additional Details:
MAS Violated : MAS 2.1.1
Tools Used : Keyboard
Repro Steps:

Launch VS Code.
Navigate to Activity Bar and select ""Explorer""(Cntrl+Shift+E) button.
Navigate to ""File Explorer"" treeview items using ""Tab"".

Actual:
Keyboard focus moves to overall treeview items which is non-interactive. Then using downward arrow keys focus goes to items in treeview.
Expected:
Keyboard focus should not move to the items as a whole which is non-interactive. The focus should move to first item in the treeview.
Recommendations:
Remove tab-index from the outer  that contains all the treeview items.
or,
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
User Impact:
The keyboard only users will move to non-interactive elements on the screen which will be time consuming to reach only interactive elements on the page.
MAS Reference:
https://microsoft.sharepoint.com/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:
A11y_VSCode_ViewExplorer_Keyboard_TreeViewItems.pptx
Does this issue occur when all extensions are disabled?: Yes","Viewlets should focus the first item, not the whole tree","Viewlets should focus the first item, not the whole treeEnvironment Details:
VSCode Version : 1.24.0
OS Version : Win10
Additional Details:
MAS Violated : MAS 2.1.1
Tools Used : Keyboard
Repro Steps:

Launch VS Code.
Navigate to Activity Bar and select ""Explorer""(Cntrl+Shift+E) button.
Navigate to ""File Explorer"" treeview items using ""Tab"".

Actual:
Keyboard focus moves to overall treeview items which is non-interactive. Then using downward arrow keys focus goes to items in treeview.
Expected:
Keyboard focus should not move to the items as a whole which is non-interactive. The focus should move to first item in the treeview.
Recommendations:
Remove tab-index from the outer  that contains all the treeview items.
or,
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
User Impact:
The keyboard only users will move to non-interactive elements on the screen which will be time consuming to reach only interactive elements on the page.
MAS Reference:
https://microsoft.sharepoint.com/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:
A11y_VSCode_ViewExplorer_Keyboard_TreeViewItems.pptx
Does this issue occur when all extensions are disabled?: Yes",3
,2818,146,38,17849,"VSCode Version:1.8.0
OS Version:win 10

Steps to Reproduce:

In html file,when a tag has style property.It's will influence format

for example
when I press shift+alt+f to format code
<div>
    <p style=""font-size: 16px;"">hello world</p>
    <p>xxxholic</p>
</div>

will become
<div>
    <p style=""font-size: 15px;"">hello world</p>
<p>xxxholic</p>
</div>",[html] format not work correctly in html file when which has style property in tag,"[html] format not work correctly in html file when which has style property in tagVSCode Version:1.8.0
OS Version:win 10

Steps to Reproduce:

In html file,when a tag has style property.It's will influence format

for example
when I press shift+alt+f to format code
<div>
    <p style=""font-size: 16px;"">hello world</p>
    <p>xxxholic</p>
</div>

will become
<div>
    <p style=""font-size: 15px;"">hello world</p>
<p>xxxholic</p>
</div>",3
,2819,148,39,34176,"I ran into a case where a soft hyphen had snuck into a regex expression, yet the character was not visible in the editor. It doesn't seem to render in most places, including Stack Exchange; it has only shown up for me when pasted into a terminal window like cmd.exe or terminal.app. Full disclosure: I don't know if this is behaviour as intended or not. This could also be an Electron issue.


VSCode Version: 1.16.0
OS Version: Windows 10 Pro 15063.540

Steps to Reproduce:

Copy this regex expression using Ctrl+A: http://rubular.com/r/M58qyBfuxF (Link provided as it seems GitHub scrubs the soft hyphen before posting)
Paste it into Code, and verify that there is no hyphen visible.
Paste it into a terminal window and verify that there is a hyphen directly after the $


Reproduces without extensions: Yes
Also reproduces with both the default font and Roboto Mono",Editor not rendering soft hyphen,"Editor not rendering soft hyphenI ran into a case where a soft hyphen had snuck into a regex expression, yet the character was not visible in the editor. It doesn't seem to render in most places, including Stack Exchange; it has only shown up for me when pasted into a terminal window like cmd.exe or terminal.app. Full disclosure: I don't know if this is behaviour as intended or not. This could also be an Electron issue.


VSCode Version: 1.16.0
OS Version: Windows 10 Pro 15063.540

Steps to Reproduce:

Copy this regex expression using Ctrl+A: http://rubular.com/r/M58qyBfuxF (Link provided as it seems GitHub scrubs the soft hyphen before posting)
Paste it into Code, and verify that there is no hyphen visible.
Paste it into a terminal window and verify that there is a hyphen directly after the $


Reproduces without extensions: Yes
Also reproduces with both the default font and Roboto Mono",3
,2820,144,40,66866,"Click a file in the explorer to select
Focus the editor
Focus the explorer again with the keyboard
Press enter to rename file
The row is highlighted but the input box doesn't appear
Try renaming other rows, sometimes the input box appears, sometimes not

Sometimes the wrong text appears after pressing enter, not sure if that's a separate issue. That happens a couple times in the gif",Rename file in explorer styling issue,"Rename file in explorer styling issueClick a file in the explorer to select
Focus the editor
Focus the explorer again with the keyboard
Press enter to rename file
The row is highlighted but the input box doesn't appear
Try renaming other rows, sometimes the input box appears, sometimes not

Sometimes the wrong text appears after pressing enter, not sure if that's a separate issue. That happens a couple times in the gif",3
,2821,144,41,8167,"When opening the output panel:
shell.js:248 Cannot read property 'getWorkspace' of undefined: TypeError: Cannot read property 'getWorkspace' of undefined
    at new OutputWorker (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/workbench/parts/output/common/outputWorker.js:25:49)
    at create (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/base/common/types.js:160:14)
    at InstantiationService._createInstance (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:141:43)
    at file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:96:33
    at Module._invokeFactory (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:755:52)
    at Module._complete (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:776:34)
    at Module.resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:831:22)
    at ModuleManager._resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1421:19)
    at ModuleManager._resolve (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1519:30)
    at ModuleManager.defineModule (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1027:18)",OutputWorker: Cannot read property 'getWorkspace' of undefined,"OutputWorker: Cannot read property 'getWorkspace' of undefinedWhen opening the output panel:
shell.js:248 Cannot read property 'getWorkspace' of undefined: TypeError: Cannot read property 'getWorkspace' of undefined
    at new OutputWorker (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/workbench/parts/output/common/outputWorker.js:25:49)
    at create (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/base/common/types.js:160:14)
    at InstantiationService._createInstance (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:141:43)
    at file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:96:33
    at Module._invokeFactory (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:755:52)
    at Module._complete (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:776:34)
    at Module.resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:831:22)
    at ModuleManager._resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1421:19)
    at ModuleManager._resolve (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1519:30)
    at ModuleManager.defineModule (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1027:18)",3
,2822,142,42,16489,We could remove that extra line between sections.,Empty line between settings sections,Empty line between settings sectionsWe could remove that extra line between sections.,3
,2823,148,43,67126,"Issue Type: Bug
I have this weird issue where AutoImport won't work for NPM packages when using JS or JSX. However it DOES work for:

Local files
when using Typescript instead of JS

I haven't found any issue quite like this, so I finally decided to ask for clues. Any hints at how to fix this would be greatly appreciated.
VS Code version: Code 1.30.2 (61122f8, 2019-01-07T22:48:31.260Z)
OS version: Darwin x64 18.2.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz (12 x 2200)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Load (avg)
2, 2, 2


Memory (System)
16.00GB (1.47GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (39)



Extension
Author (truncated)
Version




scss-lint
ada
0.1.8


rails-partial
aki
0.1.0


jest-snippets
and
1.6.1


ng-template
Ang
0.1.11


rails
bun
0.8.6


vscode-gemfile
bun
0.0.2


solargraph
cas
0.19.1


npm-intellisense
chr
1.3.0


bracket-pair-colorizer
Coe
1.0.61


ionic3-vs-ionView-snippets
dan
1.0.2


vscode-markdownlint
Dav
0.23.0


vscode-eslint
dba
1.8.0


tslint
eg2
1.0.42


vsc-material-theme
Equ
2.6.3


prettier-vscode
esb
1.7.3


todo-tree
Gru
0.0.115


beautify
Hoo
1.4.7


RelativePath
jak
1.4.0


vscode-styled-components
jpo
0.0.25


rspec-snippets
kar
0.0.4


coffeelinter
lky
1.4.0


vscode-language-babel
mgm
0.0.21


dotenv
mik
1.0.1


ruby-rubocop
mis
0.7.1


vscode-elixir
mjm
1.1.0


vscode-postcss-sorting
mrm
3.0.1


vscode-scss
mrm
0.6.2


atom-keybindings
ms-
3.0.5


debugger-for-chrome
msj
4.11.1


color-highlight
nau
2.3.0


vscode-docker
Pet
0.5.1


material-icon-theme
PKi
3.6.2


ruby
reb
0.21.0


slim
sia
0.1.2


addDocComments
ste
0.0.8


highlight-matching-tag
vin
0.8.6


simple-ruby-erb
vor
0.2.1


vscode-js-import
wan
0.15.4


change-case
wma
1.0.0","AutoImport not working for NPM packages, only in JS/JSX","AutoImport not working for NPM packages, only in JS/JSXIssue Type: Bug
I have this weird issue where AutoImport won't work for NPM packages when using JS or JSX. However it DOES work for:

Local files
when using Typescript instead of JS

I haven't found any issue quite like this, so I finally decided to ask for clues. Any hints at how to fix this would be greatly appreciated.
VS Code version: Code 1.30.2 (61122f8, 2019-01-07T22:48:31.260Z)
OS version: Darwin x64 18.2.0

System Info



Item
Value




CPUs
Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz (12 x 2200)


GPU Status
2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled


Load (avg)
2, 2, 2


Memory (System)
16.00GB (1.47GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (39)



Extension
Author (truncated)
Version




scss-lint
ada
0.1.8


rails-partial
aki
0.1.0


jest-snippets
and
1.6.1


ng-template
Ang
0.1.11


rails
bun
0.8.6


vscode-gemfile
bun
0.0.2


solargraph
cas
0.19.1


npm-intellisense
chr
1.3.0


bracket-pair-colorizer
Coe
1.0.61


ionic3-vs-ionView-snippets
dan
1.0.2


vscode-markdownlint
Dav
0.23.0


vscode-eslint
dba
1.8.0


tslint
eg2
1.0.42


vsc-material-theme
Equ
2.6.3


prettier-vscode
esb
1.7.3


todo-tree
Gru
0.0.115


beautify
Hoo
1.4.7


RelativePath
jak
1.4.0


vscode-styled-components
jpo
0.0.25


rspec-snippets
kar
0.0.4


coffeelinter
lky
1.4.0


vscode-language-babel
mgm
0.0.21


dotenv
mik
1.0.1


ruby-rubocop
mis
0.7.1


vscode-elixir
mjm
1.1.0


vscode-postcss-sorting
mrm
3.0.1


vscode-scss
mrm
0.6.2


atom-keybindings
ms-
3.0.5


debugger-for-chrome
msj
4.11.1


color-highlight
nau
2.3.0


vscode-docker
Pet
0.5.1


material-icon-theme
PKi
3.6.2


ruby
reb
0.21.0


slim
sia
0.1.2


addDocComments
ste
0.0.8


highlight-matching-tag
vin
0.8.6


simple-ruby-erb
vor
0.2.1


vscode-js-import
wan
0.15.4


change-case
wma
1.0.0",3
,2824,146,44,23459,"This is not a bug
It is more an enhancement
Thing is when I close a tab (file), file tree changes it focus automatically to the now active file; this is really annoying because many times I just open and close many files (as I'm sure other devs do) and I have to scroll all ways down/up again! Hope you improve this.
Note: Sublime Text doesn't automatically focus the file tree after closing a file.",File Tree Automatically Focus when closing a file,"File Tree Automatically Focus when closing a fileThis is not a bug
It is more an enhancement
Thing is when I close a tab (file), file tree changes it focus automatically to the now active file; this is really annoying because many times I just open and close many files (as I'm sure other devs do) and I have to scroll all ways down/up again! Hope you improve this.
Note: Sublime Text doesn't automatically focus the file tree after closing a file.",3
,2825,148,45,34204,"Problem with syntax highlighting as reported below:


VSCode Version: 1.16.0 (1.16.0)
OS Version: Mac OSX Sierra 10.12.5

Steps to Reproduce:

Write HTML code
Insert multiline pieces of PHP code
See that the syntax doesn't recognise the open/close php tag",Problem with syntax highlighting (HTML/PHP),"Problem with syntax highlighting (HTML/PHP)Problem with syntax highlighting as reported below:


VSCode Version: 1.16.0 (1.16.0)
OS Version: Mac OSX Sierra 10.12.5

Steps to Reproduce:

Write HTML code
Insert multiline pieces of PHP code
See that the syntax doesn't recognise the open/close php tag",3
,2826,142,46,24249,"Version 1.11 changed the default action when you clicked on the branch name on the lower left from opening an input with git checkout already filled to an input for the git checkout command itself.  The previous pre-filled input was nice in that you could replace ""checkout"" with ""branch"" and create a new branch.
Instead of cluttering the new popup ui with a branch option you could add a context menu to the branch name in the lower left corner that would contain branch and optionally other common commands which you can access now in the ellipsis menu in the git pane (note: branch is not available right now in the git pane ellipsis menu).",Feature request: Add context menu to branch name in lower left,"Feature request: Add context menu to branch name in lower leftVersion 1.11 changed the default action when you clicked on the branch name on the lower left from opening an input with git checkout already filled to an input for the git checkout command itself.  The previous pre-filled input was nice in that you could replace ""checkout"" with ""branch"" and create a new branch.
Instead of cluttering the new popup ui with a branch option you could add a context menu to the branch name in the lower left corner that would contain branch and optionally other common commands which you can access now in the ellipsis menu in the git pane (note: branch is not available right now in the git pane ellipsis menu).",3
,2827,144,47,32716,"VSCode Version: Code 1.15.1 (41abd21, 2017-08-16T17:15:57.756Z)
OS Version: Darwin x64 16.7.0
Extensions:




Extension
Author (truncated)
Version




markdown-toc
Ala
1.5.6


xml
Dot
1.9.2


EditorConfig
Edi
0.9.4


material-icon-theme
PKi
2.1.0


vscode-docker
Pet
0.0.16


code-settings-sync
Sha
2.8.2


html-css-class-completion
Zig
1.8.0


hugofy
akm
0.1.0


vscode-color
ans
0.4.5


cform
aws
0.0.10


better-toml
bun
0.2.0


path-intellisense
chr
1.4.2


gitignore
cod
0.5.0


typewriter
dan
1.0.1


vscode-eslint
dba
1.2.11


githistory
don
0.2.3


gitlens
eam
4.3.3


vscode-npm-script
eg2
0.2.0


lambda-snippets
log
0.2.0


Go
luk
0.6.63


prettify-json
moh
0.0.3


debugger-for-chrome
msj
3.1.8


vscode-icons
rob
7.12.0


gitblame
wad
2.1.0



(3 theme extensions excluded)

Steps to Reproduce:

Line 1975 and 2061 are duplicates.
Does NOT repro with extensions disabled.


Reproduces without extensions: No","Duplicate setting ""npm.runSilent"" in vscode://defaultsettings/settings.json","Duplicate setting ""npm.runSilent"" in vscode://defaultsettings/settings.jsonVSCode Version: Code 1.15.1 (41abd21, 2017-08-16T17:15:57.756Z)
OS Version: Darwin x64 16.7.0
Extensions:




Extension
Author (truncated)
Version




markdown-toc
Ala
1.5.6


xml
Dot
1.9.2


EditorConfig
Edi
0.9.4


material-icon-theme
PKi
2.1.0


vscode-docker
Pet
0.0.16


code-settings-sync
Sha
2.8.2


html-css-class-completion
Zig
1.8.0


hugofy
akm
0.1.0


vscode-color
ans
0.4.5


cform
aws
0.0.10


better-toml
bun
0.2.0


path-intellisense
chr
1.4.2


gitignore
cod
0.5.0


typewriter
dan
1.0.1


vscode-eslint
dba
1.2.11


githistory
don
0.2.3


gitlens
eam
4.3.3


vscode-npm-script
eg2
0.2.0


lambda-snippets
log
0.2.0


Go
luk
0.6.63


prettify-json
moh
0.0.3


debugger-for-chrome
msj
3.1.8


vscode-icons
rob
7.12.0


gitblame
wad
2.1.0



(3 theme extensions excluded)

Steps to Reproduce:

Line 1975 and 2061 are duplicates.
Does NOT repro with extensions disabled.


Reproduces without extensions: No",3
,2828,145,48,58987,"VSCode Version: 1.27.2
OS Version: Win 10
Typescript: 3.0.3

Steps to Reproduce:

Set update imports on move typescript setting to prompt
Create an empty project with 2 typescript files, one importing from the other.
In the explorer manually move the depended upon file to a new directory. You will be prompted to update the import. This works fine.
Now move that directory into ANOTHER new directory.
This time there is no prompt and the import will be broken",moving TypeScript files in explorer fails to update/prompt imports when containing folder is moved,"moving TypeScript files in explorer fails to update/prompt imports when containing folder is movedVSCode Version: 1.27.2
OS Version: Win 10
Typescript: 3.0.3

Steps to Reproduce:

Set update imports on move typescript setting to prompt
Create an empty project with 2 typescript files, one importing from the other.
In the explorer manually move the depended upon file to a new directory. You will be prompted to update the import. This works fine.
Now move that directory into ANOTHER new directory.
This time there is no prompt and the import will be broken",3
,2829,148,49,9764,"Testing #9698:

enable tab completion: ""editor.tabCompletion"": true
create a html snippet

     ""Div"": {
        ""prefix"": ""div"",
        ""body"": [
            ""<div>"",
            ""    ${}"",
            ""</div>""
        ],
        ""description"": ""New div""
    }

in a html file have a multiple lines with content 'div'. Set multiple cursors after each div. Press tab.
  primary div is expanded
do the same with multiple lines of span, press tab
  tab character is added at each of the cursors

Not sure if the emmet behaviour is intentional.
It would be cool if the expansion happens on every cursor.",[tab completion] multi cursor behaviour different to emmet,"[tab completion] multi cursor behaviour different to emmetTesting #9698:

enable tab completion: ""editor.tabCompletion"": true
create a html snippet

     ""Div"": {
        ""prefix"": ""div"",
        ""body"": [
            ""<div>"",
            ""    ${}"",
            ""</div>""
        ],
        ""description"": ""New div""
    }

in a html file have a multiple lines with content 'div'. Set multiple cursors after each div. Press tab.
  primary div is expanded
do the same with multiple lines of span, press tab
  tab character is added at each of the cursors

Not sure if the emmet behaviour is intentional.
It would be cool if the expansion happens on every cursor.",3
,2830,146,50,57704,"When scrolling through a file, the view is not fluid. It jumps, skips, pauses and is not a constant view as I move up or down the file.
Version: July 2018 (version 1.26)",Scrolling,"ScrollingWhen scrolling through a file, the view is not fluid. It jumps, skips, pauses and is not a constant view as I move up or down the file.
Version: July 2018 (version 1.26)",3
,2831,146,51,8633,"VSCode Version: 1.2.1
OS Version: 10.11.5 (15F34) (El Capitan)

Steps to Reproduce:

Open a file in the editor
Open the integrated terminal
Type some commands on the terminal
Try to copy some text on the editor by highlighting and typing Mac + c
You will have to click on the section twice before the text successfully copies",Mac OS - Highlighting text in the editor requires two clicks when the integrated terminal is open,"Mac OS - Highlighting text in the editor requires two clicks when the integrated terminal is openVSCode Version: 1.2.1
OS Version: 10.11.5 (15F34) (El Capitan)

Steps to Reproduce:

Open a file in the editor
Open the integrated terminal
Type some commands on the terminal
Try to copy some text on the editor by highlighting and typing Mac + c
You will have to click on the section twice before the text successfully copies",3
,2832,142,52,56153,"Please natively support opening a file in the browser. LIke a right click menu option. The extensions don't work well especially in chrome.
thanks",open in broswer,"open in broswerPlease natively support opening a file in the browser. LIke a right click menu option. The extensions don't work well especially in chrome.
thanks",3
,2833,143,53,56234,"VSCode Version: 1.25.1
OS Version: 10.13.5

I've added the following view to Todo+:

I've been benchmarking it against magento2, which contains about 30k files and 2M lines of code. If the user has ag installed in his system the process of finding those todos is quite fast, its takes about 4s to do that on my laptop.
Quite surprisingly though the bulk of the time is spent on these lines, which are executed about 1k times, since the extension creates about 1k TreeItems under these circumstances:



Adding the tooltips requires about 500 extra milliseconds, I would expect that number to be near 0ms, I see basically nothing going on here that can justify that number.


Adding the commands requires about 4 extra seconds. 4 seconds for doing what? Creating those objects should be almost free, and until those TreeItems get clicked they don't even change anything as far as the user is concerned, I think.


Adding the icons requires about 14 extra seconds.  There are less than 10 different images loaded, but for some reason loading them a few hundred times is that slow.",TreeItem is too slow,"TreeItem is too slowVSCode Version: 1.25.1
OS Version: 10.13.5

I've added the following view to Todo+:

I've been benchmarking it against magento2, which contains about 30k files and 2M lines of code. If the user has ag installed in his system the process of finding those todos is quite fast, its takes about 4s to do that on my laptop.
Quite surprisingly though the bulk of the time is spent on these lines, which are executed about 1k times, since the extension creates about 1k TreeItems under these circumstances:



Adding the tooltips requires about 500 extra milliseconds, I would expect that number to be near 0ms, I see basically nothing going on here that can justify that number.


Adding the commands requires about 4 extra seconds. 4 seconds for doing what? Creating those objects should be almost free, and until those TreeItems get clicked they don't even change anything as far as the user is concerned, I think.


Adding the icons requires about 14 extra seconds.  There are less than 10 different images loaded, but for some reason loading them a few hundred times is that slow.",3
,2834,148,54,5876,"VSCode Version:1.0.0
OS Version: Windows 7

I'm using desktops.exe from SysInternals to add virtual desktops to Windows 7.
When having an open instance of code in a not active desktop, starting an additional instance will not start on the currently active desktop.
Steps to Reproduce:

Open VSCode in desktop 1
Switch to desktop 2
Start another VSCode instance: code .. The new instance starts in desktop 1",Additional instance starts in wrong virtual desktop,"Additional instance starts in wrong virtual desktopVSCode Version:1.0.0
OS Version: Windows 7

I'm using desktops.exe from SysInternals to add virtual desktops to Windows 7.
When having an open instance of code in a not active desktop, starting an additional instance will not start on the currently active desktop.
Steps to Reproduce:

Open VSCode in desktop 1
Switch to desktop 2
Start another VSCode instance: code .. The new instance starts in desktop 1",3
,2835,148,55,14679,"From #13835 (comment),
When typing quickly, I'm selecting packages by initials (or something), but when I type slowly, I'm selecting packages by prefix:

Open package.json
Put the cursor in ""dependencies""
ctrl+space, then esc (It doesn't repro unless the intellisense window has been shown on this line)
type blu quickly
The intellisense window appears again and I expect to see suggestions for all packages that match. Instead I only see ""blackbaud-npi-datamart-ux"", with the 'blu' highlighted. It's always this one package.

If you type blu more slowly, then it shows 'blu', 'blu-css', 'blu-generator' ...
Another example -
Typing ""blah"" quickly

Slowly:

The first type of fuzzy matching would be useful but it should be more consistent.",Intellisense in package.json shows packages inconsistently,"Intellisense in package.json shows packages inconsistentlyFrom #13835 (comment),
When typing quickly, I'm selecting packages by initials (or something), but when I type slowly, I'm selecting packages by prefix:

Open package.json
Put the cursor in ""dependencies""
ctrl+space, then esc (It doesn't repro unless the intellisense window has been shown on this line)
type blu quickly
The intellisense window appears again and I expect to see suggestions for all packages that match. Instead I only see ""blackbaud-npi-datamart-ux"", with the 'blu' highlighted. It's always this one package.

If you type blu more slowly, then it shows 'blu', 'blu-css', 'blu-generator' ...
Another example -
Typing ""blah"" quickly

Slowly:

The first type of fuzzy matching would be useful but it should be more consistent.",3
,2836,144,56,2470,Issue Id: 3c0f493a-37c5-196b-ce04-755343aba4dbVersions - 0.10.6-release-  dfc08dcStack SyntaxError: Unexpected end of input    at Object.parse (native)[/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 (V8Protocol.dispatch)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 %28V8Protocol.dispatch%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 (V8Protocol.handleData)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 %28V8Protocol.handleData%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 (V8Protocol.connect)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 %28V8Protocol.connect%29)    at emitOne (events.js:77:13)    at Socket.emit (events.js:169:7)    at readableAddChunk (_stream_readable.js:146:16)    at Socket.Readable.push (_stream_readable.js:110:10)    at Pipe.onread (net.js:523:20), Uncaught SyntaxError: Unexpected end of input,Uncaught SyntaxError: Unexpected end of inputIssue Id: 3c0f493a-37c5-196b-ce04-755343aba4dbVersions - 0.10.6-release-  dfc08dcStack SyntaxError: Unexpected end of input    at Object.parse (native)[/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 (V8Protocol.dispatch)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 %28V8Protocol.dispatch%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 (V8Protocol.handleData)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 %28V8Protocol.handleData%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 (V8Protocol.connect)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 %28V8Protocol.connect%29)    at emitOne (events.js:77:13)    at Socket.emit (events.js:169:7)    at readableAddChunk (_stream_readable.js:146:16)    at Socket.Readable.push (_stream_readable.js:110:10)    at Pipe.onread (net.js:523:20),3
,2837,148,57,48410,"VSCode Version: Version 1.22.2 (1.22.2)
OS Version: Mac OS 10.11.6

Steps to Reproduce:

Make two identically named files in different subfolders of a project.
Set   ""workbench.editor.labelFormat"": ""default"" in settings.
Open both files. See path info that differentiates them.
Split the window and move one over. No longer see path info that differentiates them.

Desired behavior:
I should still see path info that differentiates them, since there are two different files open with the same title and the goal of ""default"" is to be able to differentiate them by path.

Does this issue occur when all extensions are disabled?: Yes/No
Yes.","""workbench.editor.labelFormat"": ""default"" should show paths with split window","""workbench.editor.labelFormat"": ""default"" should show paths with split windowVSCode Version: Version 1.22.2 (1.22.2)
OS Version: Mac OS 10.11.6

Steps to Reproduce:

Make two identically named files in different subfolders of a project.
Set   ""workbench.editor.labelFormat"": ""default"" in settings.
Open both files. See path info that differentiates them.
Split the window and move one over. No longer see path info that differentiates them.

Desired behavior:
I should still see path info that differentiates them, since there are two different files open with the same title and the goal of ""default"" is to be able to differentiate them by path.

Does this issue occur when all extensions are disabled?: Yes/No
Yes.",3
,2838,145,58,56364,"Hello!
After installing VSCode 1.26, the GUI is almost completely unresponsive when I open my project. After a few minutes, it can be used. However, the CPU usage will remain at 24% (i have 4 cores). Even after the project or folder is closed.
If I open VSCode without a project or folder, it behaves.
VSCode is responsive if launched with --disable-extensions.
Could it be an extentions that misbehaves?
Here is a --status dump:

Version:          Code 1.26.0 (4e93618, 2018-08-13T16:29:31.933Z)
OS Version:       Windows_NT x64 10.0.16299
CPUs:             Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz (8 x 2712)
Memory (System):  15.85GB (3.24GB free)
VM:               0%
Screen Reader:    no
Process Argv:     C:\Users\forsbdan\AppData\Local\Programs\Microsoft VS Code\Code.exe
GPU Status:       2d_canvas:                    enabled
checker_imaging:              disabled_off
flash_3d:                     enabled
flash_stage3d:                enabled
flash_stage3d_baseline:       enabled
gpu_compositing:              enabled
multiple_raster_threads:      enabled_on
native_gpu_memory_buffers:    disabled_software
rasterization:                enabled
video_decode:                 enabled
video_encode:                 enabled
webgl:                        enabled
webgl2:                       enabled
CPU %   Mem MB     PID  Process
0      100   34680  code main
0       84   22972     shared-process
0      146   23460     gpu-process
23     1271   28764     window (Program.cs - demo2 - Visual Studio Code)
0        6   26188       winpty-process
0       65   16204         C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe
0       10   19472         console-window-host (Windows internal process)
0       51   30296       searchService
0       14   31020       electron-crash-reporter
0       11   31384       watcherService
0       10   21716         console-window-host (Windows internal process)
0      191   33784       extensionHost
0       38   22568         searchService
0        3   37816         cmd /s /c ""C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4""
0       10    7140           console-window-host (Windows internal process)
0      115   19888           C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe  -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4
Workspace Stats:
|  Window (Program.cs - demo2 - Visual Studio Code)
|    Folder (demo2): 83 files
|      File types: json(15) cshtml(12) cs(8) js(5) ts(5) scss(5) cache(5)
|                  map(3) vue(3) txt(2)
|      Conf files: launch.json(2) tasks.json(2) sln(1) csproj(1)
|                  package.json(1) tsconfig.json(1) webpack.config.js(1)
|                  settings.json(1)
|      Launch Configs: coreclr(2)",Unresponsive VSCode after upgrade to 1.26.,"Unresponsive VSCode after upgrade to 1.26.Hello!
After installing VSCode 1.26, the GUI is almost completely unresponsive when I open my project. After a few minutes, it can be used. However, the CPU usage will remain at 24% (i have 4 cores). Even after the project or folder is closed.
If I open VSCode without a project or folder, it behaves.
VSCode is responsive if launched with --disable-extensions.
Could it be an extentions that misbehaves?
Here is a --status dump:

Version:          Code 1.26.0 (4e93618, 2018-08-13T16:29:31.933Z)
OS Version:       Windows_NT x64 10.0.16299
CPUs:             Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz (8 x 2712)
Memory (System):  15.85GB (3.24GB free)
VM:               0%
Screen Reader:    no
Process Argv:     C:\Users\forsbdan\AppData\Local\Programs\Microsoft VS Code\Code.exe
GPU Status:       2d_canvas:                    enabled
checker_imaging:              disabled_off
flash_3d:                     enabled
flash_stage3d:                enabled
flash_stage3d_baseline:       enabled
gpu_compositing:              enabled
multiple_raster_threads:      enabled_on
native_gpu_memory_buffers:    disabled_software
rasterization:                enabled
video_decode:                 enabled
video_encode:                 enabled
webgl:                        enabled
webgl2:                       enabled
CPU %   Mem MB     PID  Process
0      100   34680  code main
0       84   22972     shared-process
0      146   23460     gpu-process
23     1271   28764     window (Program.cs - demo2 - Visual Studio Code)
0        6   26188       winpty-process
0       65   16204         C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe
0       10   19472         console-window-host (Windows internal process)
0       51   30296       searchService
0       14   31020       electron-crash-reporter
0       11   31384       watcherService
0       10   21716         console-window-host (Windows internal process)
0      191   33784       extensionHost
0       38   22568         searchService
0        3   37816         cmd /s /c ""C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4""
0       10    7140           console-window-host (Windows internal process)
0      115   19888           C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe  -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4
Workspace Stats:
|  Window (Program.cs - demo2 - Visual Studio Code)
|    Folder (demo2): 83 files
|      File types: json(15) cshtml(12) cs(8) js(5) ts(5) scss(5) cache(5)
|                  map(3) vue(3) txt(2)
|      Conf files: launch.json(2) tasks.json(2) sln(1) csproj(1)
|                  package.json(1) tsconfig.json(1) webpack.config.js(1)
|                  settings.json(1)
|      Launch Configs: coreclr(2)",3
,2839,144,59,46332,"Issue Type: Bug
create dummy text1
do block mode edit inside text1
create more dummy text, preferably so much you dont see text1 any more
ctrl + z
VS Code version: Code 1.21.1 (79b44aa, 2018-03-14T14:46:47.128Z)
OS version: Windows_NT x64 10.0.15063

Extensions (4)



Extension
Author (truncated)
Version




vscode-eslint
dba
1.4.7


gc-excelviewer
Gra
2.0.20


python
ms-
2018.2.1


vscode-docker
Pet
0.0.25




Reproduces only with extensions",ctrl+z/clrl+y after block mode edit does not follow the (multi-line) cursor to follow the undo/redo steps,"ctrl+z/clrl+y after block mode edit does not follow the (multi-line) cursor to follow the undo/redo stepsIssue Type: Bug
create dummy text1
do block mode edit inside text1
create more dummy text, preferably so much you dont see text1 any more
ctrl + z
VS Code version: Code 1.21.1 (79b44aa, 2018-03-14T14:46:47.128Z)
OS version: Windows_NT x64 10.0.15063

Extensions (4)



Extension
Author (truncated)
Version




vscode-eslint
dba
1.4.7


gc-excelviewer
Gra
2.0.20


python
ms-
2018.2.1


vscode-docker
Pet
0.0.25




Reproduces only with extensions",3
,2840,148,60,4886,https://travis-ci.org/Microsoft/vscode/jobs/120206982,HTML tests are failing,HTML tests are failinghttps://travis-ci.org/Microsoft/vscode/jobs/120206982,3
,2841,144,61,72813,"Open settings editor
Tab away
Tab back to it",Settings editor crashes when restoring,"Settings editor crashes when restoringOpen settings editor
Tab away
Tab back to it",3
,2842,146,62,70591,"Issue Type: Feature Request
Decent editors provide a drop-down list of older search phrases in their search (and replace!) text entry UI elements.
But VSCode doesn't seem to do this, for some reason.
It's only polite to remember such things for the comfy user, you know :)
VS Code version: Code 1.32.3 (a3db5be, 2019-03-14T23:43:35.476Z)
OS version: Windows_NT x64 10.0.18348","VSCode should do like any decent editor, and provide a drop-down from all search text boxes","VSCode should do like any decent editor, and provide a drop-down from all search text boxesIssue Type: Feature Request
Decent editors provide a drop-down list of older search phrases in their search (and replace!) text entry UI elements.
But VSCode doesn't seem to do this, for some reason.
It's only polite to remember such things for the comfy user, you know :)
VS Code version: Code 1.32.3 (a3db5be, 2019-03-14T23:43:35.476Z)
OS version: Windows_NT x64 10.0.18348",3
,2843,148,63,65327,"Please reopen either #21518 or #18782 . The bug still repros with C/C++.
void foo(int, const char*, const char *) { // also repros when the params are subsets, e.g. a, aa, aaa
}
int main()
{
foo(a, ) // signature Help selects the the 3rd param instead of the 2nd.
}",Reopen issue 21518 or 18782,"Reopen issue 21518 or 18782Please reopen either #21518 or #18782 . The bug still repros with C/C++.
void foo(int, const char*, const char *) { // also repros when the params are subsets, e.g. a, aa, aaa
}
int main()
{
foo(a, ) // signature Help selects the the 3rd param instead of the 2nd.
}",3
,2844,148,64,81237,"VSCode Version: 1.38.1
OS Version: macOS 10.14.4

Steps to Reproduce:

Specify ""problemMatcher"": [] at top level of tasks.json object
Run a task (i.e. an npm task) that has no task config
VS Code still prompts you for how to scan the task output


Does this issue occur when all extensions are disabled?: No
Other comments
Essentially, I want to be able to disable the problem matcher by default. Personally, I never use it, and it's just noise, because every time I run a task I haven't ran before, I have to select Never scan the task output, which opens up tasks.json and adds a new config for that task, just so it can specifiy ""problemMatcher"": [].
Related issues

#43003

This issue was closed, but I still have the problem",Cannot specify global problemMatcher in tasks.json,"Cannot specify global problemMatcher in tasks.jsonVSCode Version: 1.38.1
OS Version: macOS 10.14.4

Steps to Reproduce:

Specify ""problemMatcher"": [] at top level of tasks.json object
Run a task (i.e. an npm task) that has no task config
VS Code still prompts you for how to scan the task output


Does this issue occur when all extensions are disabled?: No
Other comments
Essentially, I want to be able to disable the problem matcher by default. Personally, I never use it, and it's just noise, because every time I run a task I haven't ran before, I have to select Never scan the task output, which opens up tasks.json and adds a new config for that task, just so it can specifiy ""problemMatcher"": [].
Related issues

#43003

This issue was closed, but I still have the problem",3
,2845,148,65,4391,"VSCode Version:alpha 0.10.12 commit 05e203c
OS Version:Windows 7

Steps to Reproduce:

Try Ctrl+Shift+Alt+arrow keys to ""draw"" a rectangle.
Nothing happens

Shift+Alt and mouse works correctly.",Can't do column selection with keyboard over Windows remote desktop,"Can't do column selection with keyboard over Windows remote desktopVSCode Version:alpha 0.10.12 commit 05e203c
OS Version:Windows 7

Steps to Reproduce:

Try Ctrl+Shift+Alt+arrow keys to ""draw"" a rectangle.
Nothing happens

Shift+Alt and mouse works correctly.",3
,2846,145,66,22970,"This issue appears to have come back (or some variation of it) #881

VSCode Version:1.10.2 with arm tools 0.3.4
OS Version: Windows 10",ARM template type not accepted even though valid.,"ARM template type not accepted even though valid.This issue appears to have come back (or some variation of it) #881

VSCode Version:1.10.2 with arm tools 0.3.4
OS Version: Windows 10",3
,2847,148,67,47566,"Found while translating to Hungarian. The message is ""Unknown error while"" in
src/vs/platform/extensionManagement/node/extensionManagementService.ts.
It does not make any sense, the end is missing. Added by @sandy081 in db7ddfd.",Nonsensical message in extensionManagementService.ts,"Nonsensical message in extensionManagementService.tsFound while translating to Hungarian. The message is ""Unknown error while"" in
src/vs/platform/extensionManagement/node/extensionManagementService.ts.
It does not make any sense, the end is missing. Added by @sandy081 in db7ddfd.",3
,2848,148,68,83854,"Issue Type: Bug
I am not able to run my code in Visual Studio. It is repeatedly showing that  include errors are detected and to update my includePath for which I have already installed vcpkg from GitHub , still the issue is not yet solved.
VS Code version: Code 1.39.2 (6ab5985, 2019-10-15T15:35:18.241Z)
OS version: Windows_NT x64 10.0.18362

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz (8 x 1800)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
undefined


Memory (System)
7.90GB (3.42GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (2)



Extension
Author (truncated)
Version




code-runner
for
0.9.14


cpptools
ms-
0.26.1",I am not able to run my code.,"I am not able to run my code.Issue Type: Bug
I am not able to run my code in Visual Studio. It is repeatedly showing that  include errors are detected and to update my includePath for which I have already installed vcpkg from GitHub , still the issue is not yet solved.
VS Code version: Code 1.39.2 (6ab5985, 2019-10-15T15:35:18.241Z)
OS version: Windows_NT x64 10.0.18362

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz (8 x 1800)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
undefined


Memory (System)
7.90GB (3.42GB free)


Process Argv



Screen Reader
no


VM
0%



Extensions (2)



Extension
Author (truncated)
Version




code-runner
for
0.9.14


cpptools
ms-
0.26.1",3
,2849,146,69,17371,Pretty self explanatory. I don't have anything against the new UI or anything but I don't like that I can no longer see the default settings in a single plain text file.,Add option to view default settings in single plain text file,Add option to view default settings in single plain text filePretty self explanatory. I don't have anything against the new UI or anything but I don't like that I can no longer see the default settings in a single plain text file.,3
,2850,142,70,66308,"Currently, when I press Ctrl+Shift+B and click ""Configure Build Task..."" then ""Create tasks.json from template"", the only options are MSBuild, maven, .NET Core, and Others.
I would like to be able to easily create a build task template for my Mono projects. .NET Core does not have the full feature-set of Mono or .NET Framework yet, and even if it did, I still have existing projects using Mono that I'd like to edit with VS Code. Please add this feature!",Allow extensions to contribute build task types,"Allow extensions to contribute build task typesCurrently, when I press Ctrl+Shift+B and click ""Configure Build Task..."" then ""Create tasks.json from template"", the only options are MSBuild, maven, .NET Core, and Others.
I would like to be able to easily create a build task template for my Mono projects. .NET Core does not have the full feature-set of Mono or .NET Framework yet, and even if it did, I still have existing projects using Mono that I'd like to edit with VS Code. Please add this feature!",3
,2851,148,71,5019,Issue Id: def759c3-62fe-b6f1-ff32-27679721a3ffVersions - 0.10.8-  f291f4a-  43ff6af-  5b5f4db-  17fa1cbStack Error: spawn osascript ENOENT    at exports._errnoException (util.js:837:11)    at Process.ChildProcess._handle.onexit (internal/child_process.js:178:32)     at onErrorNT (internal/child_process.js:344:16)     at doNTCallback2 (node.js:442:9)    at process._tickCallback (node.js:356:17), spawn osascript ENOENT,spawn osascript ENOENTIssue Id: def759c3-62fe-b6f1-ff32-27679721a3ffVersions - 0.10.8-  f291f4a-  43ff6af-  5b5f4db-  17fa1cbStack Error: spawn osascript ENOENT    at exports._errnoException (util.js:837:11)    at Process.ChildProcess._handle.onexit (internal/child_process.js:178:32)     at onErrorNT (internal/child_process.js:344:16)     at doNTCallback2 (node.js:442:9)    at process._tickCallback (node.js:356:17),3
,2852,146,72,35637,"VSCode Version: 1.16.1
OS Version: OSX

I personally have no need for recent files to be promoted to the top of the Goto File results:

These results actually slow me down, since I a used to Sublime Text fuzzy match behavior. If the ""Recently Opened"" results were not there, I would be able to jump to files in a predictable way.
Any preference option I'm missing?",Preference to disable Recently Opened Files,"Preference to disable Recently Opened FilesVSCode Version: 1.16.1
OS Version: OSX

I personally have no need for recent files to be promoted to the top of the Goto File results:

These results actually slow me down, since I a used to Sublime Text fuzzy match behavior. If the ""Recently Opened"" results were not there, I would be able to jump to files in a predictable way.
Any preference option I'm missing?",3
,2853,146,73,5650,"VSCode Version: 1.0.0
OS Version:

Instead of extensions and language servers providing their own language specific actions the workbench should provide actions to apply all available quick fixes to a file. I think that this is even possible with the extension API today since we can ask for all code actions for a given range.
See microsoft/vscode-eslint#70.",Workbench should provide language agnostic actions to apply all quick fixes to a file,"Workbench should provide language agnostic actions to apply all quick fixes to a fileVSCode Version: 1.0.0
OS Version:

Instead of extensions and language servers providing their own language specific actions the workbench should provide actions to apply all available quick fixes to a file. I think that this is even possible with the extension API today since we can ask for all code actions for a given range.
See microsoft/vscode-eslint#70.",3
,2854,146,74,13103,"VSCode Version: 1.6 insiders
OS Version: doesn't matter

In Menu Items:

Help -> Search (Mac OS X)
View -> Toggle Render Whitespace
Edit -> Start Dictation (Mac OS X)

In Settings:

The below is not localized in User Settings, but is localized in Workspace Settings
// Place your settings in this file to overwrite the default settings
The below is not localized in Keyboard Shortcut settings
// Place your key bindings in this file to overwrite the defaults

In Launch.json
Tooltips for all properties except the below are not localized in launch.json

name
type
request
preLaunch task

while debugging

The title for the Debug Console in German appears as ""DebugKonsole"". There should be a space between the ""Debug"" and the ""Console""
Add watch for a symbol. When it is not available, the value says ""not available"" in English

while closing unsaved files
The dialog box that appears has text that is not localized",Missing localizations in 1.6,"Missing localizations in 1.6VSCode Version: 1.6 insiders
OS Version: doesn't matter

In Menu Items:

Help -> Search (Mac OS X)
View -> Toggle Render Whitespace
Edit -> Start Dictation (Mac OS X)

In Settings:

The below is not localized in User Settings, but is localized in Workspace Settings
// Place your settings in this file to overwrite the default settings
The below is not localized in Keyboard Shortcut settings
// Place your key bindings in this file to overwrite the defaults

In Launch.json
Tooltips for all properties except the below are not localized in launch.json

name
type
request
preLaunch task

while debugging

The title for the Debug Console in German appears as ""DebugKonsole"". There should be a space between the ""Debug"" and the ""Console""
Add watch for a symbol. When it is not available, the value says ""not available"" in English

while closing unsaved files
The dialog box that appears has text that is not localized",3
,2855,148,75,74759,"VSCode Version: 1.34.0
OS Version: macOS Mojave 10.14.5

Steps to Reproduce:

Open a document with an NSDocument-aware app such as TextEdit
(Optionally, open that document with a second NSDocument-aware window)
Open that document with VS Code
(Optionally, open that document with a second VS Code window)
(Optionally, edit and save the document in the first app, observe that all VS Code windows update)
Edit and save the document in VS Code, observe that the first app doesn't update (but all other VS Code windows update)
(Optionally, re-save the document in the first app. Observe that, prior to interacting with the warning dialog, all other NSDocument-aware windows update.)

This bug impairs working with the same file in multiple applications on macOS.
This may be related to NSFilePresenter and presentedItemDidChange of NSDocument.
Does this issue occur when all extensions are disabled?: Yes",Other NSDocument apps don't recognize when VS Code changes a document,"Other NSDocument apps don't recognize when VS Code changes a documentVSCode Version: 1.34.0
OS Version: macOS Mojave 10.14.5

Steps to Reproduce:

Open a document with an NSDocument-aware app such as TextEdit
(Optionally, open that document with a second NSDocument-aware window)
Open that document with VS Code
(Optionally, open that document with a second VS Code window)
(Optionally, edit and save the document in the first app, observe that all VS Code windows update)
Edit and save the document in VS Code, observe that the first app doesn't update (but all other VS Code windows update)
(Optionally, re-save the document in the first app. Observe that, prior to interacting with the warning dialog, all other NSDocument-aware windows update.)

This bug impairs working with the same file in multiple applications on macOS.
This may be related to NSFilePresenter and presentedItemDidChange of NSDocument.
Does this issue occur when all extensions are disabled?: Yes",3
,2856,146,76,23062,"VSCode Version: 1.10.2
OS Version: Windows 10

Steps to Reproduce:
I have a laptop with touchpad and scrolling (by touching the touchpad with two fingers and sliding them vertically) file content, files list, terminal window or anything inside VSCode behaves like I am pressing an holding up/down arrow keys. This behavior is not only annoying but it doesn't allow me to scroll say terminal window, because scrolling simply shows the list of last run commands - the same as if you press up arrow key multiple times. Horizontal scrolling behaves as left/right arrow keys - instead of scrolling horizontally, I move the caret. I have a wireless Microsoft mouse and when I use it for scrolling, everything works a expected. When I use my touchpad to scroll in the same way any other application like Chrome, Notepad, whatever, it behaves normally (the caret stays on its location and only the view is scrolled - actually I can normally scroll this GitHub's text area I am writing the issue text in). Does anyone have such problem with VSCode ? Could it because of touchpad drivers (I think mine is ""ELAN Pointing Device"") ?
List of extensions:
1 Debugger for Chrome 2.7.0
2. Git History (git log) 0.2.0
3. TSLint 0.8.1
4. vscode-icons 7.4.0
I tried to disable all extensions and reload VSCode but nothing changed.
The video below is made without using arrows keys - only scrolling functionality of my touchpad.",Scrolling with touchpad behaves like pressing up and down arrow keys,"Scrolling with touchpad behaves like pressing up and down arrow keysVSCode Version: 1.10.2
OS Version: Windows 10

Steps to Reproduce:
I have a laptop with touchpad and scrolling (by touching the touchpad with two fingers and sliding them vertically) file content, files list, terminal window or anything inside VSCode behaves like I am pressing an holding up/down arrow keys. This behavior is not only annoying but it doesn't allow me to scroll say terminal window, because scrolling simply shows the list of last run commands - the same as if you press up arrow key multiple times. Horizontal scrolling behaves as left/right arrow keys - instead of scrolling horizontally, I move the caret. I have a wireless Microsoft mouse and when I use it for scrolling, everything works a expected. When I use my touchpad to scroll in the same way any other application like Chrome, Notepad, whatever, it behaves normally (the caret stays on its location and only the view is scrolled - actually I can normally scroll this GitHub's text area I am writing the issue text in). Does anyone have such problem with VSCode ? Could it because of touchpad drivers (I think mine is ""ELAN Pointing Device"") ?
List of extensions:
1 Debugger for Chrome 2.7.0
2. Git History (git log) 0.2.0
3. TSLint 0.8.1
4. vscode-icons 7.4.0
I tried to disable all extensions and reload VSCode but nothing changed.
The video below is made without using arrows keys - only scrolling functionality of my touchpad.",3
,2857,145,77,41061,"Happy 2018 everyone! This plan captures our work in January. This is a 5 week iteration. We will ship early February.
Endgame

January 29th: Code freeze for the endgame
February 2nd: Endgame done

The endgame details for this iteration are tracked in #42374
Plan Items
Below is a summary of the top level plan items. Given the large number of explorations, we'll diverge from our usual practice of having plan items for all bullets upfront. This time we'll add them as we go.
Legend of annotations:



Mark
Description





work in progress



blocked task



stretch goal for this iteration



missing issue reference



more investigation required to remove uncertainty



under discussion within the team



Install/Update

 Investigate in improving the update experience on Windows #41676 @joaomoreno

Workbench

 Switch to async dialog API #39536 @bpasero
 Support saving a file in admin mode #1614 @bpasero
 UX for notification improvements #22388 @bpasero @stevencl
 Multi-select in the Explorer, Open Editor #1023 @isidorn
 Reimplement drop downs for Linux/Windows (themable, fixes initial empty contents) #25965 (PR @cleidigh) @bpasero
 Enable Error decorations in explorer #782 @jrieken
 Support natural language search in Settings editor #40957 @roblourens
 Explore improving how a user changes a setting #41040 @roblourens @sandy081

Editor

 Text model and storage reimplementation to improve performance #41042 @alexandrudima @rebornix
 Allow to save large files > 256 MB #32503 @bpasero @alexandrudima
 Support language-type independent snippets #13182 @jrieken
 More customization for the caret #41052 @ramya-rao-a

Debug

 Launch configs for multi root workspaces #38134 @isidorn
 Support auto attach for node.js subprocess (aka cluster support) #40123 @weinand
 Support to use nvm configuratons in node launch configs #25386 @weinand
 Explore how to run DebugAdapter inside extension #40906 @weinand

Terminal

 Improve accessibility of built-in terminal #8339 @Tyriar

SCM

 Submodule support @joaomoreno
 Git commit message length counter @joaomoreno

Output Panel

 Show product logs in the output panel #39638 @sandy081
 Make log viewing in output panel more memory efficient #40196 @sandy081

Languages
Language Server Support

 Create a website for LSP @dbaeumer @auchenberg
 Protocol extension for goto implementation microsoft/language-server-protocol#156 @dbaeumer

Emmet

 Explore how to improve emmet activation in html and css files #29113 @ramya-rao-a

JavaScript/TypeScript

 Adoption of TS 2.7 #41046 @mjbvz

CSS/HTML

 Catchup with latest CSS/Less syntax microsoft/vscode-css-languageservice#56 microsoft/vscode-css-languageservice#57 microsoft/vscode-css-languageservice#58 microsoft/vscode-css-languageservice#47 @octref @aeschli

Extensions

  vsce - Warn when package.json misses repository entry #41677 @joaomoreno
 Improve quality of recommended extensions #41054 @ramya-rao-a
 Tastefully extend recommendations to a wider range of file types #38543 @ramya-rao-a

Extension Contributions

 Refresh JS Hint support microsoft/vscode-jshint#48 @RMacfarlane

API

 Migrate proposed Code action API to stable #34664 @jrieken @mjbvz
 Propose API to resolve rename/definition scope #7340 @jrieken @mjbvz
 Explore improving the HTMLPreview support #41047 @mjbvz @jrieken
 Propose refactoring provider API #41048 @jrieken @mjbvz
 Expose logging API to extensions #40053 @roblourens
 Propose Search Provider API @jrieken, @roblourens
 Propose API to create/delete/rename resources for refactorings @jrieken @mjbvz
 Enhance custom tree view API, primary actions, use icons from resource URI, preserve expansion state, improve managing contributions  #27823 @sandy081
 Debug API to create/remove breakpoints @weinand
 Support to modify the root folder of a workspace #35407 @bpasero

Performance

  Use ASAR for bundled node modules #41350, #41353 @alexandrudima
 Explore using plain nodejs for helper processes #41685 @alexandrudima
 Explore local storage replacement #18439 @bpasero

Serviceability

 Issue reporter in separate renderer windows #41041 @RMacfarlane @octref
Logs

 Propagate log level to all processes dynamically #39754 @sandy081
 Support to upload logs #40056 @mjbvz @roblourens



Engineering

 Self-host on @ts-check for our JS code #41678 @joaomoreno @egamma
  Support extensions that contribute translations aka ""language packs"" #39178 @dbaeumer @sandy081 @aeschli
  Tool to generate/update a language pack from transifex #41682 @aeschli
 Speed up gulp-build @alexandrudima
Improve issue tracking support bot @chrmarti

 Delay action of auto-assignment bot by 15s #33999
 Explore support for detecting duplicate issues #41292



Documentations

 Make 5min Node.js debugging video to be embedded in docs and uploaded to YouTube channel @auchenberg
 Add Debugging Recipe for VueJS. microsoft/vscode-recipes#55 @auchenberg


Deferred


Improve documentation of our electron upgrade process #41036 @Tyriar


Add intelisense support for src  attributes and href in html #2037 @octref @aeschli


Adopt logging service  @joaomoreno the adoption by the #41680 team


 Render white space for selection option #1477 @ramya-rao-a


Provide API for creating a file based output channel #41672 @sandy081


 Electron update to 2.0 @Tyriar @bpasero


 Docathon team


 Support 32-bit apt repositories #20790 @Tyriar


Better support for webpack TBD


Support web-site for issue reporting @octref @RMacfarlane


Improve stability of smoke test, run it as part of the builds #41679 # @joaomoreno


 Process explorer as a separate renderer window #41045 @RMacfarlane",Iteration Plan for January 2018,"Iteration Plan for January 2018Happy 2018 everyone! This plan captures our work in January. This is a 5 week iteration. We will ship early February.
Endgame

January 29th: Code freeze for the endgame
February 2nd: Endgame done

The endgame details for this iteration are tracked in #42374
Plan Items
Below is a summary of the top level plan items. Given the large number of explorations, we'll diverge from our usual practice of having plan items for all bullets upfront. This time we'll add them as we go.
Legend of annotations:



Mark
Description





work in progress



blocked task



stretch goal for this iteration



missing issue reference



more investigation required to remove uncertainty



under discussion within the team



Install/Update

 Investigate in improving the update experience on Windows #41676 @joaomoreno

Workbench

 Switch to async dialog API #39536 @bpasero
 Support saving a file in admin mode #1614 @bpasero
 UX for notification improvements #22388 @bpasero @stevencl
 Multi-select in the Explorer, Open Editor #1023 @isidorn
 Reimplement drop downs for Linux/Windows (themable, fixes initial empty contents) #25965 (PR @cleidigh) @bpasero
 Enable Error decorations in explorer #782 @jrieken
 Support natural language search in Settings editor #40957 @roblourens
 Explore improving how a user changes a setting #41040 @roblourens @sandy081

Editor

 Text model and storage reimplementation to improve performance #41042 @alexandrudima @rebornix
 Allow to save large files > 256 MB #32503 @bpasero @alexandrudima
 Support language-type independent snippets #13182 @jrieken
 More customization for the caret #41052 @ramya-rao-a

Debug

 Launch configs for multi root workspaces #38134 @isidorn
 Support auto attach for node.js subprocess (aka cluster support) #40123 @weinand
 Support to use nvm configuratons in node launch configs #25386 @weinand
 Explore how to run DebugAdapter inside extension #40906 @weinand

Terminal

 Improve accessibility of built-in terminal #8339 @Tyriar

SCM

 Submodule support @joaomoreno
 Git commit message length counter @joaomoreno

Output Panel

 Show product logs in the output panel #39638 @sandy081
 Make log viewing in output panel more memory efficient #40196 @sandy081

Languages
Language Server Support

 Create a website for LSP @dbaeumer @auchenberg
 Protocol extension for goto implementation microsoft/language-server-protocol#156 @dbaeumer

Emmet

 Explore how to improve emmet activation in html and css files #29113 @ramya-rao-a

JavaScript/TypeScript

 Adoption of TS 2.7 #41046 @mjbvz

CSS/HTML

 Catchup with latest CSS/Less syntax microsoft/vscode-css-languageservice#56 microsoft/vscode-css-languageservice#57 microsoft/vscode-css-languageservice#58 microsoft/vscode-css-languageservice#47 @octref @aeschli

Extensions

  vsce - Warn when package.json misses repository entry #41677 @joaomoreno
 Improve quality of recommended extensions #41054 @ramya-rao-a
 Tastefully extend recommendations to a wider range of file types #38543 @ramya-rao-a

Extension Contributions

 Refresh JS Hint support microsoft/vscode-jshint#48 @RMacfarlane

API

 Migrate proposed Code action API to stable #34664 @jrieken @mjbvz
 Propose API to resolve rename/definition scope #7340 @jrieken @mjbvz
 Explore improving the HTMLPreview support #41047 @mjbvz @jrieken
 Propose refactoring provider API #41048 @jrieken @mjbvz
 Expose logging API to extensions #40053 @roblourens
 Propose Search Provider API @jrieken, @roblourens
 Propose API to create/delete/rename resources for refactorings @jrieken @mjbvz
 Enhance custom tree view API, primary actions, use icons from resource URI, preserve expansion state, improve managing contributions  #27823 @sandy081
 Debug API to create/remove breakpoints @weinand
 Support to modify the root folder of a workspace #35407 @bpasero

Performance

  Use ASAR for bundled node modules #41350, #41353 @alexandrudima
 Explore using plain nodejs for helper processes #41685 @alexandrudima
 Explore local storage replacement #18439 @bpasero

Serviceability

 Issue reporter in separate renderer windows #41041 @RMacfarlane @octref
Logs

 Propagate log level to all processes dynamically #39754 @sandy081
 Support to upload logs #40056 @mjbvz @roblourens



Engineering

 Self-host on @ts-check for our JS code #41678 @joaomoreno @egamma
  Support extensions that contribute translations aka ""language packs"" #39178 @dbaeumer @sandy081 @aeschli
  Tool to generate/update a language pack from transifex #41682 @aeschli
 Speed up gulp-build @alexandrudima
Improve issue tracking support bot @chrmarti

 Delay action of auto-assignment bot by 15s #33999
 Explore support for detecting duplicate issues #41292



Documentations

 Make 5min Node.js debugging video to be embedded in docs and uploaded to YouTube channel @auchenberg
 Add Debugging Recipe for VueJS. microsoft/vscode-recipes#55 @auchenberg


Deferred


Improve documentation of our electron upgrade process #41036 @Tyriar


Add intelisense support for src  attributes and href in html #2037 @octref @aeschli


Adopt logging service  @joaomoreno the adoption by the #41680 team


 Render white space for selection option #1477 @ramya-rao-a


Provide API for creating a file based output channel #41672 @sandy081


 Electron update to 2.0 @Tyriar @bpasero


 Docathon team


 Support 32-bit apt repositories #20790 @Tyriar


Better support for webpack TBD


Support web-site for issue reporting @octref @RMacfarlane


Improve stability of smoke test, run it as part of the builds #41679 # @joaomoreno


 Process explorer as a separate renderer window #41045 @RMacfarlane",3
,2858,148,78,27480,"Testing #27193

 OSX @bpasero
 windows @chrisdias
 linux @isidorn

Complexity: 2
There is a new setting editor.multicursorModifier. On Linux and Windows it can have the value ""ctrl"" or ""alt"". On OSX it can have the value ""cmd"" or ""alt"". Please check:

the default is ""alt"" (just as before). So without any changes, out of the box, multiple cursors are added via ""alt"" + click. It is a known issue that under some Linux distributions, multiple cursors cannot be added via ""alt"" + click, which is used to move windows, this new option aims to also overcome this limitation.
when using alt as the multicursor modifier: ctrl+click (cmd+click) is used for going to definition and opening links. ctrl+alt+click (cmd+alt+click) is used for going to definition and opening in a side editor and for opening a link in a side editor (you can craft a file:/// link to test this)
when using ctrl or cmd as the multicursor modifier, the 3 features swap modifiers and going to definition / opening a link is alt+click. ctrl+alt+click (cmd+alt+click) will still be used for going to definition / opening a link to the side. The hover message on links correctly shows the modifier to be used to open a link.",Test editor.multicursorModifier,"Test editor.multicursorModifierTesting #27193

 OSX @bpasero
 windows @chrisdias
 linux @isidorn

Complexity: 2
There is a new setting editor.multicursorModifier. On Linux and Windows it can have the value ""ctrl"" or ""alt"". On OSX it can have the value ""cmd"" or ""alt"". Please check:

the default is ""alt"" (just as before). So without any changes, out of the box, multiple cursors are added via ""alt"" + click. It is a known issue that under some Linux distributions, multiple cursors cannot be added via ""alt"" + click, which is used to move windows, this new option aims to also overcome this limitation.
when using alt as the multicursor modifier: ctrl+click (cmd+click) is used for going to definition and opening links. ctrl+alt+click (cmd+alt+click) is used for going to definition and opening in a side editor and for opening a link in a side editor (you can craft a file:/// link to test this)
when using ctrl or cmd as the multicursor modifier, the 3 features swap modifiers and going to definition / opening a link is alt+click. ctrl+alt+click (cmd+alt+click) will still be used for going to definition / opening a link to the side. The hover message on links correctly shows the modifier to be used to open a link.",3
,2859,143,79,66311,"VSCode Version: 1.30.2
OS Version: Windows 10 x64

Steps to Reproduce:

Debug a GO program
The program writes to the standard output
IDE starts to slow down and finally freeze.",IDE slows to freeze as more and more output written to internal debug console,"IDE slows to freeze as more and more output written to internal debug consoleVSCode Version: 1.30.2
OS Version: Windows 10 x64

Steps to Reproduce:

Debug a GO program
The program writes to the standard output
IDE starts to slow down and finally freeze.",3
,2860,144,80,15542,While smoke testing 1.7.2 eb1f17e,Markdown preview doesn't work correctly when zoomed in,Markdown preview doesn't work correctly when zoomed inWhile smoke testing 1.7.2 eb1f17e,3
,2861,144,81,65163,"i use nodemon in project and it work smooth and good but when i try to run exact gulp command 'gulp serve' in vs code debugger it cuase following error:
'[nodemon] app crashed - waiting for file changes before starting...'
it work really when i type gulp command but in debug mode i get error.
the code is:
var nodemonStream = nodemon({
       script: './Server/Bootstrap.js',
       inspect:true,
       done:done
   });",nodejs debugger nodemon crush ,"nodejs debugger nodemon crush i use nodemon in project and it work smooth and good but when i try to run exact gulp command 'gulp serve' in vs code debugger it cuase following error:
'[nodemon] app crashed - waiting for file changes before starting...'
it work really when i type gulp command but in debug mode i get error.
the code is:
var nodemonStream = nodemon({
       script: './Server/Bootstrap.js',
       inspect:true,
       done:done
   });",3
,2862,145,82,34586,"VSCode Version: Code - Insiders 1.17.0-insider (128a4e3, 2017-09-12T05:24:19.607Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




cpptools
ms-
0.12.4



IntelliSense doesn't seem to be working with SSHFS filesystem type in Windows 10 (no import errors detected and no squiggly lines). I know this is a fringe case, but I'm wondering if there is an easy fix.
IntelliSense works fine for this example in the local filesystem as well as on an exfat formatted flash drive.

Steps to Reproduce:

Install winsshfs and mount a remote file system
Open the a project locally from that mounted file system




Reproduces without extensions: No (Need the C++ extension for intellisense)",C++ intelliSense with different file system types (Windows 10),"C++ intelliSense with different file system types (Windows 10)VSCode Version: Code - Insiders 1.17.0-insider (128a4e3, 2017-09-12T05:24:19.607Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




cpptools
ms-
0.12.4



IntelliSense doesn't seem to be working with SSHFS filesystem type in Windows 10 (no import errors detected and no squiggly lines). I know this is a fringe case, but I'm wondering if there is an easy fix.
IntelliSense works fine for this example in the local filesystem as well as on an exfat formatted flash drive.

Steps to Reproduce:

Install winsshfs and mount a remote file system
Open the a project locally from that mounted file system




Reproduces without extensions: No (Need the C++ extension for intellisense)",3
,2863,142,83,39553,"Test for #23188:
Complexity: 4

 Any OS - @jrieken

The November milestone of VS Code proposes extension API for reading the breakpoints of a workspace and tracking added, removed, and changed breakpoints:
https://github.com/Microsoft/vscode/blob/a42cd0efc5b4baa17075fcd8da1c5e2097419c6f/src/vs/vscode.proposed.d.ts#L251-L329
Verify:

API makes sense (especially the Breakpoint, SourceBreakpoint, FunctionBreakpoint hierarchy and its use of the type discriminator). Will this work if we extend the API to create those types?
write a simple extension that accesses breakpoints and registers for BreakpointsChangeEvents. Please note that accessing breakpoints initially returns an empty array but triggers a subsequent event that has the full set of breakpoints in its added property.",Test extension API for breakpoints ,"Test extension API for breakpoints Test for #23188:
Complexity: 4

 Any OS - @jrieken

The November milestone of VS Code proposes extension API for reading the breakpoints of a workspace and tracking added, removed, and changed breakpoints:
https://github.com/Microsoft/vscode/blob/a42cd0efc5b4baa17075fcd8da1c5e2097419c6f/src/vs/vscode.proposed.d.ts#L251-L329
Verify:

API makes sense (especially the Breakpoint, SourceBreakpoint, FunctionBreakpoint hierarchy and its use of the type discriminator). Will this work if we extend the API to create those types?
write a simple extension that accesses breakpoints and registers for BreakpointsChangeEvents. Please note that accessing breakpoints initially returns an empty array but triggers a subsequent event that has the full set of breakpoints in its added property.",3
,2864,145,84,77406,"Issue Type: Performance Issue
The new behavior of file copy pasting (ctrl+c, ctrl+v) in the proj explorer adds whitespace to the new file name, often causing command line scripts to not work. including double quotes to the script is not always easy, and many devs(probably you as well) have a habit of eliminating whites just to avoid this clumsiness.
Older behavior was much better, if not best, file.ext -> file.1.ext
plz restore it
VS Code version: Code 1.36.1 (2213894, 2019-07-08T22:59:35.033Z)
OS version: Windows_NT x64 10.0.17134

Workspace Info
|  Window (LightStickUI.1.ino - LightStickUI - Visual Studio Code)
|  Window (Transparent_Sprite_Demo.ino - Transparent_Sprite_Demo - Visual Studio Code)
|    Folder (Transparent_Sprite_Demo): 3 files
|      File types: json(2) ino(1)
|      Conf files:
|    Folder (LightStickUI): 6 files
|      File types: json(3) ino(2) cpp(1)
|      Conf files:;


Extensions (2)



Extension
Author (truncated)
Version




cpptools
ms-
0.24.0


vscode-arduino
vsc
0.2.27



(1 theme extensions excluded)",new behavior of File duplication not smart(file<space>copy.ext),"new behavior of File duplication not smart(file<space>copy.ext)Issue Type: Performance Issue
The new behavior of file copy pasting (ctrl+c, ctrl+v) in the proj explorer adds whitespace to the new file name, often causing command line scripts to not work. including double quotes to the script is not always easy, and many devs(probably you as well) have a habit of eliminating whites just to avoid this clumsiness.
Older behavior was much better, if not best, file.ext -> file.1.ext
plz restore it
VS Code version: Code 1.36.1 (2213894, 2019-07-08T22:59:35.033Z)
OS version: Windows_NT x64 10.0.17134

Workspace Info
|  Window (LightStickUI.1.ino - LightStickUI - Visual Studio Code)
|  Window (Transparent_Sprite_Demo.ino - Transparent_Sprite_Demo - Visual Studio Code)
|    Folder (Transparent_Sprite_Demo): 3 files
|      File types: json(2) ino(1)
|      Conf files:
|    Folder (LightStickUI): 6 files
|      File types: json(3) ino(2) cpp(1)
|      Conf files:;


Extensions (2)



Extension
Author (truncated)
Version




cpptools
ms-
0.24.0


vscode-arduino
vsc
0.2.27



(1 theme extensions excluded)",3
,2865,148,85,21868,"VSCode Version: 1.10.1
OS Version: Windows 10 Home 1607

Steps to Reproduce:

Open file explorer
Open any ftp folder

Drag an drop any file from FIle Explorer into VSCode
The dragged file gets removed from FTP folder, but it is not openend in VSCode",FTP file gets deleted when drag and drop from file explorer,"FTP file gets deleted when drag and drop from file explorerVSCode Version: 1.10.1
OS Version: Windows 10 Home 1607

Steps to Reproduce:

Open file explorer
Open any ftp folder

Drag an drop any file from FIle Explorer into VSCode
The dragged file gets removed from FTP folder, but it is not openend in VSCode",3
,2866,144,86,27537,"Testing #27456
I have the vim extension installed and disabled (always), but it looks like the integration test picks it up and enables it. This causes the integration test to fail in the Data Migration -> checks if the Untitled file is restored migrating from stable to latest test
I will continue by uninstalling the vim extension.",Cannot run integration test while having the vim extension installed ,"Cannot run integration test while having the vim extension installed Testing #27456
I have the vim extension installed and disabled (always), but it looks like the integration test picks it up and enables it. This causes the integration test to fail in the Data Migration -> checks if the Untitled file is restored migrating from stable to latest test
I will continue by uninstalling the vim extension.",3
,2867,144,87,81149,"Issue Type: Bug
1- Write print('hello') in a new python file
2- Select the chunk of code and run it with run selection (shift+enter)
3- Run the code again, but using run python file in terminal.
expected: hello
actual:
File """", line 1
& C:/Users/me/AppData/Local/Programs/Python/Python37-32/python.exe ""mypath/test.py""
^
SyntaxError: invalid syntax
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:35:15.005Z)
OS version: Windows_NT x64 10.0.18362

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz (8 x 1896)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
undefined


Memory (System)
7.85GB (1.38GB free)


Process Argv



Screen Reader
no


VM
0%","Bug when running python code in console after running it using the ""run the selection"" feature","Bug when running python code in console after running it using the ""run the selection"" featureIssue Type: Bug
1- Write print('hello') in a new python file
2- Select the chunk of code and run it with run selection (shift+enter)
3- Run the code again, but using run python file in terminal.
expected: hello
actual:
File """", line 1
& C:/Users/me/AppData/Local/Programs/Python/Python37-32/python.exe ""mypath/test.py""
^
SyntaxError: invalid syntax
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:35:15.005Z)
OS version: Windows_NT x64 10.0.18362

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz (8 x 1896)


GPU Status
2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled


Load (avg)
undefined


Memory (System)
7.85GB (1.38GB free)


Process Argv



Screen Reader
no


VM
0%",3
,2868,146,88,68659,"When I use a code block for something like powershell or t-sql, the colors of the font change, making it easier to parse the text.
For example


Font color changes in accordance to powershell standards.
However, when I do so for CLI, the color doesn't change, and it should be changing, right?


No font color change in accordance to CLI standards.",CLI code snippets do not change font color,"CLI code snippets do not change font colorWhen I use a code block for something like powershell or t-sql, the colors of the font change, making it easier to parse the text.
For example


Font color changes in accordance to powershell standards.
However, when I do so for CLI, the color doesn't change, and it should be changing, right?


No font color change in accordance to CLI standards.",3
,2869,148,89,82420,"Pretty similar to #66863. SCM tree should exclude first level items and keep indent at 8px



""scm.defaultViewMode"": ""tree"",

Indent for first level items is too big, because of

""workbench.tree.indent"": 20,",[SCM tree]: Keep first level indent the same as list,"[SCM tree]: Keep first level indent the same as listPretty similar to #66863. SCM tree should exclude first level items and keep indent at 8px



""scm.defaultViewMode"": ""tree"",

Indent for first level items is too big, because of

""workbench.tree.indent"": 20,",3
,2870,146,90,37335,"While testing #35904
I don't know if this is as-designed, but this seems to be one of the original motivation for #31745.
When I'm pressing F5 while not focusing on debug viewlet, I got no visual feedback as to what's the debug target.
Would it make sense to enable the status bar item on launching debug instead of on launching debug successfully?","When launching attach config unsuccessfully, debug status bar item does not show","When launching attach config unsuccessfully, debug status bar item does not showWhile testing #35904
I don't know if this is as-designed, but this seems to be one of the original motivation for #31745.
When I'm pressing F5 while not focusing on debug viewlet, I got no visual feedback as to what's the debug target.
Would it make sense to enable the status bar item on launching debug instead of on launching debug successfully?",3
,2871,148,91,48889,"Why the dark+ theme's syntax is different from the light+ theme's syntax?


VS Code version: Code 1.22.2 (3aeede7, 2018-04-12T16:38:45.278Z)
OS version: Windows_NT x64 10.0.15063

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-4210H CPU @ 2.90GHz (4 x 2893)


Memory (System)
7.89GB (1.54GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
0%



Extensions (9)



Extension
Author (truncated)
Version




vscode-custom-css
be5
2.7.0


markdown-preview-github-styles
bie
0.1.2


vscode-eslint
dba
1.4.8


EditorConfig
Edi
0.12.1


tslint
eg2
1.0.28


vsc-material-theme
Equ
2.0.1


debugger-for-chrome
msj
4.3.0


advanced-new-file
pat
1.2.0


vscode-icons
rob
7.23.0",Why the dark+ theme's syntax is different from the light+ theme's syntax?,"Why the dark+ theme's syntax is different from the light+ theme's syntax?Why the dark+ theme's syntax is different from the light+ theme's syntax?


VS Code version: Code 1.22.2 (3aeede7, 2018-04-12T16:38:45.278Z)
OS version: Windows_NT x64 10.0.15063

System Info



Item
Value




CPUs
Intel(R) Core(TM) i5-4210H CPU @ 2.90GHz (4 x 2893)


Memory (System)
7.89GB (1.54GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
0%



Extensions (9)



Extension
Author (truncated)
Version




vscode-custom-css
be5
2.7.0


markdown-preview-github-styles
bie
0.1.2


vscode-eslint
dba
1.4.8


EditorConfig
Edi
0.12.1


tslint
eg2
1.0.28


vsc-material-theme
Equ
2.0.1


debugger-for-chrome
msj
4.3.0


advanced-new-file
pat
1.2.0


vscode-icons
rob
7.23.0",3
,2872,148,92,29736,"I've upgraded a task.json to 2.0.0 and get a deprecated warning

Trying to fix by inserting a presentation property, but there is no Intellisense proposal for presentation.",No completion proposal for the new presentation property.,"No completion proposal for the new presentation property.I've upgraded a task.json to 2.0.0 and get a deprecated warning

Trying to fix by inserting a presentation property, but there is no Intellisense proposal for presentation.",3
,2873,142,93,57059,"Hi Team,
I have replaced a word from multiple files through find tab and saved all.
But again when i open the ts file to edit some other line , its is showing a working directory.
It was fixing by VSC restart.
Please fix this with an alternative",Replace text,"Replace textHi Team,
I have replaced a word from multiple files through find tab and saved all.
But again when i open the ts file to edit some other line , its is showing a working directory.
It was fixing by VSC restart.
Please fix this with an alternative",3
,2874,146,94,32853,"VSCode Version: Code 1.15.1 (41abd21, 2017-08-16T18:07:25.676Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




spellright
ban
1.1.16


python
don
0.7.0


cpptools
ms-
0.12.3


csharp
ms-
1.12.1


PowerShell
ms-
1.4.1


blank-line-organizer
rin
0.1.2


sort-lines
Tyr
1.3.0


change-case
wma
1.0.0




Steps to Reproduce:

Set your menu bar to auto hide (i.e. set it to toggle).
Press Alt to show the menu bar.
Click somewhere near the beginning of a line of text in your currently visible text document.
As the menu bar disappears, notice that the text gets selected, started from the position where you clicked and ending on the line below where you clicked.

Expected:
Nothing should get selected when the menu bar auto hides. The cursor should simply be wherever you clicked.

Reproduces without extensions: Yes/No","Text gets selected, to the next line, when the menu bar auto hides","Text gets selected, to the next line, when the menu bar auto hidesVSCode Version: Code 1.15.1 (41abd21, 2017-08-16T18:07:25.676Z)
OS Version: Windows_NT x64 10.0.15063
Extensions:




Extension
Author (truncated)
Version




spellright
ban
1.1.16


python
don
0.7.0


cpptools
ms-
0.12.3


csharp
ms-
1.12.1


PowerShell
ms-
1.4.1


blank-line-organizer
rin
0.1.2


sort-lines
Tyr
1.3.0


change-case
wma
1.0.0




Steps to Reproduce:

Set your menu bar to auto hide (i.e. set it to toggle).
Press Alt to show the menu bar.
Click somewhere near the beginning of a line of text in your currently visible text document.
As the menu bar disappears, notice that the text gets selected, started from the position where you clicked and ending on the line below where you clicked.

Expected:
Nothing should get selected when the menu bar auto hides. The cursor should simply be wherever you clicked.

Reproduces without extensions: Yes/No",3
,2875,148,95,26483,"VSCode Version: latest
OS Version: OSX

Steps to Reproduce:

edit a file in a Git repo with VS Code
click the Git icon in the left well to see the diff.

I often want to check the differences between two files, and then immediately go to the current working index of that file.
expect: (something like) cmd+ right click on the current working index of a Git diff takes you to that file.",Go to current file from viewing diff. ,"Go to current file from viewing diff. VSCode Version: latest
OS Version: OSX

Steps to Reproduce:

edit a file in a Git repo with VS Code
click the Git icon in the left well to see the diff.

I often want to check the differences between two files, and then immediately go to the current working index of that file.
expect: (something like) cmd+ right click on the current working index of a Git diff takes you to that file.",3
,2876,145,96,51415,"Issue Type: Bug
I am using vs code, and i am facing issue regarding updates .
why it always require admin permission to start, and
It able to fetch updates but does not able to update.
VS Code version: Code 1.24.0 (6a6e02c, 2018-06-06T17:35:40.560Z)
OS version: Windows_NT x64 6.0.6002

System Info



Item
Value




CPUs
Intel(R) Core(TM) i3-5005U CPU @ 2.00GHz (4 x 1995)


GPU Status
2d_canvas: unavailable_softwareflash_3d: unavailable_softwareflash_stage3d: unavailable_softwareflash_stage3d_baseline: unavailable_softwaregpu_compositing: unavailable_softwaremultiple_raster_threads: unavailable_offnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: unavailable_softwarevideo_encode: unavailable_softwarevpx_decode: unavailable_softwarewebgl: unavailable_offwebgl2: unavailable_off


Memory (System)
7.92GB (4.17GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
0%



Extensions (4)



Extension
Author (truncated)
Version




html-snippets
abu
0.2.1


vscode-html-css
ecm
0.2.0


beautify
Hoo
1.3.0


trimspaces
igo
0.0.24",unable to update ,"unable to update Issue Type: Bug
I am using vs code, and i am facing issue regarding updates .
why it always require admin permission to start, and
It able to fetch updates but does not able to update.
VS Code version: Code 1.24.0 (6a6e02c, 2018-06-06T17:35:40.560Z)
OS version: Windows_NT x64 6.0.6002

System Info



Item
Value




CPUs
Intel(R) Core(TM) i3-5005U CPU @ 2.00GHz (4 x 1995)


GPU Status
2d_canvas: unavailable_softwareflash_3d: unavailable_softwareflash_stage3d: unavailable_softwareflash_stage3d_baseline: unavailable_softwaregpu_compositing: unavailable_softwaremultiple_raster_threads: unavailable_offnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: unavailable_softwarevideo_encode: unavailable_softwarevpx_decode: unavailable_softwarewebgl: unavailable_offwebgl2: unavailable_off


Memory (System)
7.92GB (4.17GB free)


Process Argv
C:\Program Files\Microsoft VS Code\Code.exe


Screen Reader
no


VM
0%



Extensions (4)



Extension
Author (truncated)
Version




html-snippets
abu
0.2.1


vscode-html-css
ecm
0.2.0


beautify
Hoo
1.3.0


trimspaces
igo
0.0.24",3
,2877,148,97,53485,"Environment Details:
VSCode Version : 1.24.1
Additional Details:
MAS Violated: MAS2.1.1
Repro Steps:
1)Launch VS Code.
2)Open Keyboard Shortcuts.
Actual:
Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working. After using this command, screen starts rotating.
Expected:
Both the commands should work.
Recommendations:
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
MAS Reference
https://microsoft.sharepoint.com/:w:/r/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:

Does this issue occur when all extensions are disabled?: Yes",Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working,"Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not workingEnvironment Details:
VSCode Version : 1.24.1
Additional Details:
MAS Violated: MAS2.1.1
Repro Steps:
1)Launch VS Code.
2)Open Keyboard Shortcuts.
Actual:
Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working. After using this command, screen starts rotating.
Expected:
Both the commands should work.
Recommendations:
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
MAS Reference
https://microsoft.sharepoint.com/:w:/r/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:

Does this issue occur when all extensions are disabled?: Yes",3
,2878,146,98,9638,"The terminal, debug output, console, etc. panes at the bottom have a small down arrow for the action to close or hide the pane. clicking on this makes the pane slide down and disappear.

The down arrow to me suggests that the window will be collapsed down rather than closed. Whenever I click on this I expect there to be an action at the bottom of the editor to restore the pane. Instead I have to use the keyboard or menu to bring these back.
As a result, a better icon for this pane would be the close ""x"".
Alternatively, there should be a visualization at the bottom of the editor that there is a pane that can be restored.","close icon on terminal/console/etc. pane should be ""x""","close icon on terminal/console/etc. pane should be ""x""The terminal, debug output, console, etc. panes at the bottom have a small down arrow for the action to close or hide the pane. clicking on this makes the pane slide down and disappear.

The down arrow to me suggests that the window will be collapsed down rather than closed. Whenever I click on this I expect there to be an action at the bottom of the editor to restore the pane. Instead I have to use the keyboard or menu to bring these back.
As a result, a better icon for this pane would be the close ""x"".
Alternatively, there should be a visualization at the bottom of the editor that there is a pane that can be restored.",3
,2879,148,99,53533,"ERR Model is disposed!: Error: Model is disposed!
    at TextModel._assertNotDisposed (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:245:23)
    at TextModel.getVersionId (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:499:18)
    at OutlinePanel.<anonymous> (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:548:61)
    at step (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:50:23)
    at Object.next (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:31:53)
    at fulfilled (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:22:58)
    at <anonymous>
    at process._tickCallback (internal/process/next_tick.js:109:7)",ERR Model is disposed!: Error: Model is disposed!,"ERR Model is disposed!: Error: Model is disposed!ERR Model is disposed!: Error: Model is disposed!
    at TextModel._assertNotDisposed (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:245:23)
    at TextModel.getVersionId (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:499:18)
    at OutlinePanel.<anonymous> (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:548:61)
    at step (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:50:23)
    at Object.next (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:31:53)
    at fulfilled (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:22:58)
    at <anonymous>
    at process._tickCallback (internal/process/next_tick.js:109:7)",3